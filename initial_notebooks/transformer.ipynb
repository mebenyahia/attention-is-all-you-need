{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2716.73s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in ./.venv/lib/python3.11/site-packages (from pytorch_lightning) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in ./.venv/lib/python3.11/site-packages (from pytorch_lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in ./.venv/lib/python3.11/site-packages (from pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.9.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from pytorch_lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./.venv/lib/python3.11/site-packages (from pytorch_lightning) (4.12.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.11)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (65.5.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in ./.venv/lib/python3.11/site-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
      "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.11.9 pytorch_lightning-2.5.0.post0 torchmetrics-1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer, trainers, models, pre_tokenizers, normalizers\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1223)\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English-German subset of the WMT 2014 dataset\n",
    "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "# Explore the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(dataset_split, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Convert dataset to a list of dictionaries and save to JSON\n",
    "        json.dump([example for example in dataset_split], f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def load_from_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(dataset_split, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Convert dataset to a list of dictionaries and save to JSON\n",
    "        json.dump([example for example in dataset_split], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# save train, validation, or test splits locally in json format\n",
    "save_to_json(dataset['train'], 'train_data.json')\n",
    "save_to_json(dataset['validation'], 'validation_data.json')\n",
    "save_to_json(dataset['test'], 'test_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train, validation and test splits\n",
    "train_dataset = load_from_json('train_data.json')\n",
    "validation_dataset = load_from_json('validation_data.json')\n",
    "test_dataset = load_from_json('test_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets imported from JSON files.\n",
      "Example 0:\n",
      "  German: Wiederaufnahme der Sitzungsperiode\n",
      "  English: Resumption of the session\n",
      "Example 1:\n",
      "  German: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "  English: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Example 2:\n",
      "  German: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "  English: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n"
     ]
    }
   ],
   "source": [
    "print(\"Datasets imported from JSON files.\")\n",
    "# Print first 3 examples from re-imported train dataset\n",
    "for i in range(3):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  German: {train_dataset[i]['translation']['de']}\")\n",
    "    print(f\"  English: {train_dataset[i]['translation']['en']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['translation'],\n",
       "     num_rows: 4508785\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['translation'],\n",
       "     num_rows: 3000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['translation'],\n",
       "     num_rows: 3003\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used for training separate tokenizers\n",
    "# don't use for a single tokenizer\n",
    "os.makedirs(\"tokenizer_data\", exist_ok=True)\n",
    "\n",
    "with open(\"tokenizer_data/english.txt\", \"w\", encoding=\"utf-8\") as eng_file, \\\n",
    "     open(\"tokenizer_data/german.txt\", \"w\", encoding=\"utf-8\") as ger_file:\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        for example in dataset[split]:\n",
    "            eng_file.write(example['translation']['en'] + \"\\n\")\n",
    "            ger_file.write(example['translation']['de'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_data/common.txt\", \"w\", encoding=\"utf-8\") as common_file:\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        for example in dataset[split]:\n",
    "            common_file.write(example['translation']['en'] + \"\\n\")\n",
    "            common_file.write(example['translation']['de'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_sequence_length=1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token='[UNK]'))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "     normalizers.Lowercase()\n",
    "])\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "     pre_tokenizers.Whitespace(),\n",
    "])\n",
    "\n",
    "tokenizer.enable_truncation(max_length=allowed_sequence_length)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=37000, special_tokens=[\"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"], show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "tokenizer.train([\"tokenizer_data/common.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('bpe2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('bpe2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7695, 'bleibt'),\n",
       "  (7035, 'establish'),\n",
       "  (27362, 'vaters'),\n",
       "  (25984, 'steering'),\n",
       "  (7204, 'acy'),\n",
       "  (22122, 'certificates'),\n",
       "  (27713, 'zugriffs'),\n",
       "  (315, '̃'),\n",
       "  (14920, 'plural'),\n",
       "  (18637, 'contractual')],\n",
       " [('bleibt', 7695),\n",
       "  ('establish', 7035),\n",
       "  ('vaters', 27362),\n",
       "  ('steering', 25984),\n",
       "  ('acy', 7204),\n",
       "  ('certificates', 22122),\n",
       "  ('zugriffs', 27713),\n",
       "  ('̃', 315),\n",
       "  ('plural', 14920),\n",
       "  ('contractual', 18637)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = tokenizer.get_vocab()\n",
    "itos = { i:s for s, i in stoi.items() }\n",
    "list(itos.items())[:10], list(stoi.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[START]', '[END]', '[MASK]', '!')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[0], itos[1], itos[2], itos[3], itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n",
      "Tokens: ['resumption', 'of', 'the', 'session']\n",
      "Ids: [34500, 3600, 3581, 8962]\n",
      "------\n",
      "Wiederaufnahme der Sitzungsperiode\n",
      "Tokens: ['wiederaufnahme', 'der', 'sitzungsperiode']\n",
      "Ids: [24924, 3588, 24101]\n"
     ]
    }
   ],
   "source": [
    "ger_sent = train_dataset[0]['translation']['de']\n",
    "eng_sent = train_dataset[0]['translation']['en']\n",
    "\n",
    "ger_enc = tokenizer.encode(ger_sent)\n",
    "eng_enc = tokenizer.encode(eng_sent)\n",
    "\n",
    "print(eng_sent)\n",
    "print(f'Tokens: {eng_enc.tokens}')\n",
    "print(f'Ids: {eng_enc.ids}')\n",
    "print('------')\n",
    "print(ger_sent)\n",
    "print(f'Tokens: {ger_enc.tokens}')\n",
    "print(f'Ids: {ger_enc.ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest sentence over 14000 tokens long\n",
    "eng_sentences = [ example['translation']['en'] for example in train_dataset ]\n",
    "ger_sentences = [ example['translation']['de'] for example in train_dataset ]\n",
    "max_length = max(max(len(tokenizer.encode(eng).tokens), len(tokenizer.encode(ger).tokens)) for eng, ger in zip(eng_sentences, ger_sentences))\n",
    "print(f'Longest sentence: {max_length} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 512]), device(type='cpu'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the positional encoding\n",
    "\n",
    "def gen_pos_enc(seq_len, d_model, n):\n",
    "    pos_enc = torch.zeros((seq_len, d_model), dtype=torch.float)\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d_model//2):\n",
    "            theta = k / (n ** (2*i / d_model))\n",
    "            pos_enc[k, 2*i] = math.sin(theta)\n",
    "            pos_enc[k, 2*i+1] = math.cos(theta)\n",
    "            \n",
    "    pos_enc.requires_grad=False\n",
    "    return pos_enc\n",
    "\n",
    "pos_enc = gen_pos_enc(allowed_sequence_length, 512, 10000)\n",
    "pos_enc.shape, pos_enc.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc = pos_enc.to(mps_device)\n",
    "pos_enc.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources shape: torch.Size([64, 92])\n",
      "Targets shape: torch.Size([64, 74])\n",
      "{'sources': tensor([[    1,  3878,    15,  ...,     2,     2,     2],\n",
      "        [    1,  3581,  7862,  ...,     2,     2,     2],\n",
      "        [    1,  3581,  5571,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    1,  4805,  3809,  ...,     2,     2,     2],\n",
      "        [    1,  3581, 10254,  ...,     2,     2,     2],\n",
      "        [    1,    42, 27071,  ...,     2,     2,     2]]), 'targets': tensor([[    1,  3757,  3641,  ...,     2,     2,     2],\n",
      "        [    1,  3605,  7862,  ...,     2,     2,     2],\n",
      "        [    1,  3605,  6586,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    1, 18254,  3651,  ...,  3760,    17,     2],\n",
      "        [    1,  4980,  3605,  ...,     2,     2,     2],\n",
      "        [    1,  3599, 27071,  ...,     2,     2,     2]])}\n"
     ]
    }
   ],
   "source": [
    "# Custom collate function for tokenization and padding\n",
    "def collate_fn(batch):\n",
    "    tokenized_batch = []\n",
    "    for example in batch:\n",
    "        # Tokenize and encode sentences\n",
    "        eng = tokenizer.encode(example[\"translation\"][\"en\"]).ids\n",
    "        ger = tokenizer.encode(example[\"translation\"][\"de\"]).ids\n",
    "        \n",
    "        tokenized_batch.append((eng, ger))\n",
    "    \n",
    "    # Find the maximum sequence length in the batch\n",
    "    max_eng_length = max(len(eng) for (eng, _) in tokenized_batch)\n",
    "    max_ger_length = max(len(ger) for (_, ger) in tokenized_batch)\n",
    "\n",
    "    # Pad all sequences to the maximum length\n",
    "    sources = [[1] + eng + [2] * (max_eng_length - len(eng)) + [2] for eng, _ in tokenized_batch]\n",
    "    targets = [[1] + ger + [2] * (max_ger_length - len(ger)) + [2] for _, ger in tokenized_batch]\n",
    "    \n",
    "    batch = {\n",
    "        \"sources\": torch.tensor(sources, dtype=torch.long), \n",
    "        \"targets\": torch.tensor(targets, dtype=torch.long)\n",
    "    }\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    return batch\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Example: Iterate through the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(\"Sources shape:\", batch['sources'].shape)  # Shape: (batch_size, max_sequence_length)\n",
    "    print(\"Targets shape:\", batch['targets'].shape)  # Shape: (batch_size, max_sequence_length)\n",
    "    print(batch)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "\n",
    "class FeedForwardNetword(Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear_out = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# the following implementation of Multi Head Attention is slow and unsuitable for training \n",
    "class ScaledDotProductAttention(Module):\n",
    "    \n",
    "    def __init__(self, d_model, head_size, use_mask=False):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.w_q = nn.Linear(d_model, head_size, bias=False) # (batch_size, head_size)\n",
    "        self.w_k = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.use_mask = use_mask        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        q = self.w_q(q) # (batch_size, sec_len, d_model) @ (d_model, head_size) = (batch_size, sec_len, head_size)\n",
    "        k = self.w_k(k) \n",
    "        v = self.w_v(v)\n",
    "        \n",
    "        rel = q @ k.transpose(-2, -1) # (batch_size, sec_length, sec_length)\n",
    "        rel = rel * self.head_size**-0.5 \n",
    "        \n",
    "        if self.use_mask:\n",
    "            sec_len = rel.size(-1)\n",
    "            mask = torch.tril(torch.ones(sec_len, sec_len, requires_grad=False,)).to(self.device)\n",
    "            rel = rel.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        value_weights = self.softmax(rel)\n",
    "        \n",
    "        return value_weights @ v # (batch_size, sec_length, sec_length) @ ()\n",
    "\n",
    "        \n",
    "class MultiHeadAttention(Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, use_mask=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "        head_size = d_model // num_heads\n",
    "        self.heads = nn.ModuleList([ScaledDotProductAttention(d_model, head_size, use_mask) for _ in range(num_heads)])\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        merged = torch.cat([head(q, k, v) for head in self.heads], dim=-1)\n",
    "        return self.w_o(merged)\n",
    "\n",
    "\n",
    "class EncoderLayer(Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.encoderLayers = []\n",
    "            \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "            \n",
    "        self.ffn = FeedForwardNetword(d_model, d_ff)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        att_out = self.attention(x, x, x)\n",
    "        x = self.layer_norm_1(x + self.dropout_1(att_out))\n",
    "        \n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.layer_norm_2(x + self.dropout_2(ffn_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, use_mask=True)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "            \n",
    "        self.ffn = FeedForwardNetword(d_model, d_ff)\n",
    "        self.dropout_3 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        att_out = self.attention(x, x, x)\n",
    "        x = self.layer_norm_1(x + self.dropout_1(att_out))\n",
    "        \n",
    "        enc_att_out = self.enc_attention(q=x, k=enc_out, v=enc_out)\n",
    "        x = self.layer_norm_2(x + self.dropout_2(enc_att_out))\n",
    "        \n",
    "        fnn_out = self.ffn(x)\n",
    "        x = self.layer_norm_3(x + self.dropout_3(fnn_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embedding(Module):\n",
    "        \n",
    "    def __init__(self, vocab_size, d_model, pos_enc, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, 2) # 2 is the eos token, we also pad with it\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pos_enc = pos_enc\n",
    "            \n",
    "    def forward(self, x):\n",
    "        emb_out = self.embedding(x) # (batch, sequence, embedding)\n",
    "        sec_len = emb_out.shape[1]\n",
    "        return self.dropout(emb_out + self.pos_enc[:sec_len,:])\n",
    "    \n",
    "\n",
    "class Transformer(Module):\n",
    "        \n",
    "    def __init__(self, vocab_size, d_model, d_ff, pos_enc, num_heads=8, N=6, seed=5012025, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.source_embedding = Embedding(vocab_size, d_model, pos_enc, dropout_rate=dropout_rate) \n",
    "        self.target_embedding = Embedding(vocab_size, d_model, pos_enc, dropout_rate=dropout_rate)\n",
    "        \n",
    "\n",
    "        self.encoder_stack = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads, dropout_rate) for _ in range(N)])\n",
    "        self.decoder_stack = nn.ModuleList([DecoderLayer(d_model, d_ff, num_heads, dropout_rate) for _ in range(N)])\n",
    "            \n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "            \n",
    "        self.to(self.device)\n",
    "            \n",
    "        torch.seed()\n",
    "            \n",
    "    def forward(self, source, target):\n",
    "        \n",
    "        enc_out = self.source_embedding(source)\n",
    "        for encoder_layer in self.encoder_stack:\n",
    "            enc_out = encoder_layer(enc_out)\n",
    "            \n",
    "        dec_out = self.target_embedding(target)\n",
    "        for decoder_layer in self.decoder_stack:\n",
    "            dec_out = decoder_layer(dec_out, enc_out)\n",
    "            \n",
    "        return self.linear(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 10.383936882019043\n",
      "training loss: 9.614827156066895\n",
      "training loss: 8.692097663879395\n",
      "training loss: 7.98657751083374\n",
      "training loss: 7.713242530822754\n",
      "training loss: 7.339689254760742\n",
      "training loss: 6.295281410217285\n",
      "training loss: 5.978188514709473\n",
      "training loss: 5.56548547744751\n",
      "training loss: 4.410912990570068\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model: Transformer, train_dataloader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        sources = batch['sources'].to(model.device)\n",
    "        targets = batch['targets'].to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sources, targets[:, :-1]) \n",
    "        \n",
    "        B, S, C = predictions.shape\n",
    "        \n",
    "        loss = loss_function(predictions.reshape(-1, C), targets[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'training loss: {loss.item()}')\n",
    "        train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches == 10:\n",
    "            break\n",
    "\n",
    "\n",
    "model = Transformer(37000, 512, 2048, pos_enc, 8, 6)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-9)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "sheduler = ReduceLROnPlateau(optimizer, factor=0.5)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch(model, train_dataloader, optimizer, loss_function)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37000, 512])\n",
      "torch.Size([37000, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([37000, 512])\n",
      "torch.Size([37000])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'example_input_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_summary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[0;32m----> 2\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m~/Uni/python/transformer/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:499\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m(lightning_module, max_depth)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(lightning_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_depth: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelSummary:\n\u001b[1;32m    487\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Summarize the LightningModule specified by `lightning_module`.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m \n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelSummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/python/transformer/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:216\u001b[0m, in \u001b[0;36mModelSummary.__init__\u001b[0;34m(self, model, max_depth)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_depth` can be -1, 0 or > 0, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_depth \u001b[38;5;241m=\u001b[39m max_depth\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# 1 byte -> 8 bits\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# TODO: how do we compute precision_megabytes in case of mixed precision?\u001b[39;00m\n\u001b[1;32m    219\u001b[0m precision_to_bits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m}\n",
      "File \u001b[0;32m~/Uni/python/transformer/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:284\u001b[0m, in \u001b[0;36mModelSummary.summarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, LayerSummary]:\n\u001b[1;32m    283\u001b[0m     summary \u001b[38;5;241m=\u001b[39m OrderedDict((name, LayerSummary(module)) \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules)\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_input_array\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_example_input()\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/Uni/python/transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'example_input_array'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
