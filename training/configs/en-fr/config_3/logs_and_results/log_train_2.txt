Script started on 2025-01-27 13:27:50+00:00 [COMMAND="python train.py" TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="149" LINES="66"]
Loading training set from saved...
Dataset size: 300000
Loading validation set from saved...
Loading tokenizer from saved...
Loading processed dataset...
Loading processed dataset...
Preparing model...
Loading accelerator state...
Training: 
Batch 1600: training loss 3.01393985748291
Batch 1700: training loss 3.045903205871582
Batch 1800: training loss 2.7823476791381836
Batch 1900: training loss 2.932041645050049
Batch 2000: training loss 2.937631845474243
Saving checkpoint to accelerator_checkpoints_1...
Batch 2100: training loss 3.0090413093566895
Batch 2200: training loss 3.039548397064209
Batch 2300: training loss 2.9509315490722656
Epoch 1: Average Training Loss: 2.9380964371265392, Average Validation Loss: 3.2960998713970184
Batch 100: training loss 3.0304417610168457
Batch 200: training loss 2.9154934883117676
Batch 300: training loss 2.770216464996338
Batch 400: training loss 2.9424080848693848
Batch 500: training loss 2.816287040710449
Saving checkpoint to accelerator_checkpoints_1...
Batch 600: training loss 3.034607410430908
Batch 700: training loss 3.165721893310547
Batch 800: training loss 2.9332823753356934
Batch 900: training loss 3.0447998046875
Batch 1000: training loss 2.853358268737793
Saving checkpoint to accelerator_checkpoints_1...
Batch 1100: training loss 2.9868693351745605
Batch 1200: training loss 2.9601597785949707
Batch 1300: training loss 3.038431406021118
Batch 1400: training loss 2.9145121574401855
Batch 1500: training loss 2.912121534347534
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.030122756958008
Batch 1700: training loss 2.9186625480651855
Batch 1800: training loss 2.8502283096313477
Batch 1900: training loss 2.9458789825439453
Batch 2000: training loss 2.7638509273529053
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.9699230194091797
Batch 2200: training loss 3.0530335903167725
Batch 2300: training loss 2.8422722816467285
Epoch 2: Average Training Loss: 2.950412372069961, Average Validation Loss: 3.287790447473526
Batch 100: training loss 3.0780725479125977
Batch 200: training loss 2.8387036323547363
Batch 300: training loss 2.9611432552337646
Batch 400: training loss 2.9122085571289062
Batch 500: training loss 2.849118232727051
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.957515239715576
Batch 700: training loss 2.9835257530212402
Batch 800: training loss 3.0662643909454346
Batch 900: training loss 2.932039260864258
Batch 1000: training loss 2.995497226715088
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.0354042053222656
Batch 1200: training loss 2.9528908729553223
Batch 1300: training loss 2.9586195945739746
Batch 1400: training loss 3.0827131271362305
Batch 1500: training loss 3.0301733016967773
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.06070613861084
Batch 1700: training loss 2.9301295280456543
Batch 1800: training loss 2.777172088623047
Batch 1900: training loss 2.875030279159546
Batch 2000: training loss 2.8720760345458984
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8304390907287598
Batch 2200: training loss 3.003635883331299
Batch 2300: training loss 3.2056357860565186
Epoch 3: Average Training Loss: 2.946021857420332, Average Validation Loss: 3.289205471674601
Batch 100: training loss 3.051023483276367
Batch 200: training loss 2.9342429637908936
Batch 300: training loss 2.940737247467041
Batch 400: training loss 3.0359368324279785
Batch 500: training loss 2.960179328918457
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.868534564971924
Batch 700: training loss 2.9328274726867676
Batch 800: training loss 2.9679155349731445
Batch 900: training loss 2.924381732940674
Batch 1000: training loss 2.934236526489258
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.9274868965148926
Batch 1200: training loss 2.9250779151916504
Batch 1300: training loss 2.9533839225769043
Batch 1400: training loss 2.898085117340088
Batch 1500: training loss 3.074693202972412
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.918414354324341
Batch 1700: training loss 2.9721479415893555
Batch 1800: training loss 3.112164258956909
Batch 1900: training loss 2.929522752761841
Batch 2000: training loss 2.9734108448028564
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.971285581588745
Batch 2200: training loss 2.734099864959717
Batch 2300: training loss 2.9154491424560547
Epoch 4: Average Training Loss: 2.9432348059712825, Average Validation Loss: 3.280725210905075
Batch 100: training loss 2.9707882404327393
Batch 200: training loss 3.1679959297180176
Batch 300: training loss 2.8651938438415527
Batch 400: training loss 2.9643919467926025
Batch 500: training loss 2.943359375
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.7914981842041016
Batch 700: training loss 2.800741672515869
Batch 800: training loss 2.798769950866699
Batch 900: training loss 2.90310001373291
Batch 1000: training loss 2.8783621788024902
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.9139537811279297
Batch 1200: training loss 2.939727783203125
Batch 1300: training loss 2.9820938110351562
Batch 1400: training loss 2.8129091262817383
Batch 1500: training loss 3.0529420375823975
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.8606786727905273
Batch 1700: training loss 2.8713126182556152
Batch 1800: training loss 2.987253189086914
Batch 1900: training loss 2.8993520736694336
Batch 2000: training loss 2.9690465927124023
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.881459951400757
Batch 2200: training loss 2.867319107055664
Batch 2300: training loss 2.9183454513549805
Epoch 5: Average Training Loss: 2.939550412799718, Average Validation Loss: 3.282018860181173
Batch 100: training loss 2.8163628578186035
Batch 200: training loss 2.912656307220459
Batch 300: training loss 2.9932186603546143
Batch 400: training loss 3.0485587120056152
Batch 500: training loss 2.799790382385254
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 3.009413242340088
Batch 700: training loss 2.9058918952941895
Batch 800: training loss 2.8262596130371094
Batch 900: training loss 3.0447356700897217
Batch 1000: training loss 3.0056042671203613
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.979292869567871
Batch 1200: training loss 2.934671401977539
Batch 1300: training loss 3.0494446754455566
Batch 1400: training loss 2.851017951965332
Batch 1500: training loss 3.0246431827545166
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.892085075378418
Batch 1700: training loss 3.017037868499756
Batch 1800: training loss 2.7477521896362305
Batch 1900: training loss 2.868448257446289
Batch 2000: training loss 2.7963387966156006
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8972978591918945
Batch 2200: training loss 2.9561147689819336
Batch 2300: training loss 2.8960013389587402
Epoch 6: Average Training Loss: 2.9363832513423502, Average Validation Loss: 3.283761183420817
Batch 100: training loss 2.8248281478881836
Batch 200: training loss 2.8763463497161865
Batch 300: training loss 2.8501083850860596
Batch 400: training loss 2.9478187561035156
Batch 500: training loss 2.8362178802490234
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.817713499069214
Batch 700: training loss 2.892430305480957
Batch 800: training loss 2.8920836448669434
Batch 900: training loss 3.039839267730713
Batch 1000: training loss 3.0864648818969727
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.8652453422546387
Batch 1200: training loss 2.8965282440185547
Batch 1300: training loss 3.095846176147461
Batch 1400: training loss 2.952662467956543
Batch 1500: training loss 2.987389326095581
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.0221500396728516
Batch 1700: training loss 3.049915313720703
Batch 1800: training loss 2.944801092147827
Batch 1900: training loss 2.877519369125366
Batch 2000: training loss 2.890367031097412
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.9415931701660156
Batch 2200: training loss 2.863182544708252
Batch 2300: training loss 2.8820042610168457
Epoch 7: Average Training Loss: 2.933478845060889, Average Validation Loss: 3.2722078760464988
Batch 100: training loss 2.905322790145874
Batch 200: training loss 2.796111583709717
Batch 300: training loss 3.011054754257202
Batch 400: training loss 2.8356270790100098
Batch 500: training loss 2.879863739013672
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.950049877166748
Batch 700: training loss 2.799669027328491
Batch 800: training loss 2.850949764251709
Batch 900: training loss 2.8151988983154297
Batch 1000: training loss 2.8584513664245605
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.8935282230377197
Batch 1200: training loss 3.092223644256592
Batch 1300: training loss 2.7871408462524414
Batch 1400: training loss 2.871634006500244
Batch 1500: training loss 3.013850450515747
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.807511806488037
Batch 1700: training loss 3.023773431777954
Batch 1800: training loss 2.8580687046051025
Batch 1900: training loss 2.8550148010253906
Batch 2000: training loss 2.9669599533081055
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.908749580383301
Batch 2200: training loss 2.8857829570770264
Batch 2300: training loss 3.0195913314819336
Epoch 8: Average Training Loss: 2.9306350198825473, Average Validation Loss: 3.2810799181461334
Batch 100: training loss 2.9793355464935303
Batch 200: training loss 2.876784324645996
Batch 300: training loss 2.8498661518096924
Batch 400: training loss 2.895493984222412
Batch 500: training loss 2.794631004333496
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.8167078495025635
Batch 700: training loss 2.8996243476867676
Batch 800: training loss 2.880399703979492
Batch 900: training loss 2.939833164215088
Batch 1000: training loss 2.8130688667297363
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.00388240814209
Batch 1200: training loss 2.883258819580078
Batch 1300: training loss 2.7687883377075195
Batch 1400: training loss 3.063197612762451
Batch 1500: training loss 2.8110482692718506
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.018436908721924
Batch 1700: training loss 2.989407539367676
Batch 1800: training loss 2.9362902641296387
Batch 1900: training loss 2.953313112258911
Batch 2000: training loss 2.80332612991333
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.902371883392334
Batch 2200: training loss 2.9167399406433105
Batch 2300: training loss 2.893420696258545
Epoch 9: Average Training Loss: 2.9277536872100502, Average Validation Loss: 3.2740921874841056
Batch 100: training loss 2.997130870819092
Batch 200: training loss 2.9005720615386963
Batch 300: training loss 3.0591940879821777
Batch 400: training loss 2.8112287521362305
Batch 500: training loss 2.744326114654541
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.9409494400024414
Batch 700: training loss 3.020138740539551
Batch 800: training loss 3.039229393005371
Batch 900: training loss 2.8830084800720215
Batch 1000: training loss 2.9617550373077393
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.779428243637085
Batch 1200: training loss 2.889120578765869
Batch 1300: training loss 3.145171880722046
Batch 1400: training loss 2.889155864715576
Batch 1500: training loss 2.898037910461426
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.9411864280700684
Batch 1700: training loss 2.7734038829803467
Batch 1800: training loss 2.8815813064575195
Batch 1900: training loss 3.027283191680908
Batch 2000: training loss 2.728365898132324
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 3.0121724605560303
Batch 2200: training loss 2.9041643142700195
Batch 2300: training loss 3.0051894187927246
Epoch 10: Average Training Loss: 2.9249824476730293, Average Validation Loss: 3.2757653892040253
Batch 100: training loss 3.0020761489868164
Batch 200: training loss 2.9181411266326904
Batch 300: training loss 2.926638603210449
Batch 400: training loss 3.0271315574645996
Batch 500: training loss 3.0556585788726807
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.9554061889648438
Batch 700: training loss 2.894188165664673
Batch 800: training loss 2.842679738998413
Batch 900: training loss 2.8674936294555664
Batch 1000: training loss 2.8398380279541016
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.009145975112915
Batch 1200: training loss 2.8982653617858887
Batch 1300: training loss 2.9325695037841797
Batch 1400: training loss 2.955822467803955
Batch 1500: training loss 3.0866174697875977
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.915574073791504
Batch 1700: training loss 2.894436836242676
Batch 1800: training loss 3.031306266784668
Batch 1900: training loss 2.9349071979522705
Batch 2000: training loss 2.963266611099243
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8910293579101562
Batch 2200: training loss 2.7920215129852295
Batch 2300: training loss 2.9491660594940186
Epoch 11: Average Training Loss: 2.922228069130471, Average Validation Loss: 3.2682116429011026
Batch 100: training loss 3.015350341796875
Batch 200: training loss 2.9191977977752686
Batch 300: training loss 2.952151298522949
Batch 400: training loss 3.01491379737854
Batch 500: training loss 2.9334630966186523
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 3.0188190937042236
Batch 700: training loss 3.144770622253418
Batch 800: training loss 2.9402880668640137
Batch 900: training loss 2.7514748573303223
Batch 1000: training loss 2.865262031555176
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.824674367904663
Batch 1200: training loss 2.88883638381958
Batch 1300: training loss 3.0448994636535645
Batch 1400: training loss 3.0119245052337646
Batch 1500: training loss 2.8199636936187744
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.765545606613159
Batch 1700: training loss 3.0178983211517334
Batch 1800: training loss 2.969125270843506
Batch 1900: training loss 2.964653253555298
Batch 2000: training loss 2.749350070953369
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8559961318969727
Batch 2200: training loss 2.791508913040161
Batch 2300: training loss 2.769070863723755
Epoch 12: Average Training Loss: 2.919459419954352, Average Validation Loss: 3.271350701649984
Batch 100: training loss 3.018589496612549
Batch 200: training loss 2.842226982116699
Batch 300: training loss 3.0562517642974854
Batch 400: training loss 2.951000928878784
Batch 500: training loss 2.8076391220092773
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.8150792121887207
Batch 700: training loss 2.9205617904663086
Batch 800: training loss 2.945864677429199
Batch 900: training loss 2.7531261444091797
Batch 1000: training loss 3.0323121547698975
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.0608577728271484
Batch 1200: training loss 3.0477676391601562
Batch 1300: training loss 2.8850274085998535
Batch 1400: training loss 2.8342573642730713
Batch 1500: training loss 2.8870174884796143
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.9328906536102295
Batch 1700: training loss 2.8980538845062256
Batch 1800: training loss 2.919711112976074
Batch 1900: training loss 2.922393321990967
Batch 2000: training loss 2.987975597381592
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.714285135269165
Batch 2200: training loss 2.8339576721191406
Batch 2300: training loss 2.9698972702026367
Epoch 13: Average Training Loss: 2.9162953173544626, Average Validation Loss: 3.2612606982390084
Batch 100: training loss 2.9411683082580566
Batch 200: training loss 2.9141321182250977
Batch 300: training loss 3.0507421493530273
Batch 400: training loss 2.906660556793213
Batch 500: training loss 3.045304298400879
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.89022159576416
Batch 700: training loss 2.961564302444458
Batch 800: training loss 2.8592653274536133
Batch 900: training loss 2.968876600265503
Batch 1000: training loss 2.9367012977600098
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.8909316062927246
Batch 1200: training loss 2.776137590408325
Batch 1300: training loss 2.9207444190979004
Batch 1400: training loss 2.9530022144317627
Batch 1500: training loss 2.867717742919922
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.737699508666992
Batch 1700: training loss 2.930804491043091
Batch 1800: training loss 2.818941116333008
Batch 1900: training loss 3.02339506149292
Batch 2000: training loss 3.0569233894348145
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.973598003387451
Batch 2200: training loss 2.8996810913085938
Batch 2300: training loss 3.0270071029663086
Epoch 14: Average Training Loss: 2.914196796486402, Average Validation Loss: 3.26150972644488
Batch 100: training loss 2.798787832260132
Batch 200: training loss 2.9330201148986816
Batch 300: training loss 2.7820444107055664
Batch 400: training loss 2.7907819747924805
Batch 500: training loss 2.962005138397217
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.817810535430908
Batch 700: training loss 2.8247008323669434
Batch 800: training loss 2.891303300857544
Batch 900: training loss 2.9028196334838867
Batch 1000: training loss 2.9252099990844727
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.795565605163574
Batch 1200: training loss 3.0471107959747314
Batch 1300: training loss 3.0585451126098633
Batch 1400: training loss 3.127110481262207
Batch 1500: training loss 3.165508508682251
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.8201828002929688
Batch 1700: training loss 3.10744047164917
Batch 1800: training loss 3.0653555393218994
Batch 1900: training loss 3.1212499141693115
Batch 2000: training loss 3.049516201019287
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.926029920578003
Batch 2200: training loss 2.9760398864746094
Batch 2300: training loss 3.0650103092193604
Epoch 15: Average Training Loss: 2.9115040373476697, Average Validation Loss: 3.2590817511081696
Batch 100: training loss 2.827641010284424
Batch 200: training loss 3.0081043243408203
Batch 300: training loss 2.962667942047119
Batch 400: training loss 3.1118340492248535
Batch 500: training loss 2.8249690532684326
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.88857102394104
Batch 700: training loss 2.9575257301330566
Batch 800: training loss 2.8526697158813477
Batch 900: training loss 2.9710397720336914
Batch 1000: training loss 2.9376511573791504
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.9375457763671875
Batch 1200: training loss 3.0660171508789062
Batch 1300: training loss 2.9469356536865234
Batch 1400: training loss 2.887678623199463
Batch 1500: training loss 3.085515022277832
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.9173028469085693
Batch 1700: training loss 2.908480167388916
Batch 1800: training loss 2.9610767364501953
Batch 1900: training loss 2.756129264831543
Batch 2000: training loss 2.8687047958374023
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.929136276245117
Batch 2200: training loss 2.8987913131713867
Batch 2300: training loss 2.9547224044799805
Epoch 16: Average Training Loss: 2.909322952552867, Average Validation Loss: 3.2654904623826346
Batch 100: training loss 2.9002878665924072
Batch 200: training loss 2.8619518280029297
Batch 300: training loss 2.9053821563720703
Batch 400: training loss 2.9666805267333984
Batch 500: training loss 2.9256510734558105
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.9172675609588623
Batch 700: training loss 2.987818717956543
Batch 800: training loss 3.0365428924560547
Batch 900: training loss 2.6938090324401855
Batch 1000: training loss 3.0746848583221436
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.8746161460876465
Batch 1200: training loss 3.029024600982666
Batch 1300: training loss 2.953192710876465
Batch 1400: training loss 2.94400691986084
Batch 1500: training loss 2.9062814712524414
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.0312418937683105
Batch 1700: training loss 3.048086643218994
Batch 1800: training loss 2.8990731239318848
Batch 1900: training loss 2.893603801727295
Batch 2000: training loss 2.944939136505127
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.9460997581481934
Batch 2200: training loss 2.819932460784912
Batch 2300: training loss 3.0400707721710205
Epoch 17: Average Training Loss: 2.906349206336936, Average Validation Loss: 3.264425774415334
Batch 100: training loss 2.7648849487304688
Batch 200: training loss 2.9379727840423584
Batch 300: training loss 2.791625499725342
Batch 400: training loss 2.827500343322754
Batch 500: training loss 2.8911867141723633
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.941889524459839
Batch 700: training loss 2.810152530670166
Batch 800: training loss 2.836394786834717
Batch 900: training loss 2.9881885051727295
Batch 1000: training loss 2.858962059020996
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.024282217025757
Batch 1200: training loss 2.9817399978637695
Batch 1300: training loss 2.833089828491211
Batch 1400: training loss 2.8808469772338867
Batch 1500: training loss 2.7953925132751465
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.9692301750183105
Batch 1700: training loss 2.8940224647521973
Batch 1800: training loss 2.9841322898864746
Batch 1900: training loss 2.904069423675537
Batch 2000: training loss 2.8217666149139404
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 3.0341014862060547
Batch 2200: training loss 2.9806957244873047
Batch 2300: training loss 2.920172691345215
Epoch 18: Average Training Loss: 2.904130915834635, Average Validation Loss: 3.2546541690826416
Batch 100: training loss 2.7500405311584473
Batch 200: training loss 2.9721078872680664
Batch 300: training loss 2.915961742401123
Batch 400: training loss 2.886988401412964
Batch 500: training loss 2.9250566959381104
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.709099292755127
Batch 700: training loss 2.8071370124816895
Batch 800: training loss 2.9766392707824707
Batch 900: training loss 2.7742252349853516
Batch 1000: training loss 2.8723959922790527
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.7612903118133545
Batch 1200: training loss 2.8298912048339844
Batch 1300: training loss 2.953084945678711
Batch 1400: training loss 2.8333325386047363
Batch 1500: training loss 2.9355711936950684
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.7744321823120117
Batch 1700: training loss 2.8428397178649902
Batch 1800: training loss 2.9030818939208984
Batch 1900: training loss 2.927701473236084
Batch 2000: training loss 2.956542730331421
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.94283390045166
Batch 2200: training loss 2.9822847843170166
Batch 2300: training loss 2.9467403888702393
Epoch 19: Average Training Loss: 2.902042352095399, Average Validation Loss: 3.2539678514003754
Batch 100: training loss 2.8799071311950684
Batch 200: training loss 2.7887182235717773
Batch 300: training loss 3.1665706634521484
Batch 400: training loss 2.939952850341797
Batch 500: training loss 3.206663131713867
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.859816551208496
Batch 700: training loss 2.883721351623535
Batch 800: training loss 2.761490821838379
Batch 900: training loss 2.788780689239502
Batch 1000: training loss 2.8386526107788086
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.720996856689453
Batch 1200: training loss 3.013031005859375
Batch 1300: training loss 2.859114646911621
Batch 1400: training loss 2.9529809951782227
Batch 1500: training loss 2.8396778106689453
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.809558868408203
Batch 1700: training loss 2.871391534805298
Batch 1800: training loss 2.855428695678711
Batch 1900: training loss 3.001734733581543
Batch 2000: training loss 2.9085323810577393
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.84023380279541
Batch 2200: training loss 3.049905300140381
Batch 2300: training loss 2.7656431198120117
Epoch 20: Average Training Loss: 2.899036035399388, Average Validation Loss: 3.251700520515442
Batch 100: training loss 2.8151888847351074
Batch 200: training loss 3.006953716278076
Batch 300: training loss 2.982919931411743
Batch 400: training loss 2.881636381149292
Batch 500: training loss 3.0024256706237793
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.8654322624206543
Batch 700: training loss 2.8448801040649414
Batch 800: training loss 2.880063056945801
Batch 900: training loss 2.8009674549102783
Batch 1000: training loss 2.8934483528137207
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.8874363899230957
Batch 1200: training loss 2.7228457927703857
Batch 1300: training loss 2.832423210144043
Batch 1400: training loss 2.8771958351135254
Batch 1500: training loss 2.995779514312744
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.711923599243164
Batch 1700: training loss 2.93711519241333
Batch 1800: training loss 2.794382095336914
Batch 1900: training loss 2.847491502761841
Batch 2000: training loss 2.836817741394043
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.943892478942871
Batch 2200: training loss 3.061018943786621
Batch 2300: training loss 2.863084316253662
Epoch 21: Average Training Loss: 2.8970912956337065, Average Validation Loss: 3.25218058625857
Batch 100: training loss 2.8239684104919434
Batch 200: training loss 2.7381365299224854
Batch 300: training loss 2.8140811920166016
Batch 400: training loss 2.903904914855957
Batch 500: training loss 2.882615566253662
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.990009307861328
Batch 700: training loss 2.9402074813842773
Batch 800: training loss 2.7736215591430664
Batch 900: training loss 2.7822256088256836
Batch 1000: training loss 2.9088690280914307
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.9862005710601807
Batch 1200: training loss 2.924835205078125
Batch 1300: training loss 2.792990207672119
Batch 1400: training loss 2.9024205207824707
Batch 1500: training loss 2.834028720855713
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.021829128265381
Batch 1700: training loss 2.7811813354492188
Batch 1800: training loss 3.006673812866211
Batch 1900: training loss 2.984910488128662
Batch 2000: training loss 2.9467790126800537
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8260622024536133
Batch 2200: training loss 2.9704551696777344
Batch 2300: training loss 2.939183235168457
Epoch 22: Average Training Loss: 2.894710269906008, Average Validation Loss: 3.2472788095474243
Batch 100: training loss 2.8142991065979004
Batch 200: training loss 2.952838897705078
Batch 300: training loss 3.0810365676879883
Batch 400: training loss 2.989518404006958
Batch 500: training loss 2.912379741668701
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 3.100435256958008
Batch 700: training loss 2.836824417114258
Batch 800: training loss 2.9639956951141357
Batch 900: training loss 2.841489791870117
Batch 1000: training loss 2.77932071685791
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 3.019078493118286
Batch 1200: training loss 2.7521204948425293
Batch 1300: training loss 2.917036533355713
Batch 1400: training loss 2.700160026550293
Batch 1500: training loss 2.895047664642334
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 3.0376381874084473
Batch 1700: training loss 2.844350814819336
Batch 1800: training loss 2.8504743576049805
Batch 1900: training loss 2.756713390350342
Batch 2000: training loss 2.832292079925537
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.9099159240722656
Batch 2200: training loss 2.752530574798584
Batch 2300: training loss 3.0616421699523926
Epoch 23: Average Training Loss: 2.89247939153336, Average Validation Loss: 3.242468148469925
Batch 100: training loss 2.838698387145996
Batch 200: training loss 2.9133942127227783
Batch 300: training loss 2.7505874633789062
Batch 400: training loss 3.0516581535339355
Batch 500: training loss 2.889655828475952
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 600: training loss 2.8725781440734863
Batch 700: training loss 2.96035099029541
Batch 800: training loss 2.961463212966919
Batch 900: training loss 2.997835636138916
Batch 1000: training loss 2.8658573627471924
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: training loss 2.803849697113037
Batch 1200: training loss 3.063981056213379
Batch 1300: training loss 2.885972261428833
Batch 1400: training loss 2.9100141525268555
Batch 1500: training loss 2.8196558952331543
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1600: training loss 2.774224281311035
Batch 1700: training loss 2.828981876373291
Batch 1800: training loss 2.933439016342163
Batch 1900: training loss 2.8133182525634766
Batch 2000: training loss 2.94081974029541
Saving checkpoint to accelerator_checkpoints_1...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: training loss 2.8309879302978516
Batch 2200: training loss 3.0521650314331055
Batch 2300: training loss 2.8923397064208984
Epoch 24: Average Training Loss: 2.8900468832600237, Average Validation Loss: 3.2411541740099588
Saving model...

Script done on 2025-01-27 16:14:08+00:00 [COMMAND_EXIT_CODE="0"]
