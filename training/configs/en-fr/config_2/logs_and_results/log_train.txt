Script started on 2025-01-26 13:22:35+00:00 [COMMAND="python train.py" TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="138" LINES="47"]
Loading training set from saved...
Dataset size: 40000
Loading validation set from saved...
Loading tokenizer from saved...
Loading processed dataset...
Loading processed dataset...
Preparing model...
Training: 
Batch 30: training loss 8.613773345947266
Batch 60: training loss 8.30586051940918
Batch 90: training loss 7.96034574508667
Batch 120: training loss 7.685691833496094
Batch 150: training loss 7.390665531158447
Batch 180: training loss 7.322873592376709
Batch 210: training loss 7.152606010437012
Batch 240: training loss 7.094209671020508
Batch 270: training loss 6.991168022155762
Batch 300: training loss 6.967472076416016
Saving checkpoint to accelerator_checkpoints_0...
Batch 330: training loss 6.928934097290039
Batch 360: training loss 6.867293357849121
Batch 390: training loss 6.762255668640137
Batch 420: training loss 6.8617095947265625
Batch 450: training loss 6.85830020904541
Batch 480: training loss 6.817872524261475
Batch 510: training loss 6.71581506729126
Batch 540: training loss 6.675645351409912
Batch 570: training loss 6.759654521942139
Batch 600: training loss 6.7425737380981445
Saving checkpoint to accelerator_checkpoints_0...
Epoch 1: Average Training Loss: 7.211577709960937, Average Validation Loss: 6.802141818594425
Batch 30: training loss 6.614861488342285
Batch 60: training loss 6.649064064025879
Batch 90: training loss 6.6041412353515625
Batch 120: training loss 6.650911808013916
Batch 150: training loss 6.627051830291748
Batch 180: training loss 6.638045310974121
Batch 210: training loss 6.5124382972717285
Batch 240: training loss 6.580803394317627
Batch 270: training loss 6.69996976852417
Batch 300: training loss 6.504987716674805
Saving checkpoint to accelerator_checkpoints_0...
Batch 330: training loss 6.475802898406982
Batch 360: training loss 6.508896827697754
Batch 390: training loss 6.6078901290893555
Batch 420: training loss 6.386878490447998
Batch 450: training loss 6.534646511077881
Batch 480: training loss 6.461738586425781
Batch 510: training loss 6.584368705749512
Batch 540: training loss 6.462989807128906
Batch 570: training loss 6.537723064422607
Batch 600: training loss 6.391469478607178
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 2: Average Training Loss: 6.502171441650391, Average Validation Loss: 6.559996402010005
Batch 30: training loss 6.564303398132324
Batch 60: training loss 6.400752544403076
Batch 90: training loss 6.424498081207275
Batch 120: training loss 6.291033744812012
Batch 150: training loss 6.446709632873535
Batch 180: training loss 6.425757884979248
Batch 210: training loss 6.226023197174072
Batch 240: training loss 6.414711952209473
Batch 270: training loss 6.264763832092285
Batch 300: training loss 6.498263359069824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 6.166167736053467
Batch 360: training loss 6.267887592315674
Batch 390: training loss 6.461197853088379
Batch 420: training loss 6.260369777679443
Batch 450: training loss 6.147558689117432
Batch 480: training loss 6.285756587982178
Batch 510: training loss 6.330917835235596
Batch 540: training loss 6.216609001159668
Batch 570: training loss 6.057372570037842
Batch 600: training loss 6.150332927703857
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 3: Average Training Loss: 6.263079197692871, Average Validation Loss: 6.408111491101853
Batch 30: training loss 6.035317897796631
Batch 60: training loss 6.339537143707275
Batch 90: training loss 6.241405487060547
Batch 120: training loss 6.108506679534912
Batch 150: training loss 6.293035984039307
Batch 180: training loss 6.048656463623047
Batch 210: training loss 6.071599960327148
Batch 240: training loss 6.032391548156738
Batch 270: training loss 6.172855377197266
Batch 300: training loss 6.075605392456055
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 6.180567264556885
Batch 360: training loss 6.130131721496582
Batch 390: training loss 6.163310527801514
Batch 420: training loss 6.048746585845947
Batch 450: training loss 6.13252067565918
Batch 480: training loss 6.294740676879883
Batch 510: training loss 5.940359115600586
Batch 540: training loss 6.043477535247803
Batch 570: training loss 6.027156829833984
Batch 600: training loss 5.958434104919434
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 4: Average Training Loss: 6.1195451934814455, Average Validation Loss: 6.28909151604835
Batch 30: training loss 5.971020698547363
Batch 60: training loss 5.893014430999756
Batch 90: training loss 6.098437309265137
Batch 120: training loss 6.029841423034668
Batch 150: training loss 5.888813018798828
Batch 180: training loss 6.042661190032959
Batch 210: training loss 6.051496982574463
Batch 240: training loss 6.0409393310546875
Batch 270: training loss 5.908939838409424
Batch 300: training loss 5.860774040222168
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.947108268737793
Batch 360: training loss 5.844203472137451
Batch 390: training loss 5.99329948425293
Batch 420: training loss 5.795236587524414
Batch 450: training loss 6.091483116149902
Batch 480: training loss 5.940436840057373
Batch 510: training loss 6.041929244995117
Batch 540: training loss 5.999693870544434
Batch 570: training loss 6.004883289337158
Batch 600: training loss 6.119963645935059
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 5: Average Training Loss: 6.008639253997803, Average Validation Loss: 6.18812377402123
Batch 30: training loss 5.824323654174805
Batch 60: training loss 5.932925701141357
Batch 90: training loss 5.928854465484619
Batch 120: training loss 5.812979698181152
Batch 150: training loss 6.070033550262451
Batch 180: training loss 6.022838592529297
Batch 210: training loss 5.990284442901611
Batch 240: training loss 6.083639621734619
Batch 270: training loss 5.8547563552856445
Batch 300: training loss 5.943909645080566
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.883970737457275
Batch 360: training loss 5.746056079864502
Batch 390: training loss 5.970678806304932
Batch 420: training loss 5.783256530761719
Batch 450: training loss 5.988677501678467
Batch 480: training loss 5.893842697143555
Batch 510: training loss 6.020551681518555
Batch 540: training loss 6.022470474243164
Batch 570: training loss 5.716516971588135
Batch 600: training loss 5.668867111206055
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 6: Average Training Loss: 5.914053961181641, Average Validation Loss: 6.107829357715363
Batch 30: training loss 5.8877058029174805
Batch 60: training loss 5.981490135192871
Batch 90: training loss 6.208690643310547
Batch 120: training loss 5.815081596374512
Batch 150: training loss 5.635769844055176
Batch 180: training loss 5.7862677574157715
Batch 210: training loss 5.830382347106934
Batch 240: training loss 5.933723449707031
Batch 270: training loss 5.734147071838379
Batch 300: training loss 5.688377380371094
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.863080978393555
Batch 360: training loss 5.892139434814453
Batch 390: training loss 5.883091926574707
Batch 420: training loss 5.804929733276367
Batch 450: training loss 5.840280055999756
Batch 480: training loss 5.751730918884277
Batch 510: training loss 5.8349690437316895
Batch 540: training loss 5.888046741485596
Batch 570: training loss 5.7359209060668945
Batch 600: training loss 5.816826820373535
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 7: Average Training Loss: 5.828568942260742, Average Validation Loss: 6.0145457348925
Batch 30: training loss 5.964666366577148
Batch 60: training loss 5.779537200927734
Batch 90: training loss 5.912577152252197
Batch 120: training loss 5.8456878662109375
Batch 150: training loss 5.834066390991211
Batch 180: training loss 5.818795204162598
Batch 210: training loss 5.732011318206787
Batch 240: training loss 5.621506690979004
Batch 270: training loss 5.620835304260254
Batch 300: training loss 5.763803005218506
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.622035980224609
Batch 360: training loss 5.962975025177002
Batch 390: training loss 5.58928108215332
Batch 420: training loss 5.651889324188232
Batch 450: training loss 5.772651672363281
Batch 480: training loss 5.696235179901123
Batch 510: training loss 5.690528869628906
Batch 540: training loss 5.588791370391846
Batch 570: training loss 5.87703800201416
Batch 600: training loss 5.592605113983154
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 8: Average Training Loss: 5.75067169418335, Average Validation Loss: 5.95824342078351
Batch 30: training loss 5.6123833656311035
Batch 60: training loss 5.723153114318848
Batch 90: training loss 5.783557891845703
Batch 120: training loss 5.737939834594727
Batch 150: training loss 5.651105880737305
Batch 180: training loss 5.777714252471924
Batch 210: training loss 5.663735866546631
Batch 240: training loss 5.688485622406006
Batch 270: training loss 5.786868572235107
Batch 300: training loss 5.65184211730957
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.525976181030273
Batch 360: training loss 5.818470478057861
Batch 390: training loss 5.6359357833862305
Batch 420: training loss 5.489532947540283
Batch 450: training loss 5.651188373565674
Batch 480: training loss 5.847829818725586
Batch 510: training loss 5.640376091003418
Batch 540: training loss 5.580409049987793
Batch 570: training loss 5.455221652984619
Batch 600: training loss 5.624726295471191
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 9: Average Training Loss: 5.679718785095215, Average Validation Loss: 5.879538383889706
Batch 30: training loss 5.622318267822266
Batch 60: training loss 5.4554572105407715
Batch 90: training loss 5.589365482330322
Batch 120: training loss 5.582892417907715
Batch 150: training loss 5.62096643447876
Batch 180: training loss 5.711224555969238
Batch 210: training loss 5.673337936401367
Batch 240: training loss 5.717771530151367
Batch 270: training loss 5.615882396697998
Batch 300: training loss 5.743894577026367
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.566531181335449
Batch 360: training loss 5.555541038513184
Batch 390: training loss 5.652637481689453
Batch 420: training loss 5.60568380355835
Batch 450: training loss 5.5041913986206055
Batch 480: training loss 5.583365440368652
Batch 510: training loss 5.630849361419678
Batch 540: training loss 5.542917251586914
Batch 570: training loss 5.501334190368652
Batch 600: training loss 5.563840866088867
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 10: Average Training Loss: 5.613133487701416, Average Validation Loss: 5.832339966550786
Batch 30: training loss 5.516506671905518
Batch 60: training loss 5.508800983428955
Batch 90: training loss 5.655318260192871
Batch 120: training loss 5.492025375366211
Batch 150: training loss 5.346057891845703
Batch 180: training loss 5.381447792053223
Batch 210: training loss 5.558638095855713
Batch 240: training loss 5.551455497741699
Batch 270: training loss 5.513603210449219
Batch 300: training loss 5.6154866218566895
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.7609758377075195
Batch 360: training loss 5.609324932098389
Batch 390: training loss 5.514293670654297
Batch 420: training loss 5.70753288269043
Batch 450: training loss 5.441611289978027
Batch 480: training loss 5.501423358917236
Batch 510: training loss 5.387529373168945
Batch 540: training loss 5.589277744293213
Batch 570: training loss 5.487216949462891
Batch 600: training loss 5.5408525466918945
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 11: Average Training Loss: 5.549938690948486, Average Validation Loss: 5.77611542762594
Batch 30: training loss 5.533370494842529
Batch 60: training loss 5.645224571228027
Batch 90: training loss 5.499342441558838
Batch 120: training loss 5.488163471221924
Batch 150: training loss 5.447322845458984
Batch 180: training loss 5.378957271575928
Batch 210: training loss 5.609485626220703
Batch 240: training loss 5.631199836730957
Batch 270: training loss 5.471360683441162
Batch 300: training loss 5.436514377593994
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.47486686706543
Batch 360: training loss 5.740025997161865
Batch 390: training loss 5.401292324066162
Batch 420: training loss 5.5139946937561035
Batch 450: training loss 5.445727825164795
Batch 480: training loss 5.488480567932129
Batch 510: training loss 5.52760648727417
Batch 540: training loss 5.416604995727539
Batch 570: training loss 5.403215408325195
Batch 600: training loss 5.490591049194336
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 12: Average Training Loss: 5.492857954406738, Average Validation Loss: 5.724651083033136
Batch 30: training loss 5.373044490814209
Batch 60: training loss 5.455244064331055
Batch 90: training loss 5.284994602203369
Batch 120: training loss 5.593210220336914
Batch 150: training loss 5.4537672996521
Batch 180: training loss 5.31431770324707
Batch 210: training loss 5.216829299926758
Batch 240: training loss 5.347780704498291
Batch 270: training loss 5.423944473266602
Batch 300: training loss 5.428422451019287
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.513729095458984
Batch 360: training loss 5.529098033905029
Batch 390: training loss 5.383203506469727
Batch 420: training loss 5.296091079711914
Batch 450: training loss 5.324378967285156
Batch 480: training loss 5.492212295532227
Batch 510: training loss 5.611150741577148
Batch 540: training loss 5.496094703674316
Batch 570: training loss 5.484983921051025
Batch 600: training loss 5.457993507385254
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 13: Average Training Loss: 5.43933412399292, Average Validation Loss: 5.680593257254743
Batch 30: training loss 5.320111274719238
Batch 60: training loss 5.467032432556152
Batch 90: training loss 5.327597618103027
Batch 120: training loss 5.46769905090332
Batch 150: training loss 5.509632110595703
Batch 180: training loss 5.4310102462768555
Batch 210: training loss 5.538242816925049
Batch 240: training loss 5.268707275390625
Batch 270: training loss 5.221803188323975
Batch 300: training loss 5.299013137817383
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.59851598739624
Batch 360: training loss 5.465397834777832
Batch 390: training loss 5.244899749755859
Batch 420: training loss 5.224726676940918
Batch 450: training loss 5.426340579986572
Batch 480: training loss 5.3796892166137695
Batch 510: training loss 5.4257097244262695
Batch 540: training loss 5.3033552169799805
Batch 570: training loss 5.436140537261963
Batch 600: training loss 5.591317176818848
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 14: Average Training Loss: 5.388634809875488, Average Validation Loss: 5.64057690031985
Batch 30: training loss 5.222779273986816
Batch 60: training loss 5.298205375671387
Batch 90: training loss 5.503743648529053
Batch 120: training loss 5.206541061401367
Batch 150: training loss 5.187722682952881
Batch 180: training loss 5.323694705963135
Batch 210: training loss 5.343713760375977
Batch 240: training loss 5.2007646560668945
Batch 270: training loss 5.340903282165527
Batch 300: training loss 5.472631454467773
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.287622451782227
Batch 360: training loss 5.345245838165283
Batch 390: training loss 5.440868377685547
Batch 420: training loss 5.419103622436523
Batch 450: training loss 5.407711029052734
Batch 480: training loss 5.38662576675415
Batch 510: training loss 5.303775787353516
Batch 540: training loss 5.499517917633057
Batch 570: training loss 5.321174621582031
Batch 600: training loss 5.392092227935791
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 15: Average Training Loss: 5.342405759429932, Average Validation Loss: 5.600101166583122
Batch 30: training loss 5.266172409057617
Batch 60: training loss 5.299236297607422
Batch 90: training loss 5.283542633056641
Batch 120: training loss 5.284151077270508
Batch 150: training loss 5.192105293273926
Batch 180: training loss 5.487693786621094
Batch 210: training loss 5.233522415161133
Batch 240: training loss 5.226176738739014
Batch 270: training loss 5.377716541290283
Batch 300: training loss 5.3653669357299805
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.322598457336426
Batch 360: training loss 5.30908727645874
Batch 390: training loss 5.113405227661133
Batch 420: training loss 5.414486885070801
Batch 450: training loss 5.350290775299072
Batch 480: training loss 5.224006652832031
Batch 510: training loss 5.375157833099365
Batch 540: training loss 5.2339863777160645
Batch 570: training loss 5.141793727874756
Batch 600: training loss 5.2564239501953125
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 16: Average Training Loss: 5.297138635253906, Average Validation Loss: 5.575576396698647
Batch 30: training loss 5.449736595153809
Batch 60: training loss 5.310555458068848
Batch 90: training loss 5.168931007385254
Batch 120: training loss 5.425047874450684
Batch 150: training loss 5.145513534545898
Batch 180: training loss 5.335141658782959
Batch 210: training loss 5.261292457580566
Batch 240: training loss 5.275715351104736
Batch 270: training loss 5.2400970458984375
Batch 300: training loss 5.249772071838379
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.26820182800293
Batch 360: training loss 5.1961140632629395
Batch 390: training loss 5.176876068115234
Batch 420: training loss 5.381899833679199
Batch 450: training loss 5.4427690505981445
Batch 480: training loss 5.147077560424805
Batch 510: training loss 5.30206298828125
Batch 540: training loss 5.146171569824219
Batch 570: training loss 5.204414367675781
Batch 600: training loss 5.098927974700928
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 17: Average Training Loss: 5.255566199493408, Average Validation Loss: 5.541406692342555
Batch 30: training loss 5.251521110534668
Batch 60: training loss 5.141779899597168
Batch 90: training loss 5.32022762298584
Batch 120: training loss 5.139080047607422
Batch 150: training loss 5.131315231323242
Batch 180: training loss 5.243528366088867
Batch 210: training loss 5.192539215087891
Batch 240: training loss 5.213335990905762
Batch 270: training loss 5.246592998504639
Batch 300: training loss 5.264687538146973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.228687763214111
Batch 360: training loss 5.313884735107422
Batch 390: training loss 5.247054100036621
Batch 420: training loss 5.13469934463501
Batch 450: training loss 5.148809432983398
Batch 480: training loss 5.16493034362793
Batch 510: training loss 5.200028419494629
Batch 540: training loss 5.341533660888672
Batch 570: training loss 5.260858058929443
Batch 600: training loss 5.181539058685303
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 18: Average Training Loss: 5.216386051177978, Average Validation Loss: 5.514085363834463
Batch 30: training loss 5.1717071533203125
Batch 60: training loss 5.316636085510254
Batch 90: training loss 5.076906681060791
Batch 120: training loss 5.182643890380859
Batch 150: training loss 5.027945518493652
Batch 180: training loss 5.265449523925781
Batch 210: training loss 5.142999649047852
Batch 240: training loss 5.205024719238281
Batch 270: training loss 5.087947845458984
Batch 300: training loss 5.295629024505615
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.368559837341309
Batch 360: training loss 5.212081432342529
Batch 390: training loss 5.323925495147705
Batch 420: training loss 5.1586689949035645
Batch 450: training loss 5.214884281158447
Batch 480: training loss 5.115044116973877
Batch 510: training loss 5.096536159515381
Batch 540: training loss 5.243912220001221
Batch 570: training loss 5.247320175170898
Batch 600: training loss 5.108346462249756
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 19: Average Training Loss: 5.178448220062256, Average Validation Loss: 5.48434018074198
Batch 30: training loss 5.321610927581787
Batch 60: training loss 5.418740749359131
Batch 90: training loss 5.03409481048584
Batch 120: training loss 5.283493995666504
Batch 150: training loss 5.231994152069092
Batch 180: training loss 5.170768737792969
Batch 210: training loss 5.103856086730957
Batch 240: training loss 5.313327789306641
Batch 270: training loss 5.436954498291016
Batch 300: training loss 4.971694469451904
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.092145919799805
Batch 360: training loss 5.139647483825684
Batch 390: training loss 5.067598819732666
Batch 420: training loss 5.094447135925293
Batch 450: training loss 5.219509124755859
Batch 480: training loss 5.164762020111084
Batch 510: training loss 5.17057466506958
Batch 540: training loss 5.232000827789307
Batch 570: training loss 5.164056777954102
Batch 600: training loss 4.888410568237305
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 20: Average Training Loss: 5.143928730010987, Average Validation Loss: 5.451536381498296
Batch 30: training loss 5.27021598815918
Batch 60: training loss 5.087761402130127
Batch 90: training loss 4.955716609954834
Batch 120: training loss 5.389168739318848
Batch 150: training loss 5.3499250411987305
Batch 180: training loss 5.1100754737854
Batch 210: training loss 5.11952018737793
Batch 240: training loss 5.040308475494385
Batch 270: training loss 5.024419784545898
Batch 300: training loss 5.044864177703857
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.147294998168945
Batch 360: training loss 5.006574630737305
Batch 390: training loss 5.197898864746094
Batch 420: training loss 5.292106628417969
Batch 450: training loss 5.006330966949463
Batch 480: training loss 5.153350830078125
Batch 510: training loss 5.094019889831543
Batch 540: training loss 5.092822551727295
Batch 570: training loss 5.224484443664551
Batch 600: training loss 5.18778133392334
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 21: Average Training Loss: 5.109884044647217, Average Validation Loss: 5.43510809350521
Batch 30: training loss 5.071578025817871
Batch 60: training loss 5.080953121185303
Batch 90: training loss 5.172151565551758
Batch 120: training loss 5.067084312438965
Batch 150: training loss 5.040924072265625
Batch 180: training loss 5.210103988647461
Batch 210: training loss 5.027626037597656
Batch 240: training loss 5.180400371551514
Batch 270: training loss 5.127584457397461
Batch 300: training loss 5.027774333953857
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.011425018310547
Batch 360: training loss 5.059970855712891
Batch 390: training loss 4.974620819091797
Batch 420: training loss 4.9742751121521
Batch 450: training loss 4.970688819885254
Batch 480: training loss 5.149996757507324
Batch 510: training loss 5.395942687988281
Batch 540: training loss 5.024326324462891
Batch 570: training loss 5.100078582763672
Batch 600: training loss 4.910924911499023
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 22: Average Training Loss: 5.078925975036621, Average Validation Loss: 5.399885664594934
Batch 30: training loss 4.913959980010986
Batch 60: training loss 4.915957450866699
Batch 90: training loss 4.962488651275635
Batch 120: training loss 5.042028903961182
Batch 150: training loss 4.857588768005371
Batch 180: training loss 5.115293502807617
Batch 210: training loss 4.968470096588135
Batch 240: training loss 5.116022109985352
Batch 270: training loss 5.0758585929870605
Batch 300: training loss 5.206912040710449
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.9334025382995605
Batch 360: training loss 4.909854412078857
Batch 390: training loss 5.088736534118652
Batch 420: training loss 4.881772518157959
Batch 450: training loss 5.048672199249268
Batch 480: training loss 5.242546558380127
Batch 510: training loss 5.006667137145996
Batch 540: training loss 5.123218059539795
Batch 570: training loss 5.190966606140137
Batch 600: training loss 5.121426582336426
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 23: Average Training Loss: 5.047729605865478, Average Validation Loss: 5.377418649957535
Batch 30: training loss 5.0200324058532715
Batch 60: training loss 4.88344669342041
Batch 90: training loss 5.220469951629639
Batch 120: training loss 5.032147407531738
Batch 150: training loss 5.025049686431885
Batch 180: training loss 4.912290573120117
Batch 210: training loss 5.045782566070557
Batch 240: training loss 4.9643235206604
Batch 270: training loss 5.0656208992004395
Batch 300: training loss 4.9483642578125
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.999322414398193
Batch 360: training loss 5.084043502807617
Batch 390: training loss 5.032876968383789
Batch 420: training loss 5.016559600830078
Batch 450: training loss 4.988027095794678
Batch 480: training loss 4.935301780700684
Batch 510: training loss 5.071959018707275
Batch 540: training loss 5.199183464050293
Batch 570: training loss 5.081358432769775
Batch 600: training loss 4.9576311111450195
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 24: Average Training Loss: 5.018685684967041, Average Validation Loss: 5.366729817491897
Batch 30: training loss 4.923559188842773
Batch 60: training loss 4.918949604034424
Batch 90: training loss 4.82318115234375
Batch 120: training loss 4.873575210571289
Batch 150: training loss 4.950733661651611
Batch 180: training loss 4.972789764404297
Batch 210: training loss 4.916149139404297
Batch 240: training loss 5.0617475509643555
Batch 270: training loss 5.1225409507751465
Batch 300: training loss 4.886116981506348
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.101733684539795
Batch 360: training loss 4.8799967765808105
Batch 390: training loss 4.914538860321045
Batch 420: training loss 5.005102157592773
Batch 450: training loss 5.158648490905762
Batch 480: training loss 4.917605876922607
Batch 510: training loss 4.979674339294434
Batch 540: training loss 5.128359794616699
Batch 570: training loss 4.983667373657227
Batch 600: training loss 5.062904357910156
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 25: Average Training Loss: 4.992753350067138, Average Validation Loss: 5.338845628373166
Batch 30: training loss 4.927133083343506
Batch 60: training loss 5.0470709800720215
Batch 90: training loss 4.99530029296875
Batch 120: training loss 4.675404071807861
Batch 150: training loss 5.057683944702148
Batch 180: training loss 5.05340051651001
Batch 210: training loss 5.061646938323975
Batch 240: training loss 4.959433078765869
Batch 270: training loss 4.9574761390686035
Batch 300: training loss 4.83857536315918
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.88592529296875
Batch 360: training loss 4.815936088562012
Batch 390: training loss 5.178473472595215
Batch 420: training loss 4.848424911499023
Batch 450: training loss 4.918929100036621
Batch 480: training loss 5.100962162017822
Batch 510: training loss 5.124157905578613
Batch 540: training loss 4.834205627441406
Batch 570: training loss 5.021583557128906
Batch 600: training loss 5.027219772338867
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 26: Average Training Loss: 4.966341038513184, Average Validation Loss: 5.320050767127504
Batch 30: training loss 4.890101432800293
Batch 60: training loss 4.941789627075195
Batch 90: training loss 4.945009231567383
Batch 120: training loss 5.108179569244385
Batch 150: training loss 4.836516380310059
Batch 180: training loss 5.010585784912109
Batch 210: training loss 4.835630416870117
Batch 240: training loss 5.004979133605957
Batch 270: training loss 4.904382705688477
Batch 300: training loss 4.923104763031006
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.884951591491699
Batch 360: training loss 4.972853183746338
Batch 390: training loss 5.074264049530029
Batch 420: training loss 5.31074333190918
Batch 450: training loss 4.915015697479248
Batch 480: training loss 4.862166404724121
Batch 510: training loss 4.799609184265137
Batch 540: training loss 4.9423041343688965
Batch 570: training loss 4.914054870605469
Batch 600: training loss 5.0189313888549805
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 27: Average Training Loss: 4.941608395385742, Average Validation Loss: 5.2939029348657485
Batch 30: training loss 4.874955654144287
Batch 60: training loss 4.797806739807129
Batch 90: training loss 4.891374588012695
Batch 120: training loss 4.899476051330566
Batch 150: training loss 4.79193115234375
Batch 180: training loss 4.8783745765686035
Batch 210: training loss 4.751704216003418
Batch 240: training loss 5.0152411460876465
Batch 270: training loss 4.795009613037109
Batch 300: training loss 4.970950603485107
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.802365303039551
Batch 360: training loss 4.89769983291626
Batch 390: training loss 4.944875717163086
Batch 420: training loss 4.851248741149902
Batch 450: training loss 4.830846309661865
Batch 480: training loss 4.887653827667236
Batch 510: training loss 4.81806755065918
Batch 540: training loss 4.947904586791992
Batch 570: training loss 4.856597900390625
Batch 600: training loss 4.801081657409668
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 28: Average Training Loss: 4.916762270355225, Average Validation Loss: 5.28733729301615
Batch 30: training loss 4.763828754425049
Batch 60: training loss 4.748323917388916
Batch 90: training loss 4.845498085021973
Batch 120: training loss 4.898046970367432
Batch 150: training loss 4.989453315734863
Batch 180: training loss 4.651050567626953
Batch 210: training loss 4.782378196716309
Batch 240: training loss 4.972099781036377
Batch 270: training loss 4.799195766448975
Batch 300: training loss 4.893006801605225
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.91921854019165
Batch 360: training loss 4.76605749130249
Batch 390: training loss 4.7591142654418945
Batch 420: training loss 4.769497871398926
Batch 450: training loss 4.773042678833008
Batch 480: training loss 4.754020690917969
Batch 510: training loss 4.830426216125488
Batch 540: training loss 5.026575565338135
Batch 570: training loss 4.859872341156006
Batch 600: training loss 4.797019958496094
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 29: Average Training Loss: 4.8954916778564455, Average Validation Loss: 5.266642712532206
Batch 30: training loss 4.963512420654297
Batch 60: training loss 5.040261268615723
Batch 90: training loss 4.904094696044922
Batch 120: training loss 4.825235366821289
Batch 150: training loss 4.960323333740234
Batch 180: training loss 4.810085296630859
Batch 210: training loss 4.936283111572266
Batch 240: training loss 5.133241653442383
Batch 270: training loss 4.837092876434326
Batch 300: training loss 4.777199745178223
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.95048713684082
Batch 360: training loss 4.977705478668213
Batch 390: training loss 4.902983665466309
Batch 420: training loss 4.728919982910156
Batch 450: training loss 4.968497276306152
Batch 480: training loss 4.72743034362793
Batch 510: training loss 4.763864994049072
Batch 540: training loss 4.763082504272461
Batch 570: training loss 4.943665504455566
Batch 600: training loss 4.7292094230651855
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 30: Average Training Loss: 4.872400388336182, Average Validation Loss: 5.248478473501002
Batch 30: training loss 4.787912845611572
Batch 60: training loss 4.666750907897949
Batch 90: training loss 5.081965446472168
Batch 120: training loss 4.93583345413208
Batch 150: training loss 4.706526279449463
Batch 180: training loss 4.879359245300293
Batch 210: training loss 4.644694805145264
Batch 240: training loss 4.9104132652282715
Batch 270: training loss 4.793317794799805
Batch 300: training loss 4.938098907470703
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.989073753356934
Batch 360: training loss 4.854436874389648
Batch 390: training loss 4.701360702514648
Batch 420: training loss 4.756995677947998
Batch 450: training loss 4.916221618652344
Batch 480: training loss 4.835420608520508
Batch 510: training loss 4.832536697387695
Batch 540: training loss 4.812459945678711
Batch 570: training loss 4.948490142822266
Batch 600: training loss 4.7786431312561035
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 31: Average Training Loss: 4.851134992218017, Average Validation Loss: 5.233987798082068
Batch 30: training loss 4.895450592041016
Batch 60: training loss 4.753690242767334
Batch 90: training loss 4.781963348388672
Batch 120: training loss 5.082388877868652
Batch 150: training loss 4.807926177978516
Batch 180: training loss 4.73873233795166
Batch 210: training loss 4.782149791717529
Batch 240: training loss 5.096592903137207
Batch 270: training loss 4.849246978759766
Batch 300: training loss 4.796509265899658
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 5.031689643859863
Batch 360: training loss 4.893949508666992
Batch 390: training loss 4.773674488067627
Batch 420: training loss 4.846784591674805
Batch 450: training loss 4.805474758148193
Batch 480: training loss 4.586668968200684
Batch 510: training loss 4.870206832885742
Batch 540: training loss 4.852961540222168
Batch 570: training loss 4.965741157531738
Batch 600: training loss 4.78170108795166
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 32: Average Training Loss: 4.830486655426025, Average Validation Loss: 5.222676916325346
Batch 30: training loss 4.810492515563965
Batch 60: training loss 4.8256940841674805
Batch 90: training loss 4.77162504196167
Batch 120: training loss 4.678123474121094
Batch 150: training loss 4.765316963195801
Batch 180: training loss 4.697028160095215
Batch 210: training loss 4.780709266662598
Batch 240: training loss 4.7892255783081055
Batch 270: training loss 4.80811882019043
Batch 300: training loss 4.913376331329346
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.72686767578125
Batch 360: training loss 4.789547920227051
Batch 390: training loss 4.938336372375488
Batch 420: training loss 4.90420389175415
Batch 450: training loss 4.701476573944092
Batch 480: training loss 4.760782241821289
Batch 510: training loss 4.814217567443848
Batch 540: training loss 4.822169303894043
Batch 570: training loss 5.005904674530029
Batch 600: training loss 4.74302339553833
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 33: Average Training Loss: 4.810976934051514, Average Validation Loss: 5.203037292399305
Batch 30: training loss 4.813779830932617
Batch 60: training loss 4.808990001678467
Batch 90: training loss 4.839590072631836
Batch 120: training loss 4.614299774169922
Batch 150: training loss 4.955261707305908
Batch 180: training loss 4.729784965515137
Batch 210: training loss 4.908624649047852
Batch 240: training loss 4.7692694664001465
Batch 270: training loss 4.600440979003906
Batch 300: training loss 4.891590595245361
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.832152366638184
Batch 360: training loss 4.749549388885498
Batch 390: training loss 4.890795707702637
Batch 420: training loss 4.790309906005859
Batch 450: training loss 4.673630714416504
Batch 480: training loss 4.969542503356934
Batch 510: training loss 4.813105583190918
Batch 540: training loss 4.934744358062744
Batch 570: training loss 4.8872785568237305
Batch 600: training loss 4.799281597137451
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 34: Average Training Loss: 4.792684407806396, Average Validation Loss: 5.187033764859463
Batch 30: training loss 4.64348030090332
Batch 60: training loss 4.8783955574035645
Batch 90: training loss 4.7485551834106445
Batch 120: training loss 4.773812770843506
Batch 150: training loss 4.779341220855713
Batch 180: training loss 5.007967472076416
Batch 210: training loss 4.965327739715576
Batch 240: training loss 5.064006805419922
Batch 270: training loss 4.659934043884277
Batch 300: training loss 4.797235012054443
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.822428226470947
Batch 360: training loss 4.861429691314697
Batch 390: training loss 4.641078948974609
Batch 420: training loss 4.880537509918213
Batch 450: training loss 4.840699672698975
Batch 480: training loss 5.027078628540039
Batch 510: training loss 4.75184965133667
Batch 540: training loss 4.713417053222656
Batch 570: training loss 4.920705795288086
Batch 600: training loss 4.7117791175842285
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 35: Average Training Loss: 4.772604737854004, Average Validation Loss: 5.172436095298605
Batch 30: training loss 4.732119083404541
Batch 60: training loss 4.6929497718811035
Batch 90: training loss 4.661386489868164
Batch 120: training loss 4.851983070373535
Batch 150: training loss 4.689148902893066
Batch 180: training loss 4.891847610473633
Batch 210: training loss 4.6834516525268555
Batch 240: training loss 4.915372848510742
Batch 270: training loss 4.65573787689209
Batch 300: training loss 4.641173362731934
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.809680461883545
Batch 360: training loss 4.744603633880615
Batch 390: training loss 4.745208740234375
Batch 420: training loss 4.682668209075928
Batch 450: training loss 4.8084635734558105
Batch 480: training loss 4.779123783111572
Batch 510: training loss 4.648170471191406
Batch 540: training loss 4.739221572875977
Batch 570: training loss 4.735286712646484
Batch 600: training loss 4.927129745483398
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 36: Average Training Loss: 4.7558359375, Average Validation Loss: 5.155896410028985
Batch 30: training loss 4.572336673736572
Batch 60: training loss 4.846586227416992
Batch 90: training loss 5.003023624420166
Batch 120: training loss 4.909615516662598
Batch 150: training loss 4.673038005828857
Batch 180: training loss 4.592719078063965
Batch 210: training loss 4.75482177734375
Batch 240: training loss 4.898317813873291
Batch 270: training loss 4.769083023071289
Batch 300: training loss 4.794286727905273
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.787204742431641
Batch 360: training loss 4.7089972496032715
Batch 390: training loss 4.568056583404541
Batch 420: training loss 4.85526704788208
Batch 450: training loss 4.611438274383545
Batch 480: training loss 4.661613941192627
Batch 510: training loss 4.74387788772583
Batch 540: training loss 4.711054801940918
Batch 570: training loss 4.781999588012695
Batch 600: training loss 4.84044075012207
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 37: Average Training Loss: 4.738219659423828, Average Validation Loss: 5.144943521377888
Batch 30: training loss 4.69445276260376
Batch 60: training loss 4.598299026489258
Batch 90: training loss 4.844503402709961
Batch 120: training loss 4.955944538116455
Batch 150: training loss 4.615857124328613
Batch 180: training loss 4.607161045074463
Batch 210: training loss 4.761655807495117
Batch 240: training loss 4.664021015167236
Batch 270: training loss 4.797271251678467
Batch 300: training loss 4.820608139038086
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.612163066864014
Batch 360: training loss 4.635280132293701
Batch 390: training loss 4.624200344085693
Batch 420: training loss 4.7532219886779785
Batch 450: training loss 4.849400997161865
Batch 480: training loss 4.587538242340088
Batch 510: training loss 4.710175514221191
Batch 540: training loss 4.653789520263672
Batch 570: training loss 4.7856292724609375
Batch 600: training loss 4.696407318115234
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 38: Average Training Loss: 4.7220046516418455, Average Validation Loss: 5.125763406144812
Batch 30: training loss 4.521302700042725
Batch 60: training loss 4.538864612579346
Batch 90: training loss 4.809768199920654
Batch 120: training loss 4.792576313018799
Batch 150: training loss 4.703717231750488
Batch 180: training loss 4.6499223709106445
Batch 210: training loss 4.7340288162231445
Batch 240: training loss 4.627713680267334
Batch 270: training loss 5.196023941040039
Batch 300: training loss 4.479674816131592
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.674574851989746
Batch 360: training loss 4.916784286499023
Batch 390: training loss 4.6442060470581055
Batch 420: training loss 4.671374320983887
Batch 450: training loss 4.702322959899902
Batch 480: training loss 4.835545539855957
Batch 510: training loss 5.149066925048828
Batch 540: training loss 4.615573883056641
Batch 570: training loss 4.563347816467285
Batch 600: training loss 4.773562431335449
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 39: Average Training Loss: 4.705981886291504, Average Validation Loss: 5.11985507924506
Batch 30: training loss 4.690996170043945
Batch 60: training loss 4.574153423309326
Batch 90: training loss 4.493289947509766
Batch 120: training loss 4.668385028839111
Batch 150: training loss 4.5966386795043945
Batch 180: training loss 4.6455912590026855
Batch 210: training loss 4.724016189575195
Batch 240: training loss 4.622114181518555
Batch 270: training loss 4.64863920211792
Batch 300: training loss 4.5870256423950195
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.47874116897583
Batch 360: training loss 4.658255577087402
Batch 390: training loss 4.654667377471924
Batch 420: training loss 4.678741455078125
Batch 450: training loss 4.842778205871582
Batch 480: training loss 4.658638000488281
Batch 510: training loss 4.800299644470215
Batch 540: training loss 4.543586730957031
Batch 570: training loss 4.713135719299316
Batch 600: training loss 4.64962100982666
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 40: Average Training Loss: 4.690037470245361, Average Validation Loss: 5.105905076290699
Batch 30: training loss 4.754632472991943
Batch 60: training loss 4.687215328216553
Batch 90: training loss 4.517428874969482
Batch 120: training loss 4.575608730316162
Batch 150: training loss 4.506622791290283
Batch 180: training loss 4.606520652770996
Batch 210: training loss 4.641810417175293
Batch 240: training loss 4.601256370544434
Batch 270: training loss 4.57581901550293
Batch 300: training loss 4.635788917541504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.678355693817139
Batch 360: training loss 4.647968292236328
Batch 390: training loss 4.676713943481445
Batch 420: training loss 4.706998825073242
Batch 450: training loss 4.576618194580078
Batch 480: training loss 4.87814998626709
Batch 510: training loss 4.784690856933594
Batch 540: training loss 4.668605804443359
Batch 570: training loss 4.5828094482421875
Batch 600: training loss 4.670833110809326
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 41: Average Training Loss: 4.673738234710694, Average Validation Loss: 5.1007819175720215
Batch 30: training loss 4.523148536682129
Batch 60: training loss 4.660029888153076
Batch 90: training loss 4.753147125244141
Batch 120: training loss 4.6056060791015625
Batch 150: training loss 4.596437454223633
Batch 180: training loss 4.55686092376709
Batch 210: training loss 4.58477783203125
Batch 240: training loss 4.753209590911865
Batch 270: training loss 4.764976501464844
Batch 300: training loss 4.638030529022217
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.6080145835876465
Batch 360: training loss 4.705970764160156
Batch 390: training loss 4.536492347717285
Batch 420: training loss 4.595550537109375
Batch 450: training loss 4.6728949546813965
Batch 480: training loss 4.680421829223633
Batch 510: training loss 4.598077297210693
Batch 540: training loss 4.485915660858154
Batch 570: training loss 4.727249622344971
Batch 600: training loss 4.666134357452393
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 42: Average Training Loss: 4.659759728240966, Average Validation Loss: 5.087997852487767
Batch 30: training loss 4.687502861022949
Batch 60: training loss 4.752524375915527
Batch 90: training loss 4.792008399963379
Batch 120: training loss 4.616171836853027
Batch 150: training loss 4.779251575469971
Batch 180: training loss 4.775540828704834
Batch 210: training loss 4.489448070526123
Batch 240: training loss 4.540245532989502
Batch 270: training loss 4.711460113525391
Batch 300: training loss 4.685111999511719
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.774697303771973
Batch 360: training loss 4.592698574066162
Batch 390: training loss 4.625188827514648
Batch 420: training loss 4.954690456390381
Batch 450: training loss 4.559179306030273
Batch 480: training loss 4.528018474578857
Batch 510: training loss 4.554210662841797
Batch 540: training loss 4.762451648712158
Batch 570: training loss 4.55027437210083
Batch 600: training loss 4.503035545349121
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 43: Average Training Loss: 4.644380558013916, Average Validation Loss: 5.075326564464163
Batch 30: training loss 4.2988080978393555
Batch 60: training loss 4.520242691040039
Batch 90: training loss 4.551939964294434
Batch 120: training loss 4.735079765319824
Batch 150: training loss 4.466373443603516
Batch 180: training loss 4.596485137939453
Batch 210: training loss 4.536920070648193
Batch 240: training loss 4.628396511077881
Batch 270: training loss 4.8147993087768555
Batch 300: training loss 4.642477512359619
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.758411407470703
Batch 360: training loss 4.597124099731445
Batch 390: training loss 4.694169044494629
Batch 420: training loss 4.622976303100586
Batch 450: training loss 4.445797920227051
Batch 480: training loss 4.5355544090271
Batch 510: training loss 4.584245681762695
Batch 540: training loss 4.618379592895508
Batch 570: training loss 4.678389072418213
Batch 600: training loss 4.672162055969238
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 44: Average Training Loss: 4.629372536468506, Average Validation Loss: 5.064935511731087
Batch 30: training loss 4.4587082862854
Batch 60: training loss 4.812282085418701
Batch 90: training loss 4.486968517303467
Batch 120: training loss 4.607491970062256
Batch 150: training loss 4.281212329864502
Batch 180: training loss 4.664886474609375
Batch 210: training loss 4.808083534240723
Batch 240: training loss 5.147752285003662
Batch 270: training loss 4.804144859313965
Batch 300: training loss 4.6592607498168945
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.585726261138916
Batch 360: training loss 4.424229621887207
Batch 390: training loss 4.542713165283203
Batch 420: training loss 4.6479034423828125
Batch 450: training loss 4.632269859313965
Batch 480: training loss 4.712298393249512
Batch 510: training loss 4.497959136962891
Batch 540: training loss 4.636630058288574
Batch 570: training loss 4.6402363777160645
Batch 600: training loss 4.636680603027344
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 45: Average Training Loss: 4.61641077041626, Average Validation Loss: 5.055911936658494
Batch 30: training loss 4.5218586921691895
Batch 60: training loss 4.480939865112305
Batch 90: training loss 4.997069358825684
Batch 120: training loss 4.53286075592041
Batch 150: training loss 4.610354423522949
Batch 180: training loss 4.420708656311035
Batch 210: training loss 4.712794780731201
Batch 240: training loss 4.571987152099609
Batch 270: training loss 4.398685932159424
Batch 300: training loss 4.749671459197998
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.666329860687256
Batch 360: training loss 4.668113708496094
Batch 390: training loss 4.632805824279785
Batch 420: training loss 4.559828758239746
Batch 450: training loss 4.6183013916015625
Batch 480: training loss 4.62764835357666
Batch 510: training loss 4.3026556968688965
Batch 540: training loss 4.489347457885742
Batch 570: training loss 4.527629852294922
Batch 600: training loss 4.555920600891113
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 46: Average Training Loss: 4.602983393096924, Average Validation Loss: 5.0435408125532435
Batch 30: training loss 4.618138313293457
Batch 60: training loss 4.626253128051758
Batch 90: training loss 4.6628594398498535
Batch 120: training loss 4.433279514312744
Batch 150: training loss 4.675663948059082
Batch 180: training loss 4.653339862823486
Batch 210: training loss 4.609847068786621
Batch 240: training loss 4.445886611938477
Batch 270: training loss 4.456882953643799
Batch 300: training loss 4.364598751068115
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.527483940124512
Batch 360: training loss 4.4879961013793945
Batch 390: training loss 4.650907039642334
Batch 420: training loss 4.6486711502075195
Batch 450: training loss 4.4754509925842285
Batch 480: training loss 4.653390884399414
Batch 510: training loss 4.4866533279418945
Batch 540: training loss 4.706044673919678
Batch 570: training loss 4.620262622833252
Batch 600: training loss 4.7926859855651855
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 47: Average Training Loss: 4.588118445587158, Average Validation Loss: 5.027573382600825
Batch 30: training loss 4.646359920501709
Batch 60: training loss 4.459389686584473
Batch 90: training loss 4.703755855560303
Batch 120: training loss 4.756502151489258
Batch 150: training loss 4.492512226104736
Batch 180: training loss 4.517241954803467
Batch 210: training loss 4.636816024780273
Batch 240: training loss 4.617767333984375
Batch 270: training loss 4.492972373962402
Batch 300: training loss 4.505540370941162
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.682486534118652
Batch 360: training loss 4.620051383972168
Batch 390: training loss 4.649444580078125
Batch 420: training loss 4.70223331451416
Batch 450: training loss 4.219294548034668
Batch 480: training loss 4.513900279998779
Batch 510: training loss 4.489734649658203
Batch 540: training loss 4.537779808044434
Batch 570: training loss 4.597775459289551
Batch 600: training loss 4.661201000213623
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 48: Average Training Loss: 4.577187913513184, Average Validation Loss: 5.018079138816671
Batch 30: training loss 4.514267921447754
Batch 60: training loss 4.675827980041504
Batch 90: training loss 4.401079177856445
Batch 120: training loss 4.58659553527832
Batch 150: training loss 4.635046005249023
Batch 180: training loss 4.290711402893066
Batch 210: training loss 4.53934907913208
Batch 240: training loss 4.669288635253906
Batch 270: training loss 4.668888568878174
Batch 300: training loss 4.456430435180664
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.468097686767578
Batch 360: training loss 4.718715190887451
Batch 390: training loss 4.559945583343506
Batch 420: training loss 4.8056793212890625
Batch 450: training loss 4.520566940307617
Batch 480: training loss 4.664820671081543
Batch 510: training loss 4.608493804931641
Batch 540: training loss 4.483292579650879
Batch 570: training loss 4.50258731842041
Batch 600: training loss 4.650704383850098
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 49: Average Training Loss: 4.565095871734619, Average Validation Loss: 5.012887498165699
Batch 30: training loss 4.643977165222168
Batch 60: training loss 4.569310665130615
Batch 90: training loss 4.473918914794922
Batch 120: training loss 4.619543075561523
Batch 150: training loss 4.249475955963135
Batch 180: training loss 4.4821696281433105
Batch 210: training loss 4.531620979309082
Batch 240: training loss 4.553339958190918
Batch 270: training loss 4.495488166809082
Batch 300: training loss 4.433311462402344
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.6015119552612305
Batch 360: training loss 4.537661552429199
Batch 390: training loss 4.470273017883301
Batch 420: training loss 4.647997856140137
Batch 450: training loss 5.003793716430664
Batch 480: training loss 4.593800067901611
Batch 510: training loss 4.607715129852295
Batch 540: training loss 4.562191009521484
Batch 570: training loss 4.5277228355407715
Batch 600: training loss 4.409130096435547
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 50: Average Training Loss: 4.5521524681091305, Average Validation Loss: 5.002824631143124
Batch 30: training loss 4.752667427062988
Batch 60: training loss 4.57255220413208
Batch 90: training loss 4.475338935852051
Batch 120: training loss 4.730050563812256
Batch 150: training loss 4.4740753173828125
Batch 180: training loss 4.478196144104004
Batch 210: training loss 4.688241481781006
Batch 240: training loss 4.475188732147217
Batch 270: training loss 4.347562789916992
Batch 300: training loss 4.5050859451293945
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.4085869789123535
Batch 360: training loss 4.5950164794921875
Batch 390: training loss 4.557470321655273
Batch 420: training loss 4.624910354614258
Batch 450: training loss 4.622028350830078
Batch 480: training loss 4.3846917152404785
Batch 510: training loss 4.496817588806152
Batch 540: training loss 4.35677433013916
Batch 570: training loss 4.627433776855469
Batch 600: training loss 4.395037651062012
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 51: Average Training Loss: 4.540528394317627, Average Validation Loss: 4.994866624791571
Batch 30: training loss 4.677718162536621
Batch 60: training loss 4.815761566162109
Batch 90: training loss 4.727446556091309
Batch 120: training loss 4.583841800689697
Batch 150: training loss 4.69452428817749
Batch 180: training loss 4.399118423461914
Batch 210: training loss 4.597946643829346
Batch 240: training loss 4.531626224517822
Batch 270: training loss 4.502877712249756
Batch 300: training loss 4.2863969802856445
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.591686248779297
Batch 360: training loss 4.6749701499938965
Batch 390: training loss 4.300037384033203
Batch 420: training loss 4.643510341644287
Batch 450: training loss 4.525631904602051
Batch 480: training loss 4.394840717315674
Batch 510: training loss 4.408909320831299
Batch 540: training loss 4.484443664550781
Batch 570: training loss 4.551802635192871
Batch 600: training loss 4.67971658706665
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 52: Average Training Loss: 4.528697428131103, Average Validation Loss: 4.983998826209535
Batch 30: training loss 4.482692718505859
Batch 60: training loss 4.508417129516602
Batch 90: training loss 4.392022609710693
Batch 120: training loss 4.5430192947387695
Batch 150: training loss 4.648183822631836
Batch 180: training loss 4.430133819580078
Batch 210: training loss 4.597345352172852
Batch 240: training loss 4.660986423492432
Batch 270: training loss 4.708061218261719
Batch 300: training loss 4.305344581604004
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.418042182922363
Batch 360: training loss 4.446793556213379
Batch 390: training loss 4.325671672821045
Batch 420: training loss 4.479215621948242
Batch 450: training loss 4.5646891593933105
Batch 480: training loss 4.47562313079834
Batch 510: training loss 4.574922561645508
Batch 540: training loss 4.5270795822143555
Batch 570: training loss 4.585281848907471
Batch 600: training loss 4.453520774841309
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 53: Average Training Loss: 4.517030284118652, Average Validation Loss: 4.9746079546339965
Batch 30: training loss 4.234396934509277
Batch 60: training loss 4.446745872497559
Batch 90: training loss 4.580047130584717
Batch 120: training loss 4.621447563171387
Batch 150: training loss 4.609753131866455
Batch 180: training loss 4.531309127807617
Batch 210: training loss 4.513397693634033
Batch 240: training loss 4.504582405090332
Batch 270: training loss 4.46566104888916
Batch 300: training loss 4.37404203414917
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.346240997314453
Batch 360: training loss 4.503950119018555
Batch 390: training loss 4.594865798950195
Batch 420: training loss 4.5361480712890625
Batch 450: training loss 4.67333984375
Batch 480: training loss 4.429079055786133
Batch 510: training loss 4.428730010986328
Batch 540: training loss 4.427992343902588
Batch 570: training loss 4.452564239501953
Batch 600: training loss 4.376842975616455
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 54: Average Training Loss: 4.504722639465332, Average Validation Loss: 4.968323971362824
Batch 30: training loss 4.562345504760742
Batch 60: training loss 4.8079328536987305
Batch 90: training loss 4.502665996551514
Batch 120: training loss 4.482229232788086
Batch 150: training loss 4.539767265319824
Batch 180: training loss 4.43056583404541
Batch 210: training loss 4.369993209838867
Batch 240: training loss 4.344725131988525
Batch 270: training loss 4.555633544921875
Batch 300: training loss 4.384733200073242
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.480129718780518
Batch 360: training loss 4.480001926422119
Batch 390: training loss 4.433623313903809
Batch 420: training loss 4.453728675842285
Batch 450: training loss 4.525182723999023
Batch 480: training loss 4.677897930145264
Batch 510: training loss 4.407991409301758
Batch 540: training loss 4.375316619873047
Batch 570: training loss 4.652514457702637
Batch 600: training loss 4.803481101989746
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 55: Average Training Loss: 4.493607753753662, Average Validation Loss: 4.964046924672228
Batch 30: training loss 4.410799980163574
Batch 60: training loss 4.42643404006958
Batch 90: training loss 4.542679309844971
Batch 120: training loss 4.3231611251831055
Batch 150: training loss 4.565073490142822
Batch 180: training loss 4.4706244468688965
Batch 210: training loss 4.512640953063965
Batch 240: training loss 4.3258490562438965
Batch 270: training loss 4.548415184020996
Batch 300: training loss 4.491237163543701
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.177498817443848
Batch 360: training loss 4.475435256958008
Batch 390: training loss 4.393095970153809
Batch 420: training loss 4.6656389236450195
Batch 450: training loss 4.428475856781006
Batch 480: training loss 4.692108631134033
Batch 510: training loss 4.427353858947754
Batch 540: training loss 4.424538612365723
Batch 570: training loss 4.569341659545898
Batch 600: training loss 4.321719646453857
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 56: Average Training Loss: 4.482352423095703, Average Validation Loss: 4.9423130928201875
Batch 30: training loss 4.665706157684326
Batch 60: training loss 4.376669406890869
Batch 90: training loss 4.383913993835449
Batch 120: training loss 4.46279764175415
Batch 150: training loss 4.562210559844971
Batch 180: training loss 4.61985445022583
Batch 210: training loss 4.486145973205566
Batch 240: training loss 4.427515029907227
Batch 270: training loss 4.441776752471924
Batch 300: training loss 4.4846272468566895
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.485081672668457
Batch 360: training loss 4.412964820861816
Batch 390: training loss 4.488705635070801
Batch 420: training loss 4.2274322509765625
Batch 450: training loss 4.280892848968506
Batch 480: training loss 4.4406914710998535
Batch 510: training loss 4.4593892097473145
Batch 540: training loss 4.523921012878418
Batch 570: training loss 4.423710823059082
Batch 600: training loss 4.419281482696533
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 57: Average Training Loss: 4.471812635803222, Average Validation Loss: 4.944689223106871
Batch 30: training loss 4.4227375984191895
Batch 60: training loss 4.448122024536133
Batch 90: training loss 4.433679103851318
Batch 120: training loss 4.3866424560546875
Batch 150: training loss 4.61700963973999
Batch 180: training loss 4.4493408203125
Batch 210: training loss 4.5694098472595215
Batch 240: training loss 4.1029582023620605
Batch 270: training loss 4.2728705406188965
Batch 300: training loss 4.601120948791504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.851205348968506
Batch 360: training loss 4.438846111297607
Batch 390: training loss 4.372569561004639
Batch 420: training loss 4.545309066772461
Batch 450: training loss 4.561887741088867
Batch 480: training loss 4.668917179107666
Batch 510: training loss 4.532004356384277
Batch 540: training loss 4.33049201965332
Batch 570: training loss 4.262772560119629
Batch 600: training loss 4.495840549468994
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 58: Average Training Loss: 4.461729986572266, Average Validation Loss: 4.927952867873172
Batch 30: training loss 4.317007064819336
Batch 60: training loss 4.45602560043335
Batch 90: training loss 4.273650646209717
Batch 120: training loss 4.328400611877441
Batch 150: training loss 4.52534294128418
Batch 180: training loss 4.645182132720947
Batch 210: training loss 4.3287034034729
Batch 240: training loss 4.3333940505981445
Batch 270: training loss 4.5475754737854
Batch 300: training loss 4.484167575836182
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.569840908050537
Batch 360: training loss 4.42190408706665
Batch 390: training loss 4.281949996948242
Batch 420: training loss 4.222410678863525
Batch 450: training loss 4.297680377960205
Batch 480: training loss 4.678493976593018
Batch 510: training loss 4.312095642089844
Batch 540: training loss 4.30657958984375
Batch 570: training loss 4.6699042320251465
Batch 600: training loss 4.4379048347473145
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 59: Average Training Loss: 4.451307427215577, Average Validation Loss: 4.92437149615998
Batch 30: training loss 4.428136825561523
Batch 60: training loss 4.286172389984131
Batch 90: training loss 4.208118438720703
Batch 120: training loss 4.434116840362549
Batch 150: training loss 4.640050888061523
Batch 180: training loss 4.693496227264404
Batch 210: training loss 4.5820159912109375
Batch 240: training loss 4.423907279968262
Batch 270: training loss 4.571292400360107
Batch 300: training loss 4.306624412536621
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.246973514556885
Batch 360: training loss 4.534160614013672
Batch 390: training loss 4.4681572914123535
Batch 420: training loss 4.508786678314209
Batch 450: training loss 4.226593971252441
Batch 480: training loss 4.610421180725098
Batch 510: training loss 4.509903907775879
Batch 540: training loss 4.367293357849121
Batch 570: training loss 4.28679895401001
Batch 600: training loss 4.4006195068359375
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 60: Average Training Loss: 4.442179818725586, Average Validation Loss: 4.918476226481985
Batch 30: training loss 4.4207282066345215
Batch 60: training loss 4.570903778076172
Batch 90: training loss 4.410517692565918
Batch 120: training loss 4.4738688468933105
Batch 150: training loss 4.511741638183594
Batch 180: training loss 4.316592216491699
Batch 210: training loss 4.446714878082275
Batch 240: training loss 4.294241905212402
Batch 270: training loss 4.40509557723999
Batch 300: training loss 4.561321258544922
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.55900764465332
Batch 360: training loss 4.65884256362915
Batch 390: training loss 4.572136402130127
Batch 420: training loss 4.623592853546143
Batch 450: training loss 4.388205051422119
Batch 480: training loss 4.57434606552124
Batch 510: training loss 4.491183757781982
Batch 540: training loss 4.249887466430664
Batch 570: training loss 4.474048614501953
Batch 600: training loss 4.327781677246094
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 61: Average Training Loss: 4.431042730712891, Average Validation Loss: 4.911718865658375
Batch 30: training loss 4.355773448944092
Batch 60: training loss 4.155880451202393
Batch 90: training loss 4.308750152587891
Batch 120: training loss 4.295511722564697
Batch 150: training loss 4.305473804473877
Batch 180: training loss 4.602785587310791
Batch 210: training loss 4.496642589569092
Batch 240: training loss 4.275488376617432
Batch 270: training loss 4.3486104011535645
Batch 300: training loss 4.448938369750977
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.484020233154297
Batch 360: training loss 4.39502477645874
Batch 390: training loss 4.487074851989746
Batch 420: training loss 4.218252182006836
Batch 450: training loss 4.383903980255127
Batch 480: training loss 4.253841876983643
Batch 510: training loss 4.263846397399902
Batch 540: training loss 4.406250953674316
Batch 570: training loss 4.46596097946167
Batch 600: training loss 4.602579593658447
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 62: Average Training Loss: 4.421540390396118, Average Validation Loss: 4.902582452652302
Batch 30: training loss 4.440820693969727
Batch 60: training loss 4.382933139801025
Batch 90: training loss 4.663625240325928
Batch 120: training loss 4.456106662750244
Batch 150: training loss 4.327551364898682
Batch 180: training loss 4.561599254608154
Batch 210: training loss 4.398750305175781
Batch 240: training loss 4.512983798980713
Batch 270: training loss 4.336420059204102
Batch 300: training loss 4.408663749694824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.460605621337891
Batch 360: training loss 4.155581951141357
Batch 390: training loss 4.431255340576172
Batch 420: training loss 4.392520904541016
Batch 450: training loss 4.280723571777344
Batch 480: training loss 4.442918300628662
Batch 510: training loss 4.296514987945557
Batch 540: training loss 4.444375991821289
Batch 570: training loss 4.387481212615967
Batch 600: training loss 4.4077534675598145
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 63: Average Training Loss: 4.411770282745361, Average Validation Loss: 4.898548075493346
Batch 30: training loss 4.261001110076904
Batch 60: training loss 4.4181013107299805
Batch 90: training loss 4.33743953704834
Batch 120: training loss 4.671282768249512
Batch 150: training loss 4.347843647003174
Batch 180: training loss 4.642059326171875
Batch 210: training loss 4.294623851776123
Batch 240: training loss 4.40191650390625
Batch 270: training loss 4.3762593269348145
Batch 300: training loss 4.363557815551758
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.370062351226807
Batch 360: training loss 4.3573174476623535
Batch 390: training loss 4.377813816070557
Batch 420: training loss 4.120186805725098
Batch 450: training loss 4.2897491455078125
Batch 480: training loss 4.507439613342285
Batch 510: training loss 4.653792381286621
Batch 540: training loss 4.242910861968994
Batch 570: training loss 4.435566425323486
Batch 600: training loss 4.600317478179932
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 64: Average Training Loss: 4.402189302062988, Average Validation Loss: 4.883797219459047
Batch 30: training loss 4.274686813354492
Batch 60: training loss 4.0804595947265625
Batch 90: training loss 4.499807357788086
Batch 120: training loss 4.452978134155273
Batch 150: training loss 4.450728893280029
Batch 180: training loss 4.642000198364258
Batch 210: training loss 4.432900905609131
Batch 240: training loss 4.546690940856934
Batch 270: training loss 4.510997772216797
Batch 300: training loss 4.500067710876465
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.540394306182861
Batch 360: training loss 4.476835250854492
Batch 390: training loss 4.349984645843506
Batch 420: training loss 4.42487907409668
Batch 450: training loss 4.514804840087891
Batch 480: training loss 4.291722297668457
Batch 510: training loss 4.452336311340332
Batch 540: training loss 4.691927909851074
Batch 570: training loss 4.410879611968994
Batch 600: training loss 4.395390033721924
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 65: Average Training Loss: 4.393135704040527, Average Validation Loss: 4.883571503010202
Batch 30: training loss 4.380786418914795
Batch 60: training loss 4.259474754333496
Batch 90: training loss 4.536838531494141
Batch 120: training loss 4.361230373382568
Batch 150: training loss 4.452462196350098
Batch 180: training loss 4.5096588134765625
Batch 210: training loss 4.306333541870117
Batch 240: training loss 4.361042022705078
Batch 270: training loss 4.211688995361328
Batch 300: training loss 4.278322219848633
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.30883264541626
Batch 360: training loss 4.306365966796875
Batch 390: training loss 4.313365936279297
Batch 420: training loss 4.508437156677246
Batch 450: training loss 4.4998345375061035
Batch 480: training loss 4.395541191101074
Batch 510: training loss 4.275265216827393
Batch 540: training loss 4.215035915374756
Batch 570: training loss 4.46516227722168
Batch 600: training loss 4.403589248657227
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 66: Average Training Loss: 4.38399273147583, Average Validation Loss: 4.872493398950455
Batch 30: training loss 4.46028995513916
Batch 60: training loss 4.1329827308654785
Batch 90: training loss 4.272515296936035
Batch 120: training loss 4.451756954193115
Batch 150: training loss 4.141872406005859
Batch 180: training loss 4.431303024291992
Batch 210: training loss 4.313239097595215
Batch 240: training loss 4.2240118980407715
Batch 270: training loss 4.249306678771973
Batch 300: training loss 4.302575588226318
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.241633415222168
Batch 360: training loss 4.396824836730957
Batch 390: training loss 4.31126594543457
Batch 420: training loss 4.374098777770996
Batch 450: training loss 4.406352996826172
Batch 480: training loss 4.479262828826904
Batch 510: training loss 4.459919452667236
Batch 540: training loss 4.4138288497924805
Batch 570: training loss 4.4795074462890625
Batch 600: training loss 4.397534370422363
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 67: Average Training Loss: 4.3751163581848145, Average Validation Loss: 4.863730481330385
Batch 30: training loss 4.044544696807861
Batch 60: training loss 4.5532755851745605
Batch 90: training loss 4.245723724365234
Batch 120: training loss 4.297630786895752
Batch 150: training loss 4.464727401733398
Batch 180: training loss 4.346928596496582
Batch 210: training loss 4.404748916625977
Batch 240: training loss 4.111941814422607
Batch 270: training loss 4.251767635345459
Batch 300: training loss 4.314345359802246
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.393523693084717
Batch 360: training loss 4.368992805480957
Batch 390: training loss 4.554213523864746
Batch 420: training loss 4.48257303237915
Batch 450: training loss 4.3456807136535645
Batch 480: training loss 4.27785587310791
Batch 510: training loss 4.500994682312012
Batch 540: training loss 4.463608264923096
Batch 570: training loss 4.235699653625488
Batch 600: training loss 4.4777092933654785
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 68: Average Training Loss: 4.364791954803467, Average Validation Loss: 4.858097299616388
Batch 30: training loss 4.437377452850342
Batch 60: training loss 4.258476257324219
Batch 90: training loss 4.245354652404785
Batch 120: training loss 4.527280807495117
Batch 150: training loss 4.246236801147461
Batch 180: training loss 4.321569919586182
Batch 210: training loss 4.489548683166504
Batch 240: training loss 4.452667713165283
Batch 270: training loss 4.443325519561768
Batch 300: training loss 4.438342571258545
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.401067733764648
Batch 360: training loss 4.451176643371582
Batch 390: training loss 4.600196361541748
Batch 420: training loss 4.3234710693359375
Batch 450: training loss 4.459879398345947
Batch 480: training loss 4.263366222381592
Batch 510: training loss 4.279407978057861
Batch 540: training loss 4.343406677246094
Batch 570: training loss 4.798858642578125
Batch 600: training loss 4.260476589202881
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 69: Average Training Loss: 4.35599028930664, Average Validation Loss: 4.845915621899544
Batch 30: training loss 4.285984039306641
Batch 60: training loss 4.281111240386963
Batch 90: training loss 4.100834369659424
Batch 120: training loss 4.289034366607666
Batch 150: training loss 4.36295223236084
Batch 180: training loss 4.266738414764404
Batch 210: training loss 4.424683570861816
Batch 240: training loss 4.0643486976623535
Batch 270: training loss 4.31869649887085
Batch 300: training loss 4.278048515319824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.254244804382324
Batch 360: training loss 4.36629056930542
Batch 390: training loss 4.254292011260986
Batch 420: training loss 4.374992847442627
Batch 450: training loss 4.229252815246582
Batch 480: training loss 4.333714008331299
Batch 510: training loss 4.462116241455078
Batch 540: training loss 4.214853286743164
Batch 570: training loss 4.1379828453063965
Batch 600: training loss 4.294213771820068
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 70: Average Training Loss: 4.346682277679443, Average Validation Loss: 4.843090534210205
Batch 30: training loss 4.3685173988342285
Batch 60: training loss 4.422624111175537
Batch 90: training loss 4.350644111633301
Batch 120: training loss 4.430049419403076
Batch 150: training loss 4.250380516052246
Batch 180: training loss 4.252779483795166
Batch 210: training loss 4.273571968078613
Batch 240: training loss 4.463127136230469
Batch 270: training loss 4.283773422241211
Batch 300: training loss 4.199331283569336
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.735404968261719
Batch 360: training loss 4.139934539794922
Batch 390: training loss 4.24908447265625
Batch 420: training loss 4.2533278465271
Batch 450: training loss 4.218822956085205
Batch 480: training loss 4.2195940017700195
Batch 510: training loss 4.405648708343506
Batch 540: training loss 4.120428085327148
Batch 570: training loss 4.233608722686768
Batch 600: training loss 4.533254623413086
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 71: Average Training Loss: 4.337261685180664, Average Validation Loss: 4.839027303330441
Batch 30: training loss 4.384259223937988
Batch 60: training loss 4.354310035705566
Batch 90: training loss 4.411458969116211
Batch 120: training loss 4.456429481506348
Batch 150: training loss 4.206476211547852
Batch 180: training loss 4.311792373657227
Batch 210: training loss 4.478280067443848
Batch 240: training loss 4.360404014587402
Batch 270: training loss 4.4690046310424805
Batch 300: training loss 4.490062236785889
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.417235374450684
Batch 360: training loss 4.489528656005859
Batch 390: training loss 4.1668500900268555
Batch 420: training loss 4.287829875946045
Batch 450: training loss 4.611508369445801
Batch 480: training loss 4.195295810699463
Batch 510: training loss 4.247259140014648
Batch 540: training loss 4.230791091918945
Batch 570: training loss 4.386172294616699
Batch 600: training loss 4.316707611083984
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 72: Average Training Loss: 4.330271774291992, Average Validation Loss: 4.82753436108853
Batch 30: training loss 4.285589218139648
Batch 60: training loss 4.45314884185791
Batch 90: training loss 4.454634189605713
Batch 120: training loss 4.31624698638916
Batch 150: training loss 4.228236198425293
Batch 180: training loss 4.356657981872559
Batch 210: training loss 4.4187421798706055
Batch 240: training loss 4.191852569580078
Batch 270: training loss 4.105526447296143
Batch 300: training loss 4.411752223968506
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.461402893066406
Batch 360: training loss 4.468955039978027
Batch 390: training loss 4.311885356903076
Batch 420: training loss 4.410970687866211
Batch 450: training loss 4.459175109863281
Batch 480: training loss 4.461966514587402
Batch 510: training loss 4.3533172607421875
Batch 540: training loss 4.326915264129639
Batch 570: training loss 4.390063285827637
Batch 600: training loss 4.508569717407227
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 73: Average Training Loss: 4.321933901214599, Average Validation Loss: 4.816624702291286
Batch 30: training loss 4.263662338256836
Batch 60: training loss 4.435969352722168
Batch 90: training loss 4.377679824829102
Batch 120: training loss 4.417910099029541
Batch 150: training loss 4.050607204437256
Batch 180: training loss 4.436680793762207
Batch 210: training loss 4.061667442321777
Batch 240: training loss 4.221431255340576
Batch 270: training loss 4.3276777267456055
Batch 300: training loss 4.4885149002075195
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.268309593200684
Batch 360: training loss 4.154728412628174
Batch 390: training loss 4.293281555175781
Batch 420: training loss 4.357911586761475
Batch 450: training loss 4.1615729331970215
Batch 480: training loss 4.433962821960449
Batch 510: training loss 4.424637317657471
Batch 540: training loss 4.420294284820557
Batch 570: training loss 4.4843430519104
Batch 600: training loss 4.116461277008057
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 74: Average Training Loss: 4.3138886711120605, Average Validation Loss: 4.823762579167143
Batch 30: training loss 4.640964508056641
Batch 60: training loss 4.338825225830078
Batch 90: training loss 4.4727582931518555
Batch 120: training loss 4.204603672027588
Batch 150: training loss 4.35102653503418
Batch 180: training loss 4.285284996032715
Batch 210: training loss 4.48829460144043
Batch 240: training loss 4.405311584472656
Batch 270: training loss 4.119725704193115
Batch 300: training loss 4.395138263702393
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.386516571044922
Batch 360: training loss 4.665994644165039
Batch 390: training loss 4.330202102661133
Batch 420: training loss 4.507697582244873
Batch 450: training loss 4.2613935470581055
Batch 480: training loss 4.296060562133789
Batch 510: training loss 4.430006504058838
Batch 540: training loss 4.602104663848877
Batch 570: training loss 4.376891136169434
Batch 600: training loss 4.273190021514893
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 75: Average Training Loss: 4.305458071899414, Average Validation Loss: 4.813006198152583
Batch 30: training loss 4.381202697753906
Batch 60: training loss 4.22194766998291
Batch 90: training loss 4.304306507110596
Batch 120: training loss 4.220874309539795
Batch 150: training loss 4.209285259246826
Batch 180: training loss 4.246026992797852
Batch 210: training loss 4.338492393493652
Batch 240: training loss 4.388598442077637
Batch 270: training loss 4.255253791809082
Batch 300: training loss 4.1528425216674805
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.211181640625
Batch 360: training loss 4.3700408935546875
Batch 390: training loss 4.210874557495117
Batch 420: training loss 4.044859409332275
Batch 450: training loss 4.359403133392334
Batch 480: training loss 4.273595809936523
Batch 510: training loss 4.118911266326904
Batch 540: training loss 4.139376163482666
Batch 570: training loss 4.3554816246032715
Batch 600: training loss 4.500799179077148
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 76: Average Training Loss: 4.297498342895508, Average Validation Loss: 4.795622328494457
Batch 30: training loss 4.117002487182617
Batch 60: training loss 4.056337356567383
Batch 90: training loss 4.458651542663574
Batch 120: training loss 4.370635032653809
Batch 150: training loss 4.330695152282715
Batch 180: training loss 4.524631977081299
Batch 210: training loss 4.361313819885254
Batch 240: training loss 4.112497329711914
Batch 270: training loss 4.223752021789551
Batch 300: training loss 4.158161163330078
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.21160364151001
Batch 360: training loss 4.234984397888184
Batch 390: training loss 4.179897308349609
Batch 420: training loss 4.214621067047119
Batch 450: training loss 4.278654098510742
Batch 480: training loss 4.3719305992126465
Batch 510: training loss 4.522440433502197
Batch 540: training loss 4.213936805725098
Batch 570: training loss 4.2063446044921875
Batch 600: training loss 4.388343811035156
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 77: Average Training Loss: 4.288861801147461, Average Validation Loss: 4.793913283246629
Batch 30: training loss 4.326025009155273
Batch 60: training loss 4.343845367431641
Batch 90: training loss 4.1934919357299805
Batch 120: training loss 4.359793663024902
Batch 150: training loss 4.390174388885498
Batch 180: training loss 4.444477558135986
Batch 210: training loss 4.420123100280762
Batch 240: training loss 4.325512886047363
Batch 270: training loss 4.198420524597168
Batch 300: training loss 4.265111923217773
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.426774978637695
Batch 360: training loss 4.358614444732666
Batch 390: training loss 4.190252780914307
Batch 420: training loss 4.324195384979248
Batch 450: training loss 4.2332539558410645
Batch 480: training loss 4.168436527252197
Batch 510: training loss 4.493098258972168
Batch 540: training loss 4.532537937164307
Batch 570: training loss 4.044926643371582
Batch 600: training loss 4.522469997406006
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 78: Average Training Loss: 4.281354734420776, Average Validation Loss: 4.789371744115302
Batch 30: training loss 4.295029640197754
Batch 60: training loss 4.49390172958374
Batch 90: training loss 4.3892974853515625
Batch 120: training loss 4.206913471221924
Batch 150: training loss 4.362903594970703
Batch 180: training loss 4.39488410949707
Batch 210: training loss 4.2718505859375
Batch 240: training loss 4.153322219848633
Batch 270: training loss 4.444379806518555
Batch 300: training loss 4.2044172286987305
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.288850784301758
Batch 360: training loss 4.259555816650391
Batch 390: training loss 4.18496561050415
Batch 420: training loss 4.196572780609131
Batch 450: training loss 4.335939884185791
Batch 480: training loss 4.297256946563721
Batch 510: training loss 4.113257884979248
Batch 540: training loss 4.304862976074219
Batch 570: training loss 4.375485897064209
Batch 600: training loss 4.341790199279785
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 79: Average Training Loss: 4.273762562561036, Average Validation Loss: 4.780563729874631
Batch 30: training loss 4.087060451507568
Batch 60: training loss 4.328330993652344
Batch 90: training loss 4.108110427856445
Batch 120: training loss 4.402836322784424
Batch 150: training loss 4.285162925720215
Batch 180: training loss 4.134010314941406
Batch 210: training loss 4.108598709106445
Batch 240: training loss 4.536370277404785
Batch 270: training loss 4.251413822174072
Batch 300: training loss 4.274286270141602
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.292206764221191
Batch 360: training loss 4.34844446182251
Batch 390: training loss 4.244330406188965
Batch 420: training loss 4.320272445678711
Batch 450: training loss 4.106693267822266
Batch 480: training loss 4.388295650482178
Batch 510: training loss 4.448007583618164
Batch 540: training loss 4.305619239807129
Batch 570: training loss 4.3485283851623535
Batch 600: training loss 4.4889445304870605
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 80: Average Training Loss: 4.265033402633667, Average Validation Loss: 4.776832600857349
Batch 30: training loss 4.069610595703125
Batch 60: training loss 4.060161590576172
Batch 90: training loss 4.349266529083252
Batch 120: training loss 4.1721062660217285
Batch 150: training loss 4.147762775421143
Batch 180: training loss 4.3617987632751465
Batch 210: training loss 4.419496536254883
Batch 240: training loss 4.305878162384033
Batch 270: training loss 4.206140518188477
Batch 300: training loss 4.339199066162109
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.342844009399414
Batch 360: training loss 4.422616481781006
Batch 390: training loss 4.26894998550415
Batch 420: training loss 4.255342483520508
Batch 450: training loss 4.521735191345215
Batch 480: training loss 3.9580302238464355
Batch 510: training loss 4.323648452758789
Batch 540: training loss 4.338871002197266
Batch 570: training loss 4.3675713539123535
Batch 600: training loss 4.294161796569824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 81: Average Training Loss: 4.257579061126709, Average Validation Loss: 4.761119183073652
Batch 30: training loss 4.050910949707031
Batch 60: training loss 4.041871070861816
Batch 90: training loss 4.476851463317871
Batch 120: training loss 4.495590686798096
Batch 150: training loss 4.240377902984619
Batch 180: training loss 4.303553581237793
Batch 210: training loss 4.331047058105469
Batch 240: training loss 4.346441268920898
Batch 270: training loss 4.1376752853393555
Batch 300: training loss 4.279539108276367
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.324710845947266
Batch 360: training loss 4.584842681884766
Batch 390: training loss 4.159862518310547
Batch 420: training loss 4.072982311248779
Batch 450: training loss 4.135526657104492
Batch 480: training loss 4.258289337158203
Batch 510: training loss 3.9996132850646973
Batch 540: training loss 4.060133457183838
Batch 570: training loss 4.283066272735596
Batch 600: training loss 4.321437835693359
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 82: Average Training Loss: 4.249688953781128, Average Validation Loss: 4.772032423222319
Batch 30: training loss 4.187735080718994
Batch 60: training loss 4.128169059753418
Batch 90: training loss 4.227718830108643
Batch 120: training loss 3.9781851768493652
Batch 150: training loss 4.010554313659668
Batch 180: training loss 4.38093376159668
Batch 210: training loss 4.463088512420654
Batch 240: training loss 4.281680107116699
Batch 270: training loss 4.310894012451172
Batch 300: training loss 4.257979393005371
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.346689701080322
Batch 360: training loss 4.118790149688721
Batch 390: training loss 4.252793312072754
Batch 420: training loss 4.272688388824463
Batch 450: training loss 4.238386631011963
Batch 480: training loss 4.298098087310791
Batch 510: training loss 4.087590217590332
Batch 540: training loss 4.3662614822387695
Batch 570: training loss 4.207225799560547
Batch 600: training loss 4.33541202545166
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 83: Average Training Loss: 4.242062446975708, Average Validation Loss: 4.764312612249496
Batch 30: training loss 4.218432426452637
Batch 60: training loss 4.315079689025879
Batch 90: training loss 4.443565368652344
Batch 120: training loss 4.219700813293457
Batch 150: training loss 4.165231704711914
Batch 180: training loss 4.121720314025879
Batch 210: training loss 4.62581729888916
Batch 240: training loss 4.343515872955322
Batch 270: training loss 4.443758964538574
Batch 300: training loss 4.303013801574707
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.243193626403809
Batch 360: training loss 3.99373197555542
Batch 390: training loss 4.4349565505981445
Batch 420: training loss 4.30979585647583
Batch 450: training loss 4.115752220153809
Batch 480: training loss 4.150951385498047
Batch 510: training loss 4.31363582611084
Batch 540: training loss 4.105995178222656
Batch 570: training loss 4.349532604217529
Batch 600: training loss 4.0270161628723145
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 84: Average Training Loss: 4.233767452239991, Average Validation Loss: 4.756439330730032
Batch 30: training loss 4.465429306030273
Batch 60: training loss 4.189892292022705
Batch 90: training loss 4.170168876647949
Batch 120: training loss 4.132503986358643
Batch 150: training loss 4.040439128875732
Batch 180: training loss 4.179999351501465
Batch 210: training loss 4.318984031677246
Batch 240: training loss 4.192915439605713
Batch 270: training loss 4.10840368270874
Batch 300: training loss 4.397947311401367
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.177922248840332
Batch 360: training loss 4.107115745544434
Batch 390: training loss 4.174598217010498
Batch 420: training loss 4.196261405944824
Batch 450: training loss 4.142673015594482
Batch 480: training loss 4.11229133605957
Batch 510: training loss 4.1659836769104
Batch 540: training loss 4.070230960845947
Batch 570: training loss 4.367234230041504
Batch 600: training loss 4.4089436531066895
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 85: Average Training Loss: 4.228694069671631, Average Validation Loss: 4.744524276002925
Batch 30: training loss 4.345988750457764
Batch 60: training loss 4.341907501220703
Batch 90: training loss 4.064633369445801
Batch 120: training loss 4.462550640106201
Batch 150: training loss 4.1945319175720215
Batch 180: training loss 4.294924736022949
Batch 210: training loss 4.129960060119629
Batch 240: training loss 4.004063606262207
Batch 270: training loss 4.296095371246338
Batch 300: training loss 4.180933952331543
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.155604839324951
Batch 360: training loss 4.405197620391846
Batch 390: training loss 4.123124599456787
Batch 420: training loss 4.243365287780762
Batch 450: training loss 4.112430095672607
Batch 480: training loss 4.195878505706787
Batch 510: training loss 4.2888946533203125
Batch 540: training loss 4.148695468902588
Batch 570: training loss 4.252384185791016
Batch 600: training loss 4.356914043426514
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 86: Average Training Loss: 4.220177132034301, Average Validation Loss: 4.742930919566053
Batch 30: training loss 4.089639186859131
Batch 60: training loss 4.0881876945495605
Batch 90: training loss 4.216370582580566
Batch 120: training loss 4.168304443359375
Batch 150: training loss 4.282013416290283
Batch 180: training loss 4.098635673522949
Batch 210: training loss 4.209617614746094
Batch 240: training loss 4.433070182800293
Batch 270: training loss 4.009120941162109
Batch 300: training loss 4.074246883392334
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.051345348358154
Batch 360: training loss 4.3583879470825195
Batch 390: training loss 4.207862377166748
Batch 420: training loss 4.228419303894043
Batch 450: training loss 4.294310092926025
Batch 480: training loss 4.343657493591309
Batch 510: training loss 4.184108734130859
Batch 540: training loss 4.3655781745910645
Batch 570: training loss 4.164182662963867
Batch 600: training loss 4.322132587432861
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 87: Average Training Loss: 4.214006380081177, Average Validation Loss: 4.7322230440505
Batch 30: training loss 4.1419997215271
Batch 60: training loss 4.351131439208984
Batch 90: training loss 4.182665824890137
Batch 120: training loss 4.103038311004639
Batch 150: training loss 4.019896507263184
Batch 180: training loss 4.185603618621826
Batch 210: training loss 3.9584522247314453
Batch 240: training loss 4.079728126525879
Batch 270: training loss 4.351284027099609
Batch 300: training loss 4.146070957183838
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.120791435241699
Batch 360: training loss 4.209786415100098
Batch 390: training loss 4.178888320922852
Batch 420: training loss 4.2422285079956055
Batch 450: training loss 4.270216464996338
Batch 480: training loss 4.251010894775391
Batch 510: training loss 4.26706075668335
Batch 540: training loss 4.218145847320557
Batch 570: training loss 4.051036357879639
Batch 600: training loss 4.314619064331055
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 88: Average Training Loss: 4.207138580703735, Average Validation Loss: 4.725077436325398
Batch 30: training loss 4.343338489532471
Batch 60: training loss 4.325312614440918
Batch 90: training loss 4.152998924255371
Batch 120: training loss 4.106778621673584
Batch 150: training loss 4.360995769500732
Batch 180: training loss 4.088809490203857
Batch 210: training loss 4.1770100593566895
Batch 240: training loss 4.222142696380615
Batch 270: training loss 4.077380180358887
Batch 300: training loss 4.354510307312012
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.032784938812256
Batch 360: training loss 4.2194647789001465
Batch 390: training loss 4.354033946990967
Batch 420: training loss 4.198942184448242
Batch 450: training loss 4.065011024475098
Batch 480: training loss 4.148432731628418
Batch 510: training loss 4.19410514831543
Batch 540: training loss 4.334687232971191
Batch 570: training loss 4.3211259841918945
Batch 600: training loss 4.13329553604126
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 89: Average Training Loss: 4.19905245475769, Average Validation Loss: 4.724545793330416
Batch 30: training loss 4.182467460632324
Batch 60: training loss 4.210694789886475
Batch 90: training loss 4.053414344787598
Batch 120: training loss 3.8998334407806396
Batch 150: training loss 4.068872928619385
Batch 180: training loss 4.203300476074219
Batch 210: training loss 4.313265800476074
Batch 240: training loss 4.109313011169434
Batch 270: training loss 4.1504106521606445
Batch 300: training loss 4.108570098876953
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.103893280029297
Batch 360: training loss 4.197088241577148
Batch 390: training loss 3.994457721710205
Batch 420: training loss 3.9828248023986816
Batch 450: training loss 4.23182487487793
Batch 480: training loss 4.116546154022217
Batch 510: training loss 4.007891654968262
Batch 540: training loss 4.114449501037598
Batch 570: training loss 4.319448471069336
Batch 600: training loss 4.233055591583252
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 90: Average Training Loss: 4.193416292572022, Average Validation Loss: 4.716202776482764
Batch 30: training loss 4.368288993835449
Batch 60: training loss 4.272496223449707
Batch 90: training loss 4.2905192375183105
Batch 120: training loss 4.287868499755859
Batch 150: training loss 4.151815891265869
Batch 180: training loss 4.126101016998291
Batch 210: training loss 4.4720611572265625
Batch 240: training loss 4.205890655517578
Batch 270: training loss 4.312436103820801
Batch 300: training loss 4.510608196258545
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.363290786743164
Batch 360: training loss 4.304690361022949
Batch 390: training loss 4.091368198394775
Batch 420: training loss 4.190964221954346
Batch 450: training loss 4.265905380249023
Batch 480: training loss 4.348958969116211
Batch 510: training loss 4.209053993225098
Batch 540: training loss 4.193796157836914
Batch 570: training loss 4.158199310302734
Batch 600: training loss 4.130190849304199
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 91: Average Training Loss: 4.185877184295654, Average Validation Loss: 4.713524372019666
Batch 30: training loss 4.078033447265625
Batch 60: training loss 4.190014839172363
Batch 90: training loss 4.254310607910156
Batch 120: training loss 4.1448445320129395
Batch 150: training loss 4.231651306152344
Batch 180: training loss 4.097085952758789
Batch 210: training loss 4.336997985839844
Batch 240: training loss 4.194869518280029
Batch 270: training loss 4.295494079589844
Batch 300: training loss 4.296939849853516
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.243185997009277
Batch 360: training loss 4.20408821105957
Batch 390: training loss 4.309453964233398
Batch 420: training loss 4.291478157043457
Batch 450: training loss 4.041784286499023
Batch 480: training loss 4.287160873413086
Batch 510: training loss 4.225531578063965
Batch 540: training loss 4.190577507019043
Batch 570: training loss 4.22808313369751
Batch 600: training loss 4.113641738891602
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 92: Average Training Loss: 4.1803413612365725, Average Validation Loss: 4.710365376573928
Batch 30: training loss 3.9969239234924316
Batch 60: training loss 4.110013008117676
Batch 90: training loss 4.133775234222412
Batch 120: training loss 3.892764091491699
Batch 150: training loss 4.048520088195801
Batch 180: training loss 4.143071174621582
Batch 210: training loss 4.174536228179932
Batch 240: training loss 3.8341598510742188
Batch 270: training loss 4.301096439361572
Batch 300: training loss 4.392258167266846
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.07659912109375
Batch 360: training loss 4.265923023223877
Batch 390: training loss 4.289205551147461
Batch 420: training loss 4.093184471130371
Batch 450: training loss 4.193479061126709
Batch 480: training loss 4.05735969543457
Batch 510: training loss 4.2323760986328125
Batch 540: training loss 4.2673444747924805
Batch 570: training loss 3.9240880012512207
Batch 600: training loss 4.260393142700195
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 93: Average Training Loss: 4.173368808746338, Average Validation Loss: 4.699310779571533
Batch 30: training loss 4.048885345458984
Batch 60: training loss 4.245179176330566
Batch 90: training loss 4.007530212402344
Batch 120: training loss 4.319540977478027
Batch 150: training loss 4.061039924621582
Batch 180: training loss 4.185317039489746
Batch 210: training loss 4.165847301483154
Batch 240: training loss 4.082438945770264
Batch 270: training loss 4.1468892097473145
Batch 300: training loss 4.198660373687744
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.1382527351379395
Batch 360: training loss 4.308505535125732
Batch 390: training loss 4.253044605255127
Batch 420: training loss 4.443899154663086
Batch 450: training loss 4.202574729919434
Batch 480: training loss 4.104613780975342
Batch 510: training loss 4.356097221374512
Batch 540: training loss 4.168209075927734
Batch 570: training loss 4.101539611816406
Batch 600: training loss 4.163593292236328
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 94: Average Training Loss: 4.168014340209961, Average Validation Loss: 4.6919491544682925
Batch 30: training loss 4.1451263427734375
Batch 60: training loss 4.281650543212891
Batch 90: training loss 4.092909812927246
Batch 120: training loss 4.098428249359131
Batch 150: training loss 4.207508563995361
Batch 180: training loss 4.119223117828369
Batch 210: training loss 3.9985032081604004
Batch 240: training loss 4.205182075500488
Batch 270: training loss 4.3091630935668945
Batch 300: training loss 4.2638092041015625
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.153564453125
Batch 360: training loss 3.9431352615356445
Batch 390: training loss 3.9811909198760986
Batch 420: training loss 4.056399345397949
Batch 450: training loss 4.293695449829102
Batch 480: training loss 4.231786251068115
Batch 510: training loss 4.009045124053955
Batch 540: training loss 3.955151081085205
Batch 570: training loss 4.136369228363037
Batch 600: training loss 4.069161415100098
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 95: Average Training Loss: 4.160483771514893, Average Validation Loss: 4.687614481499854
Batch 30: training loss 4.335734844207764
Batch 60: training loss 3.9885611534118652
Batch 90: training loss 4.109086036682129
Batch 120: training loss 4.070795059204102
Batch 150: training loss 4.367128372192383
Batch 180: training loss 4.152501106262207
Batch 210: training loss 3.9418582916259766
Batch 240: training loss 4.128081321716309
Batch 270: training loss 4.0464067459106445
Batch 300: training loss 4.196215629577637
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.015048980712891
Batch 360: training loss 4.259189605712891
Batch 390: training loss 4.201511383056641
Batch 420: training loss 4.024743556976318
Batch 450: training loss 4.12406063079834
Batch 480: training loss 4.216429710388184
Batch 510: training loss 4.075643539428711
Batch 540: training loss 4.115745544433594
Batch 570: training loss 4.119367599487305
Batch 600: training loss 4.160078048706055
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 96: Average Training Loss: 4.154424048995971, Average Validation Loss: 4.691438502453743
Batch 30: training loss 4.1631059646606445
Batch 60: training loss 4.192963600158691
Batch 90: training loss 4.2245097160339355
Batch 120: training loss 4.055866241455078
Batch 150: training loss 4.092382907867432
Batch 180: training loss 4.268645763397217
Batch 210: training loss 4.212340354919434
Batch 240: training loss 4.50693416595459
Batch 270: training loss 3.9526264667510986
Batch 300: training loss 4.200984477996826
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9903149604797363
Batch 360: training loss 4.343006610870361
Batch 390: training loss 4.333914756774902
Batch 420: training loss 4.052648544311523
Batch 450: training loss 4.110292911529541
Batch 480: training loss 3.951779842376709
Batch 510: training loss 4.416752815246582
Batch 540: training loss 4.304119110107422
Batch 570: training loss 4.033745765686035
Batch 600: training loss 4.164797782897949
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 97: Average Training Loss: 4.1479649841308595, Average Validation Loss: 4.685864377529063
Batch 30: training loss 4.058544158935547
Batch 60: training loss 4.07289981842041
Batch 90: training loss 4.488196849822998
Batch 120: training loss 4.124306678771973
Batch 150: training loss 4.033295154571533
Batch 180: training loss 4.186352729797363
Batch 210: training loss 4.105048179626465
Batch 240: training loss 4.037783622741699
Batch 270: training loss 4.1516923904418945
Batch 300: training loss 4.160393714904785
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.346414089202881
Batch 360: training loss 4.242698669433594
Batch 390: training loss 4.243406772613525
Batch 420: training loss 4.175170421600342
Batch 450: training loss 4.232846260070801
Batch 480: training loss 4.111299991607666
Batch 510: training loss 4.1392951011657715
Batch 540: training loss 4.155616283416748
Batch 570: training loss 4.208946228027344
Batch 600: training loss 4.362797737121582
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 98: Average Training Loss: 4.141307822799683, Average Validation Loss: 4.674623418361582
Batch 30: training loss 4.0959296226501465
Batch 60: training loss 4.194834232330322
Batch 90: training loss 4.134317874908447
Batch 120: training loss 4.1894707679748535
Batch 150: training loss 4.119488716125488
Batch 180: training loss 4.132679462432861
Batch 210: training loss 4.045566558837891
Batch 240: training loss 4.400488376617432
Batch 270: training loss 4.117077827453613
Batch 300: training loss 3.971806526184082
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9125142097473145
Batch 360: training loss 4.134233474731445
Batch 390: training loss 4.095885753631592
Batch 420: training loss 4.10642671585083
Batch 450: training loss 4.077313423156738
Batch 480: training loss 4.015690326690674
Batch 510: training loss 4.355237007141113
Batch 540: training loss 4.1292901039123535
Batch 570: training loss 3.9766130447387695
Batch 600: training loss 4.2693023681640625
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 99: Average Training Loss: 4.13585284576416, Average Validation Loss: 4.66725219564235
Batch 30: training loss 4.179606914520264
Batch 60: training loss 3.985072135925293
Batch 90: training loss 4.219158172607422
Batch 120: training loss 4.414663314819336
Batch 150: training loss 4.090839385986328
Batch 180: training loss 3.9928760528564453
Batch 210: training loss 4.197589874267578
Batch 240: training loss 4.087702751159668
Batch 270: training loss 4.104393005371094
Batch 300: training loss 4.156172275543213
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9660043716430664
Batch 360: training loss 4.192246913909912
Batch 390: training loss 4.297380447387695
Batch 420: training loss 4.4419426918029785
Batch 450: training loss 3.9861671924591064
Batch 480: training loss 4.117280006408691
Batch 510: training loss 4.017614841461182
Batch 540: training loss 4.011335372924805
Batch 570: training loss 4.134628772735596
Batch 600: training loss 4.204402923583984
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 100: Average Training Loss: 4.129314140701294, Average Validation Loss: 4.6656788460751795
Batch 30: training loss 4.02140998840332
Batch 60: training loss 4.2082905769348145
Batch 90: training loss 4.0933380126953125
Batch 120: training loss 4.086389541625977
Batch 150: training loss 4.112603187561035
Batch 180: training loss 4.119627952575684
Batch 210: training loss 4.100619316101074
Batch 240: training loss 4.049381732940674
Batch 270: training loss 4.518252372741699
Batch 300: training loss 3.994499683380127
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.2441816329956055
Batch 360: training loss 4.095222473144531
Batch 390: training loss 4.137428283691406
Batch 420: training loss 4.223696708679199
Batch 450: training loss 4.183037757873535
Batch 480: training loss 3.9985313415527344
Batch 510: training loss 4.00415563583374
Batch 540: training loss 4.147568702697754
Batch 570: training loss 4.0471343994140625
Batch 600: training loss 4.182221412658691
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 101: Average Training Loss: 4.123553712081909, Average Validation Loss: 4.661809911119177
Batch 30: training loss 3.881040573120117
Batch 60: training loss 4.106334209442139
Batch 90: training loss 4.193777561187744
Batch 120: training loss 4.312460422515869
Batch 150: training loss 4.194728851318359
Batch 180: training loss 4.0876030921936035
Batch 210: training loss 4.310921669006348
Batch 240: training loss 4.267586708068848
Batch 270: training loss 4.000067710876465
Batch 300: training loss 4.082779884338379
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.359764099121094
Batch 360: training loss 3.941676616668701
Batch 390: training loss 4.165157794952393
Batch 420: training loss 4.144644737243652
Batch 450: training loss 4.123153209686279
Batch 480: training loss 3.968970775604248
Batch 510: training loss 4.132911682128906
Batch 540: training loss 4.308714866638184
Batch 570: training loss 4.135894775390625
Batch 600: training loss 3.955833911895752
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 102: Average Training Loss: 4.11732697982788, Average Validation Loss: 4.6586257853406545
Batch 30: training loss 4.084103584289551
Batch 60: training loss 4.161158561706543
Batch 90: training loss 4.121367454528809
Batch 120: training loss 4.106072902679443
Batch 150: training loss 4.253654956817627
Batch 180: training loss 4.05667781829834
Batch 210: training loss 4.055905818939209
Batch 240: training loss 4.066065788269043
Batch 270: training loss 3.9906721115112305
Batch 300: training loss 4.191544055938721
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.017024040222168
Batch 360: training loss 4.041817665100098
Batch 390: training loss 4.145623683929443
Batch 420: training loss 3.9636502265930176
Batch 450: training loss 4.072673320770264
Batch 480: training loss 4.274271011352539
Batch 510: training loss 4.0804643630981445
Batch 540: training loss 4.210407733917236
Batch 570: training loss 4.11167573928833
Batch 600: training loss 4.169934272766113
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 103: Average Training Loss: 4.111569577789306, Average Validation Loss: 4.652872876918062
Batch 30: training loss 4.27302885055542
Batch 60: training loss 4.063962936401367
Batch 90: training loss 4.20856237411499
Batch 120: training loss 4.126291751861572
Batch 150: training loss 3.9516971111297607
Batch 180: training loss 4.010788440704346
Batch 210: training loss 4.0077104568481445
Batch 240: training loss 4.04719352722168
Batch 270: training loss 4.050326347351074
Batch 300: training loss 4.121516227722168
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.188411712646484
Batch 360: training loss 4.044512748718262
Batch 390: training loss 4.094805717468262
Batch 420: training loss 4.23553991317749
Batch 450: training loss 4.111992835998535
Batch 480: training loss 4.092414855957031
Batch 510: training loss 4.178950786590576
Batch 540: training loss 3.9340929985046387
Batch 570: training loss 3.8954782485961914
Batch 600: training loss 3.8336901664733887
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 104: Average Training Loss: 4.106805027389527, Average Validation Loss: 4.643705104259735
Batch 30: training loss 4.114809513092041
Batch 60: training loss 4.188098907470703
Batch 90: training loss 4.324368476867676
Batch 120: training loss 4.057651042938232
Batch 150: training loss 3.980401039123535
Batch 180: training loss 4.261916160583496
Batch 210: training loss 4.1226725578308105
Batch 240: training loss 4.001070499420166
Batch 270: training loss 4.248029708862305
Batch 300: training loss 4.174117088317871
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.143401145935059
Batch 360: training loss 4.172277927398682
Batch 390: training loss 4.007324695587158
Batch 420: training loss 4.237812519073486
Batch 450: training loss 4.1481242179870605
Batch 480: training loss 3.970445156097412
Batch 510: training loss 4.095323085784912
Batch 540: training loss 4.036917686462402
Batch 570: training loss 3.9909417629241943
Batch 600: training loss 4.027708053588867
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 105: Average Training Loss: 4.0990406494140625, Average Validation Loss: 4.648215446066349
Batch 30: training loss 4.114805221557617
Batch 60: training loss 4.223802089691162
Batch 90: training loss 4.158511161804199
Batch 120: training loss 3.9398093223571777
Batch 150: training loss 4.336789131164551
Batch 180: training loss 3.8909525871276855
Batch 210: training loss 4.20563268661499
Batch 240: training loss 4.119119644165039
Batch 270: training loss 4.118796348571777
Batch 300: training loss 4.006977081298828
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.234065055847168
Batch 360: training loss 4.1213765144348145
Batch 390: training loss 4.127345561981201
Batch 420: training loss 4.043132781982422
Batch 450: training loss 4.05889368057251
Batch 480: training loss 4.076725959777832
Batch 510: training loss 4.033156394958496
Batch 540: training loss 4.135896682739258
Batch 570: training loss 4.049576759338379
Batch 600: training loss 4.021446228027344
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 106: Average Training Loss: 4.0931263999938965, Average Validation Loss: 4.636232112316375
Batch 30: training loss 4.120972633361816
Batch 60: training loss 3.9991025924682617
Batch 90: training loss 4.004316329956055
Batch 120: training loss 4.026861667633057
Batch 150: training loss 4.229763984680176
Batch 180: training loss 4.239236831665039
Batch 210: training loss 3.9762134552001953
Batch 240: training loss 4.315984725952148
Batch 270: training loss 4.055112838745117
Batch 300: training loss 4.017790794372559
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.068946838378906
Batch 360: training loss 4.136424541473389
Batch 390: training loss 4.192385196685791
Batch 420: training loss 4.031721591949463
Batch 450: training loss 4.113654613494873
Batch 480: training loss 4.1359405517578125
Batch 510: training loss 3.894862174987793
Batch 540: training loss 4.046338081359863
Batch 570: training loss 3.960756778717041
Batch 600: training loss 4.230364799499512
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 107: Average Training Loss: 4.087376005935669, Average Validation Loss: 4.628519474191869
Batch 30: training loss 3.887327194213867
Batch 60: training loss 3.970600128173828
Batch 90: training loss 4.2305378913879395
Batch 120: training loss 4.037015438079834
Batch 150: training loss 4.107562065124512
Batch 180: training loss 4.287224769592285
Batch 210: training loss 4.185626029968262
Batch 240: training loss 3.9353139400482178
Batch 270: training loss 3.9372668266296387
Batch 300: training loss 4.10775899887085
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.292830467224121
Batch 360: training loss 4.1518425941467285
Batch 390: training loss 4.00960636138916
Batch 420: training loss 3.9763426780700684
Batch 450: training loss 4.101942539215088
Batch 480: training loss 4.122391700744629
Batch 510: training loss 4.0045061111450195
Batch 540: training loss 4.234152317047119
Batch 570: training loss 4.008453845977783
Batch 600: training loss 4.148972511291504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 108: Average Training Loss: 4.083612916564942, Average Validation Loss: 4.623863230360315
Batch 30: training loss 4.104898929595947
Batch 60: training loss 3.8396811485290527
Batch 90: training loss 4.125187873840332
Batch 120: training loss 4.202667236328125
Batch 150: training loss 3.9807615280151367
Batch 180: training loss 4.1699628829956055
Batch 210: training loss 3.8602917194366455
Batch 240: training loss 3.9744982719421387
Batch 270: training loss 4.110691070556641
Batch 300: training loss 4.28029203414917
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.217894077301025
Batch 360: training loss 3.985217571258545
Batch 390: training loss 4.073049545288086
Batch 420: training loss 4.1578569412231445
Batch 450: training loss 4.324596881866455
Batch 480: training loss 4.278878211975098
Batch 510: training loss 3.9141530990600586
Batch 540: training loss 4.2000837326049805
Batch 570: training loss 4.071544647216797
Batch 600: training loss 4.0662126541137695
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 109: Average Training Loss: 4.077853758621216, Average Validation Loss: 4.623901752715415
Batch 30: training loss 3.8853349685668945
Batch 60: training loss 4.184765815734863
Batch 90: training loss 4.0973591804504395
Batch 120: training loss 4.180004119873047
Batch 150: training loss 4.057434558868408
Batch 180: training loss 4.188579082489014
Batch 210: training loss 4.108623504638672
Batch 240: training loss 3.8347463607788086
Batch 270: training loss 4.105008125305176
Batch 300: training loss 3.9888243675231934
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.99367618560791
Batch 360: training loss 4.059606552124023
Batch 390: training loss 4.060798645019531
Batch 420: training loss 3.9388976097106934
Batch 450: training loss 3.908233165740967
Batch 480: training loss 4.112419128417969
Batch 510: training loss 4.134443283081055
Batch 540: training loss 4.194363117218018
Batch 570: training loss 3.830897331237793
Batch 600: training loss 4.304199695587158
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 110: Average Training Loss: 4.073743141174316, Average Validation Loss: 4.624811294231009
Batch 30: training loss 3.9804115295410156
Batch 60: training loss 4.294426441192627
Batch 90: training loss 4.00469446182251
Batch 120: training loss 4.051138877868652
Batch 150: training loss 4.110456466674805
Batch 180: training loss 3.964534282684326
Batch 210: training loss 3.9432051181793213
Batch 240: training loss 3.9322776794433594
Batch 270: training loss 4.277651786804199
Batch 300: training loss 4.011701583862305
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.06809139251709
Batch 360: training loss 3.9783127307891846
Batch 390: training loss 4.129101753234863
Batch 420: training loss 4.1339802742004395
Batch 450: training loss 4.036373615264893
Batch 480: training loss 3.96915602684021
Batch 510: training loss 4.096988677978516
Batch 540: training loss 3.8040361404418945
Batch 570: training loss 3.867466449737549
Batch 600: training loss 4.209640026092529
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 111: Average Training Loss: 4.065048671340942, Average Validation Loss: 4.612744676305892
Batch 30: training loss 3.968613624572754
Batch 60: training loss 4.103269577026367
Batch 90: training loss 4.293997764587402
Batch 120: training loss 3.824747085571289
Batch 150: training loss 4.184725284576416
Batch 180: training loss 4.120936393737793
Batch 210: training loss 4.062810897827148
Batch 240: training loss 3.982818841934204
Batch 270: training loss 4.405251979827881
Batch 300: training loss 3.94326114654541
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.962839126586914
Batch 360: training loss 4.120030403137207
Batch 390: training loss 4.105603218078613
Batch 420: training loss 3.7434089183807373
Batch 450: training loss 4.104408264160156
Batch 480: training loss 4.09335470199585
Batch 510: training loss 4.125574588775635
Batch 540: training loss 4.130995750427246
Batch 570: training loss 4.169379234313965
Batch 600: training loss 3.825843095779419
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 112: Average Training Loss: 4.062070945358276, Average Validation Loss: 4.603087212177033
Batch 30: training loss 3.9506168365478516
Batch 60: training loss 4.199169635772705
Batch 90: training loss 3.865217685699463
Batch 120: training loss 3.843822479248047
Batch 150: training loss 4.283499717712402
Batch 180: training loss 3.883309841156006
Batch 210: training loss 4.076798915863037
Batch 240: training loss 3.97145938873291
Batch 270: training loss 4.312932968139648
Batch 300: training loss 3.9130444526672363
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9832873344421387
Batch 360: training loss 4.191544055938721
Batch 390: training loss 3.9877891540527344
Batch 420: training loss 3.8610639572143555
Batch 450: training loss 3.915525436401367
Batch 480: training loss 4.064919948577881
Batch 510: training loss 4.188108444213867
Batch 540: training loss 4.216612815856934
Batch 570: training loss 4.067209243774414
Batch 600: training loss 4.019973278045654
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 113: Average Training Loss: 4.057668631362915, Average Validation Loss: 4.601309512523895
Batch 30: training loss 3.83804988861084
Batch 60: training loss 4.020272731781006
Batch 90: training loss 3.9754390716552734
Batch 120: training loss 4.112777233123779
Batch 150: training loss 3.9461324214935303
Batch 180: training loss 3.9134106636047363
Batch 210: training loss 4.020368576049805
Batch 240: training loss 3.8605480194091797
Batch 270: training loss 4.057622909545898
Batch 300: training loss 3.9400439262390137
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.073205471038818
Batch 360: training loss 3.9707188606262207
Batch 390: training loss 3.937840223312378
Batch 420: training loss 4.284598350524902
Batch 450: training loss 4.067452430725098
Batch 480: training loss 4.151793479919434
Batch 510: training loss 4.237560272216797
Batch 540: training loss 4.116051197052002
Batch 570: training loss 3.897855758666992
Batch 600: training loss 4.244471549987793
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 114: Average Training Loss: 4.051456987762451, Average Validation Loss: 4.598045511448637
Batch 30: training loss 3.949577808380127
Batch 60: training loss 4.066540718078613
Batch 90: training loss 3.968461513519287
Batch 120: training loss 3.847548723220825
Batch 150: training loss 4.0715227127075195
Batch 180: training loss 3.89288330078125
Batch 210: training loss 4.114132881164551
Batch 240: training loss 4.18940544128418
Batch 270: training loss 4.079329013824463
Batch 300: training loss 4.133474349975586
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.130627632141113
Batch 360: training loss 3.9079179763793945
Batch 390: training loss 3.9964535236358643
Batch 420: training loss 4.039387226104736
Batch 450: training loss 4.0895161628723145
Batch 480: training loss 4.005581855773926
Batch 510: training loss 4.054438591003418
Batch 540: training loss 4.107638359069824
Batch 570: training loss 4.1638503074646
Batch 600: training loss 3.950620174407959
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 115: Average Training Loss: 4.0463646823883055, Average Validation Loss: 4.592108645337693
Batch 30: training loss 4.079520225524902
Batch 60: training loss 4.068650245666504
Batch 90: training loss 4.020931720733643
Batch 120: training loss 3.8400778770446777
Batch 150: training loss 4.013617992401123
Batch 180: training loss 3.9286327362060547
Batch 210: training loss 4.029165744781494
Batch 240: training loss 4.211341381072998
Batch 270: training loss 4.039210319519043
Batch 300: training loss 4.074974060058594
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9457859992980957
Batch 360: training loss 4.155826568603516
Batch 390: training loss 4.110086441040039
Batch 420: training loss 4.055400848388672
Batch 450: training loss 3.980837345123291
Batch 480: training loss 3.8439908027648926
Batch 510: training loss 4.040712356567383
Batch 540: training loss 4.096762657165527
Batch 570: training loss 4.065258026123047
Batch 600: training loss 3.9199910163879395
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 116: Average Training Loss: 4.041955346679687, Average Validation Loss: 4.588779479899305
Batch 30: training loss 4.164759635925293
Batch 60: training loss 4.098768711090088
Batch 90: training loss 3.97304630279541
Batch 120: training loss 3.9183218479156494
Batch 150: training loss 4.369717121124268
Batch 180: training loss 4.075453281402588
Batch 210: training loss 3.8553085327148438
Batch 240: training loss 4.206432342529297
Batch 270: training loss 4.079533576965332
Batch 300: training loss 4.049613952636719
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8921618461608887
Batch 360: training loss 4.187605857849121
Batch 390: training loss 4.2017412185668945
Batch 420: training loss 4.034607887268066
Batch 450: training loss 4.025398254394531
Batch 480: training loss 4.029105186462402
Batch 510: training loss 4.082674026489258
Batch 540: training loss 4.028128623962402
Batch 570: training loss 4.060275077819824
Batch 600: training loss 4.007296562194824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 117: Average Training Loss: 4.034087467956543, Average Validation Loss: 4.589049755258763
Batch 30: training loss 3.9924492835998535
Batch 60: training loss 4.039393901824951
Batch 90: training loss 3.911156177520752
Batch 120: training loss 3.7269625663757324
Batch 150: training loss 3.94407320022583
Batch 180: training loss 4.149435043334961
Batch 210: training loss 3.9325547218322754
Batch 240: training loss 3.9120047092437744
Batch 270: training loss 4.094362735748291
Batch 300: training loss 3.943455696105957
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.277693271636963
Batch 360: training loss 4.070224285125732
Batch 390: training loss 4.0772504806518555
Batch 420: training loss 4.057263374328613
Batch 450: training loss 4.036497116088867
Batch 480: training loss 3.851475238800049
Batch 510: training loss 4.3955078125
Batch 540: training loss 3.96652889251709
Batch 570: training loss 4.162905693054199
Batch 600: training loss 3.9956600666046143
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 118: Average Training Loss: 4.03049733505249, Average Validation Loss: 4.580057702165969
Batch 30: training loss 3.978302001953125
Batch 60: training loss 3.8547298908233643
Batch 90: training loss 4.008505344390869
Batch 120: training loss 3.888805866241455
Batch 150: training loss 3.838413715362549
Batch 180: training loss 4.21392822265625
Batch 210: training loss 4.134171485900879
Batch 240: training loss 4.068840980529785
Batch 270: training loss 3.985515832901001
Batch 300: training loss 4.060641765594482
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.024442195892334
Batch 360: training loss 4.190082550048828
Batch 390: training loss 4.095185279846191
Batch 420: training loss 3.8753719329833984
Batch 450: training loss 4.04427433013916
Batch 480: training loss 4.083115100860596
Batch 510: training loss 4.022900581359863
Batch 540: training loss 3.8848578929901123
Batch 570: training loss 3.8864407539367676
Batch 600: training loss 4.099599838256836
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 119: Average Training Loss: 4.025476137161255, Average Validation Loss: 4.578925244351651
Batch 30: training loss 4.149855136871338
Batch 60: training loss 3.791395664215088
Batch 90: training loss 4.080041885375977
Batch 120: training loss 3.9782142639160156
Batch 150: training loss 4.132327079772949
Batch 180: training loss 4.145580291748047
Batch 210: training loss 3.8854784965515137
Batch 240: training loss 3.71897029876709
Batch 270: training loss 4.0989885330200195
Batch 300: training loss 4.188908576965332
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.955936908721924
Batch 360: training loss 4.088916301727295
Batch 390: training loss 4.04474401473999
Batch 420: training loss 3.959571361541748
Batch 450: training loss 4.057485580444336
Batch 480: training loss 3.88537859916687
Batch 510: training loss 3.8760828971862793
Batch 540: training loss 3.9491848945617676
Batch 570: training loss 4.086706638336182
Batch 600: training loss 4.098444938659668
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 120: Average Training Loss: 4.021798155593872, Average Validation Loss: 4.580937446431911
Batch 30: training loss 4.365647792816162
Batch 60: training loss 4.141299247741699
Batch 90: training loss 4.281325817108154
Batch 120: training loss 3.937314987182617
Batch 150: training loss 3.812282085418701
Batch 180: training loss 3.995901107788086
Batch 210: training loss 3.8115158081054688
Batch 240: training loss 3.9559524059295654
Batch 270: training loss 3.972172260284424
Batch 300: training loss 3.879102945327759
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.020077228546143
Batch 360: training loss 4.099483013153076
Batch 390: training loss 4.119921684265137
Batch 420: training loss 4.075396537780762
Batch 450: training loss 3.9065985679626465
Batch 480: training loss 3.924391269683838
Batch 510: training loss 4.038165092468262
Batch 540: training loss 3.836099624633789
Batch 570: training loss 4.057548522949219
Batch 600: training loss 3.977625846862793
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 121: Average Training Loss: 4.015818740081787, Average Validation Loss: 4.574827305814053
Batch 30: training loss 4.012625694274902
Batch 60: training loss 4.08898401260376
Batch 90: training loss 4.030846118927002
Batch 120: training loss 3.937953472137451
Batch 150: training loss 4.152719497680664
Batch 180: training loss 4.060046672821045
Batch 210: training loss 3.921462059020996
Batch 240: training loss 4.298306465148926
Batch 270: training loss 3.7894771099090576
Batch 300: training loss 3.8292365074157715
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9132399559020996
Batch 360: training loss 4.098849296569824
Batch 390: training loss 3.8493294715881348
Batch 420: training loss 4.174350261688232
Batch 450: training loss 4.079234600067139
Batch 480: training loss 4.241365909576416
Batch 510: training loss 4.0990681648254395
Batch 540: training loss 4.0410919189453125
Batch 570: training loss 3.9529287815093994
Batch 600: training loss 3.9882378578186035
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 122: Average Training Loss: 4.01148776435852, Average Validation Loss: 4.572382480540174
Batch 30: training loss 4.002596855163574
Batch 60: training loss 4.010997772216797
Batch 90: training loss 4.073108673095703
Batch 120: training loss 3.8894107341766357
Batch 150: training loss 3.9507055282592773
Batch 180: training loss 3.9719812870025635
Batch 210: training loss 3.910505771636963
Batch 240: training loss 4.122255325317383
Batch 270: training loss 3.9112672805786133
Batch 300: training loss 3.941983699798584
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.071282863616943
Batch 360: training loss 3.906174659729004
Batch 390: training loss 3.9764184951782227
Batch 420: training loss 3.9257943630218506
Batch 450: training loss 3.8253095149993896
Batch 480: training loss 4.186431884765625
Batch 510: training loss 4.039334774017334
Batch 540: training loss 4.26632022857666
Batch 570: training loss 4.034233570098877
Batch 600: training loss 3.9059877395629883
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 123: Average Training Loss: 4.006830466079712, Average Validation Loss: 4.566873499687682
Batch 30: training loss 3.9032444953918457
Batch 60: training loss 4.080747127532959
Batch 90: training loss 3.7059824466705322
Batch 120: training loss 3.902331829071045
Batch 150: training loss 3.9186220169067383
Batch 180: training loss 4.328760623931885
Batch 210: training loss 4.107333183288574
Batch 240: training loss 4.0161895751953125
Batch 270: training loss 4.075702667236328
Batch 300: training loss 3.8737096786499023
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9172136783599854
Batch 360: training loss 4.03019905090332
Batch 390: training loss 4.112905025482178
Batch 420: training loss 4.1468024253845215
Batch 450: training loss 3.9620556831359863
Batch 480: training loss 3.9644994735717773
Batch 510: training loss 3.8550515174865723
Batch 540: training loss 4.053248405456543
Batch 570: training loss 4.075213432312012
Batch 600: training loss 3.962110996246338
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 124: Average Training Loss: 4.001878117752075, Average Validation Loss: 4.556077774534834
Batch 30: training loss 4.179594039916992
Batch 60: training loss 3.708632230758667
Batch 90: training loss 3.865325450897217
Batch 120: training loss 3.9621543884277344
Batch 150: training loss 3.9821360111236572
Batch 180: training loss 3.966303586959839
Batch 210: training loss 3.958916187286377
Batch 240: training loss 4.0058465003967285
Batch 270: training loss 4.123003005981445
Batch 300: training loss 4.165566444396973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.865764856338501
Batch 360: training loss 3.8505635261535645
Batch 390: training loss 3.8177413940429688
Batch 420: training loss 4.032450199127197
Batch 450: training loss 3.9185540676116943
Batch 480: training loss 3.9646573066711426
Batch 510: training loss 4.233353614807129
Batch 540: training loss 3.831651210784912
Batch 570: training loss 4.10422420501709
Batch 600: training loss 3.8732194900512695
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 125: Average Training Loss: 3.9979329696655275, Average Validation Loss: 4.56086232814383
Batch 30: training loss 3.986039638519287
Batch 60: training loss 3.774603843688965
Batch 90: training loss 4.1896843910217285
Batch 120: training loss 3.8917503356933594
Batch 150: training loss 3.924593448638916
Batch 180: training loss 3.8038625717163086
Batch 210: training loss 4.1363983154296875
Batch 240: training loss 4.235054016113281
Batch 270: training loss 3.918130874633789
Batch 300: training loss 3.974720001220703
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.959197521209717
Batch 360: training loss 3.8563594818115234
Batch 390: training loss 4.286909580230713
Batch 420: training loss 3.956099510192871
Batch 450: training loss 4.005987167358398
Batch 480: training loss 3.999690055847168
Batch 510: training loss 3.998542308807373
Batch 540: training loss 4.100834369659424
Batch 570: training loss 3.9299798011779785
Batch 600: training loss 3.9682159423828125
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 126: Average Training Loss: 3.9917562480926514, Average Validation Loss: 4.554467454869696
Batch 30: training loss 3.994746685028076
Batch 60: training loss 3.848349094390869
Batch 90: training loss 3.964641571044922
Batch 120: training loss 3.9292726516723633
Batch 150: training loss 3.887439727783203
Batch 180: training loss 4.072087287902832
Batch 210: training loss 3.9670562744140625
Batch 240: training loss 4.0660176277160645
Batch 270: training loss 3.8517186641693115
Batch 300: training loss 3.951474189758301
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.968395233154297
Batch 360: training loss 4.145998477935791
Batch 390: training loss 3.8453783988952637
Batch 420: training loss 3.9442291259765625
Batch 450: training loss 4.008700370788574
Batch 480: training loss 3.974093437194824
Batch 510: training loss 4.020334720611572
Batch 540: training loss 3.8222742080688477
Batch 570: training loss 3.862351894378662
Batch 600: training loss 4.077171802520752
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 127: Average Training Loss: 3.9892237926483154, Average Validation Loss: 4.547440924543015
Batch 30: training loss 4.076864242553711
Batch 60: training loss 4.124176025390625
Batch 90: training loss 3.6778945922851562
Batch 120: training loss 3.9446682929992676
Batch 150: training loss 3.737654685974121
Batch 180: training loss 3.9489166736602783
Batch 210: training loss 3.9510555267333984
Batch 240: training loss 3.9381847381591797
Batch 270: training loss 4.036491394042969
Batch 300: training loss 3.7541308403015137
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.118703842163086
Batch 360: training loss 3.9764761924743652
Batch 390: training loss 3.8328118324279785
Batch 420: training loss 4.142663478851318
Batch 450: training loss 3.9427175521850586
Batch 480: training loss 4.162034034729004
Batch 510: training loss 3.9021456241607666
Batch 540: training loss 3.9257044792175293
Batch 570: training loss 3.9987664222717285
Batch 600: training loss 4.199265003204346
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 128: Average Training Loss: 3.984132424545288, Average Validation Loss: 4.542381783749195
Batch 30: training loss 4.0134124755859375
Batch 60: training loss 4.126709461212158
Batch 90: training loss 3.8692362308502197
Batch 120: training loss 4.040873050689697
Batch 150: training loss 3.8163259029388428
Batch 180: training loss 4.294930458068848
Batch 210: training loss 4.079165458679199
Batch 240: training loss 4.16403865814209
Batch 270: training loss 3.907597064971924
Batch 300: training loss 3.9174227714538574
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.918422222137451
Batch 360: training loss 3.77795672416687
Batch 390: training loss 4.039453029632568
Batch 420: training loss 3.85689640045166
Batch 450: training loss 4.067224502563477
Batch 480: training loss 3.949601173400879
Batch 510: training loss 4.507244110107422
Batch 540: training loss 3.9159703254699707
Batch 570: training loss 3.9996232986450195
Batch 600: training loss 3.8850204944610596
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 129: Average Training Loss: 3.9800624298095704, Average Validation Loss: 4.545499669744613
Batch 30: training loss 3.8017349243164062
Batch 60: training loss 4.08050012588501
Batch 90: training loss 3.8396668434143066
Batch 120: training loss 3.9375314712524414
Batch 150: training loss 4.072235107421875
Batch 180: training loss 3.906092643737793
Batch 210: training loss 4.1113152503967285
Batch 240: training loss 4.166172027587891
Batch 270: training loss 3.8324694633483887
Batch 300: training loss 4.028163909912109
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9180922508239746
Batch 360: training loss 3.964625835418701
Batch 390: training loss 3.930206060409546
Batch 420: training loss 3.7986960411071777
Batch 450: training loss 4.007525444030762
Batch 480: training loss 4.037267208099365
Batch 510: training loss 4.090959548950195
Batch 540: training loss 3.9509530067443848
Batch 570: training loss 3.925436496734619
Batch 600: training loss 3.983354330062866
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 130: Average Training Loss: 3.9749150424957276, Average Validation Loss: 4.537891235757382
Batch 30: training loss 3.912616729736328
Batch 60: training loss 4.223141193389893
Batch 90: training loss 3.882535934448242
Batch 120: training loss 3.9748051166534424
Batch 150: training loss 3.8346314430236816
Batch 180: training loss 3.9810104370117188
Batch 210: training loss 3.976726531982422
Batch 240: training loss 3.9421284198760986
Batch 270: training loss 4.03480339050293
Batch 300: training loss 3.953911781311035
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.362698078155518
Batch 360: training loss 3.842592239379883
Batch 390: training loss 4.103113174438477
Batch 420: training loss 4.055954933166504
Batch 450: training loss 3.8902759552001953
Batch 480: training loss 4.016457557678223
Batch 510: training loss 3.8621091842651367
Batch 540: training loss 4.088382720947266
Batch 570: training loss 4.007160186767578
Batch 600: training loss 4.07865047454834
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 131: Average Training Loss: 3.9707926486968996, Average Validation Loss: 4.53122008100469
Batch 30: training loss 3.911649465560913
Batch 60: training loss 3.849804639816284
Batch 90: training loss 4.000977516174316
Batch 120: training loss 4.0189313888549805
Batch 150: training loss 4.085106372833252
Batch 180: training loss 4.035282611846924
Batch 210: training loss 3.8960230350494385
Batch 240: training loss 4.047360420227051
Batch 270: training loss 3.9098334312438965
Batch 300: training loss 3.9597856998443604
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.0031843185424805
Batch 360: training loss 3.866079568862915
Batch 390: training loss 4.00740385055542
Batch 420: training loss 3.9778707027435303
Batch 450: training loss 3.8694043159484863
Batch 480: training loss 3.9426512718200684
Batch 510: training loss 4.098756313323975
Batch 540: training loss 3.9589169025421143
Batch 570: training loss 4.0329484939575195
Batch 600: training loss 3.8974099159240723
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 132: Average Training Loss: 3.966292505264282, Average Validation Loss: 4.530806987843615
Batch 30: training loss 3.999412775039673
Batch 60: training loss 4.0814995765686035
Batch 90: training loss 3.9534292221069336
Batch 120: training loss 3.870579719543457
Batch 150: training loss 3.804910659790039
Batch 180: training loss 4.098650932312012
Batch 210: training loss 4.073764801025391
Batch 240: training loss 3.9202942848205566
Batch 270: training loss 3.924515724182129
Batch 300: training loss 4.088119983673096
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.839122772216797
Batch 360: training loss 3.9271111488342285
Batch 390: training loss 3.9688234329223633
Batch 420: training loss 4.086633205413818
Batch 450: training loss 4.097719669342041
Batch 480: training loss 3.928408145904541
Batch 510: training loss 4.065486907958984
Batch 540: training loss 4.152313232421875
Batch 570: training loss 4.015437602996826
Batch 600: training loss 3.931926727294922
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 133: Average Training Loss: 3.9648532764434816, Average Validation Loss: 4.524196097191344
Batch 30: training loss 3.855130195617676
Batch 60: training loss 3.8313074111938477
Batch 90: training loss 3.9549922943115234
Batch 120: training loss 4.102180480957031
Batch 150: training loss 3.7601747512817383
Batch 180: training loss 3.9737789630889893
Batch 210: training loss 4.06156063079834
Batch 240: training loss 4.083250045776367
Batch 270: training loss 3.8562846183776855
Batch 300: training loss 3.9337868690490723
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.035402774810791
Batch 360: training loss 3.8899197578430176
Batch 390: training loss 3.8748779296875
Batch 420: training loss 3.943721055984497
Batch 450: training loss 4.0504841804504395
Batch 480: training loss 3.841386318206787
Batch 510: training loss 4.102254867553711
Batch 540: training loss 3.9340367317199707
Batch 570: training loss 3.9666919708251953
Batch 600: training loss 3.909304141998291
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 134: Average Training Loss: 3.958480405807495, Average Validation Loss: 4.5225298556875675
Batch 30: training loss 3.653956651687622
Batch 60: training loss 3.86468768119812
Batch 90: training loss 3.833784580230713
Batch 120: training loss 3.9835047721862793
Batch 150: training loss 4.009695053100586
Batch 180: training loss 3.8672757148742676
Batch 210: training loss 3.9000959396362305
Batch 240: training loss 4.015077590942383
Batch 270: training loss 3.839907169342041
Batch 300: training loss 3.8665127754211426
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9780030250549316
Batch 360: training loss 3.8406996726989746
Batch 390: training loss 3.89790678024292
Batch 420: training loss 4.054944038391113
Batch 450: training loss 4.042086124420166
Batch 480: training loss 4.064237594604492
Batch 510: training loss 4.160006999969482
Batch 540: training loss 3.8314552307128906
Batch 570: training loss 3.6761932373046875
Batch 600: training loss 3.923929214477539
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 135: Average Training Loss: 3.954413430404663, Average Validation Loss: 4.518431135948668
Batch 30: training loss 3.9250681400299072
Batch 60: training loss 4.078365325927734
Batch 90: training loss 3.8842480182647705
Batch 120: training loss 4.096977233886719
Batch 150: training loss 4.013973712921143
Batch 180: training loss 3.9768409729003906
Batch 210: training loss 4.002996444702148
Batch 240: training loss 3.9026923179626465
Batch 270: training loss 3.9585297107696533
Batch 300: training loss 3.9474143981933594
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.943385601043701
Batch 360: training loss 3.7813374996185303
Batch 390: training loss 3.8138036727905273
Batch 420: training loss 3.964250087738037
Batch 450: training loss 3.7557249069213867
Batch 480: training loss 3.7319679260253906
Batch 510: training loss 3.9537100791931152
Batch 540: training loss 3.9786815643310547
Batch 570: training loss 4.190163612365723
Batch 600: training loss 3.8408584594726562
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 136: Average Training Loss: 3.9511317710876463, Average Validation Loss: 4.508936110963213
Batch 30: training loss 3.732304096221924
Batch 60: training loss 3.9429678916931152
Batch 90: training loss 3.8681600093841553
Batch 120: training loss 4.048249244689941
Batch 150: training loss 4.016986846923828
Batch 180: training loss 3.9437222480773926
Batch 210: training loss 3.940229892730713
Batch 240: training loss 3.978226661682129
Batch 270: training loss 3.8744282722473145
Batch 300: training loss 3.966123342514038
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7046010494232178
Batch 360: training loss 4.0351433753967285
Batch 390: training loss 3.891963005065918
Batch 420: training loss 3.916508197784424
Batch 450: training loss 4.099647045135498
Batch 480: training loss 4.030876636505127
Batch 510: training loss 3.8730220794677734
Batch 540: training loss 3.981081008911133
Batch 570: training loss 3.935389995574951
Batch 600: training loss 4.1431756019592285
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 137: Average Training Loss: 3.9460654426574706, Average Validation Loss: 4.512920024547171
Batch 30: training loss 3.9120707511901855
Batch 60: training loss 4.1712965965271
Batch 90: training loss 4.102536201477051
Batch 120: training loss 3.8761720657348633
Batch 150: training loss 4.111451148986816
Batch 180: training loss 4.025092124938965
Batch 210: training loss 4.048232078552246
Batch 240: training loss 3.997607707977295
Batch 270: training loss 4.058738708496094
Batch 300: training loss 3.9381015300750732
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8631246089935303
Batch 360: training loss 4.020148277282715
Batch 390: training loss 4.05729341506958
Batch 420: training loss 4.004263401031494
Batch 450: training loss 4.0338945388793945
Batch 480: training loss 3.9014649391174316
Batch 510: training loss 4.086586952209473
Batch 540: training loss 3.9058609008789062
Batch 570: training loss 3.7932071685791016
Batch 600: training loss 4.005334377288818
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 138: Average Training Loss: 3.9410380477905274, Average Validation Loss: 4.509047792312947
Batch 30: training loss 3.9765403270721436
Batch 60: training loss 3.896575927734375
Batch 90: training loss 3.8908700942993164
Batch 120: training loss 4.048688888549805
Batch 150: training loss 3.942577838897705
Batch 180: training loss 3.792557716369629
Batch 210: training loss 3.9172253608703613
Batch 240: training loss 3.904888153076172
Batch 270: training loss 3.7764594554901123
Batch 300: training loss 3.794769048690796
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9395828247070312
Batch 360: training loss 3.8101816177368164
Batch 390: training loss 3.8793485164642334
Batch 420: training loss 4.054294109344482
Batch 450: training loss 3.866541624069214
Batch 480: training loss 3.787003755569458
Batch 510: training loss 3.9936180114746094
Batch 540: training loss 3.8772988319396973
Batch 570: training loss 4.305009365081787
Batch 600: training loss 3.916525363922119
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 139: Average Training Loss: 3.936873839187622, Average Validation Loss: 4.500299291407808
Batch 30: training loss 3.825526237487793
Batch 60: training loss 3.6925745010375977
Batch 90: training loss 3.9407458305358887
Batch 120: training loss 3.8245248794555664
Batch 150: training loss 4.059004783630371
Batch 180: training loss 3.887023687362671
Batch 210: training loss 3.8676040172576904
Batch 240: training loss 3.8421967029571533
Batch 270: training loss 4.117398262023926
Batch 300: training loss 3.9243175983428955
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.002080917358398
Batch 360: training loss 3.9315402507781982
Batch 390: training loss 3.8421502113342285
Batch 420: training loss 4.063526153564453
Batch 450: training loss 3.9715137481689453
Batch 480: training loss 3.9960460662841797
Batch 510: training loss 4.153315544128418
Batch 540: training loss 4.293416976928711
Batch 570: training loss 4.2062249183654785
Batch 600: training loss 3.904879570007324
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 140: Average Training Loss: 3.9330938095092773, Average Validation Loss: 4.50406355553485
Batch 30: training loss 3.86226749420166
Batch 60: training loss 3.8242082595825195
Batch 90: training loss 3.857365369796753
Batch 120: training loss 3.8927950859069824
Batch 150: training loss 3.809828758239746
Batch 180: training loss 4.003054618835449
Batch 210: training loss 3.960474967956543
Batch 240: training loss 4.007859230041504
Batch 270: training loss 4.154561519622803
Batch 300: training loss 3.823208808898926
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.877750873565674
Batch 360: training loss 4.014071464538574
Batch 390: training loss 4.058233261108398
Batch 420: training loss 3.8542680740356445
Batch 450: training loss 3.8803391456604004
Batch 480: training loss 4.042455673217773
Batch 510: training loss 4.021076202392578
Batch 540: training loss 3.96917462348938
Batch 570: training loss 3.9985594749450684
Batch 600: training loss 3.983973979949951
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 141: Average Training Loss: 3.928895993041992, Average Validation Loss: 4.505626850939811
Batch 30: training loss 3.7588393688201904
Batch 60: training loss 3.9425876140594482
Batch 90: training loss 3.8192801475524902
Batch 120: training loss 4.027750492095947
Batch 150: training loss 3.913834810256958
Batch 180: training loss 3.8671131134033203
Batch 210: training loss 4.0586347579956055
Batch 240: training loss 4.004952430725098
Batch 270: training loss 3.8743951320648193
Batch 300: training loss 4.171865940093994
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7435660362243652
Batch 360: training loss 3.9112539291381836
Batch 390: training loss 4.001452445983887
Batch 420: training loss 4.069889068603516
Batch 450: training loss 3.813324451446533
Batch 480: training loss 3.9696502685546875
Batch 510: training loss 4.028500556945801
Batch 540: training loss 3.895646095275879
Batch 570: training loss 3.833937168121338
Batch 600: training loss 3.822124481201172
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 142: Average Training Loss: 3.9255279029846193, Average Validation Loss: 4.49568454255449
Batch 30: training loss 3.923281669616699
Batch 60: training loss 3.776529312133789
Batch 90: training loss 3.9434080123901367
Batch 120: training loss 3.972583532333374
Batch 150: training loss 3.749927520751953
Batch 180: training loss 3.850468397140503
Batch 210: training loss 4.242497444152832
Batch 240: training loss 3.8844051361083984
Batch 270: training loss 3.94118332862854
Batch 300: training loss 3.7121376991271973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.985224962234497
Batch 360: training loss 4.029664039611816
Batch 390: training loss 3.8091063499450684
Batch 420: training loss 4.219780921936035
Batch 450: training loss 3.801146984100342
Batch 480: training loss 3.993485927581787
Batch 510: training loss 3.948887825012207
Batch 540: training loss 4.015580654144287
Batch 570: training loss 3.5938143730163574
Batch 600: training loss 3.9733550548553467
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 143: Average Training Loss: 3.922266905593872, Average Validation Loss: 4.494523931056895
Batch 30: training loss 3.7688331604003906
Batch 60: training loss 3.8800830841064453
Batch 90: training loss 3.7969489097595215
Batch 120: training loss 4.10407829284668
Batch 150: training loss 3.8191308975219727
Batch 180: training loss 3.8205554485321045
Batch 210: training loss 3.9861979484558105
Batch 240: training loss 3.838141918182373
Batch 270: training loss 3.9173409938812256
Batch 300: training loss 3.9658961296081543
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9075889587402344
Batch 360: training loss 3.790639877319336
Batch 390: training loss 3.842844009399414
Batch 420: training loss 3.9160003662109375
Batch 450: training loss 3.7246429920196533
Batch 480: training loss 3.899956464767456
Batch 510: training loss 3.9862403869628906
Batch 540: training loss 4.142430782318115
Batch 570: training loss 3.6580681800842285
Batch 600: training loss 4.014146327972412
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 144: Average Training Loss: 3.9172756053924562, Average Validation Loss: 4.492153096706309
Batch 30: training loss 3.8794186115264893
Batch 60: training loss 3.7310657501220703
Batch 90: training loss 3.7874755859375
Batch 120: training loss 3.8454315662384033
Batch 150: training loss 3.9585821628570557
Batch 180: training loss 3.8511219024658203
Batch 210: training loss 3.9214353561401367
Batch 240: training loss 4.038936614990234
Batch 270: training loss 3.9486756324768066
Batch 300: training loss 3.8081021308898926
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.998199462890625
Batch 360: training loss 3.9350674152374268
Batch 390: training loss 3.797698974609375
Batch 420: training loss 4.03465461730957
Batch 450: training loss 3.8408334255218506
Batch 480: training loss 3.8576836585998535
Batch 510: training loss 3.878363609313965
Batch 540: training loss 3.8142900466918945
Batch 570: training loss 3.937617778778076
Batch 600: training loss 4.094119071960449
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 145: Average Training Loss: 3.913935304641724, Average Validation Loss: 4.49512830693671
Batch 30: training loss 3.91451358795166
Batch 60: training loss 3.863003969192505
Batch 90: training loss 3.853085994720459
Batch 120: training loss 3.8178224563598633
Batch 150: training loss 3.7780728340148926
Batch 180: training loss 4.0031962394714355
Batch 210: training loss 3.716540813446045
Batch 240: training loss 3.861692428588867
Batch 270: training loss 3.6936662197113037
Batch 300: training loss 3.7302184104919434
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8619027137756348
Batch 360: training loss 3.895075798034668
Batch 390: training loss 3.8289175033569336
Batch 420: training loss 3.882115364074707
Batch 450: training loss 4.057429313659668
Batch 480: training loss 3.990572214126587
Batch 510: training loss 3.860414743423462
Batch 540: training loss 3.778024196624756
Batch 570: training loss 3.8181772232055664
Batch 600: training loss 3.827155828475952
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 146: Average Training Loss: 3.909121092605591, Average Validation Loss: 4.4788973179269345
Batch 30: training loss 3.9853363037109375
Batch 60: training loss 3.8751020431518555
Batch 90: training loss 3.841360092163086
Batch 120: training loss 3.816288948059082
Batch 150: training loss 4.029013156890869
Batch 180: training loss 3.8450050354003906
Batch 210: training loss 3.7946059703826904
Batch 240: training loss 3.7136282920837402
Batch 270: training loss 3.880864143371582
Batch 300: training loss 3.888448715209961
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8814549446105957
Batch 360: training loss 4.014233589172363
Batch 390: training loss 3.983273983001709
Batch 420: training loss 3.9229986667633057
Batch 450: training loss 3.947324275970459
Batch 480: training loss 3.96836519241333
Batch 510: training loss 3.7440695762634277
Batch 540: training loss 3.855379581451416
Batch 570: training loss 3.9600911140441895
Batch 600: training loss 3.9080915451049805
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 147: Average Training Loss: 3.9066024089813234, Average Validation Loss: 4.481061793388204
Batch 30: training loss 3.8856191635131836
Batch 60: training loss 3.940305709838867
Batch 90: training loss 3.789886951446533
Batch 120: training loss 3.8084640502929688
Batch 150: training loss 4.212658405303955
Batch 180: training loss 3.845236301422119
Batch 210: training loss 4.008378982543945
Batch 240: training loss 3.9294815063476562
Batch 270: training loss 3.948180675506592
Batch 300: training loss 3.881748914718628
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7285022735595703
Batch 360: training loss 3.980191230773926
Batch 390: training loss 3.860715627670288
Batch 420: training loss 3.6441712379455566
Batch 450: training loss 3.6171393394470215
Batch 480: training loss 3.8064217567443848
Batch 510: training loss 3.888338327407837
Batch 540: training loss 3.7571797370910645
Batch 570: training loss 3.7976584434509277
Batch 600: training loss 4.150485515594482
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 148: Average Training Loss: 3.901348273849487, Average Validation Loss: 4.478795853066952
Batch 30: training loss 3.718843936920166
Batch 60: training loss 3.9353830814361572
Batch 90: training loss 3.962489128112793
Batch 120: training loss 3.9609851837158203
Batch 150: training loss 3.8094208240509033
Batch 180: training loss 3.9956798553466797
Batch 210: training loss 3.8963332176208496
Batch 240: training loss 3.914473533630371
Batch 270: training loss 3.906667709350586
Batch 300: training loss 3.7318620681762695
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8765547275543213
Batch 360: training loss 3.9218244552612305
Batch 390: training loss 3.9004690647125244
Batch 420: training loss 3.878892421722412
Batch 450: training loss 3.9680399894714355
Batch 480: training loss 4.108392715454102
Batch 510: training loss 3.9478302001953125
Batch 540: training loss 3.806774616241455
Batch 570: training loss 4.0184783935546875
Batch 600: training loss 4.058744430541992
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 149: Average Training Loss: 3.898391332244873, Average Validation Loss: 4.471120012567399
Batch 30: training loss 3.7961082458496094
Batch 60: training loss 4.295041561126709
Batch 90: training loss 3.696648359298706
Batch 120: training loss 3.8597676753997803
Batch 150: training loss 4.002884864807129
Batch 180: training loss 3.8945066928863525
Batch 210: training loss 3.8506762981414795
Batch 240: training loss 3.7209672927856445
Batch 270: training loss 3.8052597045898438
Batch 300: training loss 4.017460823059082
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.761019229888916
Batch 360: training loss 3.7194876670837402
Batch 390: training loss 3.962258815765381
Batch 420: training loss 4.02269172668457
Batch 450: training loss 4.00946569442749
Batch 480: training loss 4.021066188812256
Batch 510: training loss 4.346453666687012
Batch 540: training loss 3.7335636615753174
Batch 570: training loss 3.9248266220092773
Batch 600: training loss 3.979524612426758
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 150: Average Training Loss: 3.8953765941619873, Average Validation Loss: 4.4733064732653025
Batch 30: training loss 3.8774161338806152
Batch 60: training loss 3.885866165161133
Batch 90: training loss 3.8731956481933594
Batch 120: training loss 3.849839448928833
Batch 150: training loss 3.8668556213378906
Batch 180: training loss 3.7936630249023438
Batch 210: training loss 4.00583028793335
Batch 240: training loss 4.014742374420166
Batch 270: training loss 3.9664816856384277
Batch 300: training loss 4.05587100982666
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7324018478393555
Batch 360: training loss 3.8882062435150146
Batch 390: training loss 4.173061847686768
Batch 420: training loss 3.913989305496216
Batch 450: training loss 3.666484832763672
Batch 480: training loss 4.005804061889648
Batch 510: training loss 4.322915077209473
Batch 540: training loss 3.994166135787964
Batch 570: training loss 3.9301838874816895
Batch 600: training loss 3.845505714416504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 151: Average Training Loss: 3.8908369064331056, Average Validation Loss: 4.468552295197832
Batch 30: training loss 3.907733201980591
Batch 60: training loss 3.893831729888916
Batch 90: training loss 3.8330345153808594
Batch 120: training loss 3.831778049468994
Batch 150: training loss 3.704719066619873
Batch 180: training loss 3.858191967010498
Batch 210: training loss 3.9232921600341797
Batch 240: training loss 4.148108005523682
Batch 270: training loss 4.159576892852783
Batch 300: training loss 3.9524078369140625
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8477909564971924
Batch 360: training loss 3.8412437438964844
Batch 390: training loss 3.724782705307007
Batch 420: training loss 3.9531497955322266
Batch 450: training loss 3.742748737335205
Batch 480: training loss 4.137582302093506
Batch 510: training loss 3.967390537261963
Batch 540: training loss 3.9356350898742676
Batch 570: training loss 4.156418800354004
Batch 600: training loss 4.072063446044922
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 152: Average Training Loss: 3.888325590133667, Average Validation Loss: 4.459995807485377
Batch 30: training loss 3.7496042251586914
Batch 60: training loss 3.724224090576172
Batch 90: training loss 3.9559340476989746
Batch 120: training loss 3.835702896118164
Batch 150: training loss 3.895005226135254
Batch 180: training loss 3.6889240741729736
Batch 210: training loss 3.9927783012390137
Batch 240: training loss 3.9674792289733887
Batch 270: training loss 3.7975962162017822
Batch 300: training loss 3.8005874156951904
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8556747436523438
Batch 360: training loss 4.151272773742676
Batch 390: training loss 3.863476514816284
Batch 420: training loss 3.7797679901123047
Batch 450: training loss 3.9450972080230713
Batch 480: training loss 3.670926094055176
Batch 510: training loss 4.058488845825195
Batch 540: training loss 3.9291796684265137
Batch 570: training loss 3.750072479248047
Batch 600: training loss 3.882294178009033
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 153: Average Training Loss: 3.8860427471160888, Average Validation Loss: 4.456209365357744
Batch 30: training loss 3.9097981452941895
Batch 60: training loss 3.8554537296295166
Batch 90: training loss 3.759028911590576
Batch 120: training loss 3.929229497909546
Batch 150: training loss 3.7591958045959473
Batch 180: training loss 3.7536518573760986
Batch 210: training loss 4.099605560302734
Batch 240: training loss 4.017961502075195
Batch 270: training loss 3.898120880126953
Batch 300: training loss 4.247231483459473
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.870744228363037
Batch 360: training loss 3.9899179935455322
Batch 390: training loss 4.011285305023193
Batch 420: training loss 3.9618377685546875
Batch 450: training loss 3.8493082523345947
Batch 480: training loss 3.883638381958008
Batch 510: training loss 4.039765357971191
Batch 540: training loss 3.790165424346924
Batch 570: training loss 3.8568568229675293
Batch 600: training loss 3.8363428115844727
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 154: Average Training Loss: 3.8814053848266603, Average Validation Loss: 4.463303768888433
Batch 30: training loss 3.690164089202881
Batch 60: training loss 3.8014473915100098
Batch 90: training loss 3.7563300132751465
Batch 120: training loss 3.6664419174194336
Batch 150: training loss 3.7270917892456055
Batch 180: training loss 3.7798092365264893
Batch 210: training loss 4.015410423278809
Batch 240: training loss 3.968681812286377
Batch 270: training loss 3.8511910438537598
Batch 300: training loss 3.730757236480713
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8583226203918457
Batch 360: training loss 3.8940439224243164
Batch 390: training loss 3.9999773502349854
Batch 420: training loss 3.7142539024353027
Batch 450: training loss 4.011538505554199
Batch 480: training loss 3.9605045318603516
Batch 510: training loss 3.772928237915039
Batch 540: training loss 3.802431344985962
Batch 570: training loss 3.8835067749023438
Batch 600: training loss 3.837665557861328
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 155: Average Training Loss: 3.8779698917388914, Average Validation Loss: 4.458768073548662
Batch 30: training loss 3.7168965339660645
Batch 60: training loss 3.7651755809783936
Batch 90: training loss 3.8433032035827637
Batch 120: training loss 3.8805484771728516
Batch 150: training loss 3.74058198928833
Batch 180: training loss 3.9090065956115723
Batch 210: training loss 4.307884693145752
Batch 240: training loss 3.688354253768921
Batch 270: training loss 3.906658172607422
Batch 300: training loss 3.995898485183716
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.699957847595215
Batch 360: training loss 3.7903056144714355
Batch 390: training loss 3.7904796600341797
Batch 420: training loss 3.902209758758545
Batch 450: training loss 3.7824954986572266
Batch 480: training loss 4.109531879425049
Batch 510: training loss 4.022276878356934
Batch 540: training loss 4.010687828063965
Batch 570: training loss 3.8814620971679688
Batch 600: training loss 3.749337673187256
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 156: Average Training Loss: 3.874014605331421, Average Validation Loss: 4.455551269206595
Batch 30: training loss 3.957639455795288
Batch 60: training loss 3.9629592895507812
Batch 90: training loss 3.9065370559692383
Batch 120: training loss 4.027578830718994
Batch 150: training loss 3.7853307723999023
Batch 180: training loss 3.706779956817627
Batch 210: training loss 3.8712453842163086
Batch 240: training loss 3.8547000885009766
Batch 270: training loss 3.9024722576141357
Batch 300: training loss 3.962430953979492
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8174831867218018
Batch 360: training loss 3.858860731124878
Batch 390: training loss 3.8632335662841797
Batch 420: training loss 3.882519483566284
Batch 450: training loss 3.9247474670410156
Batch 480: training loss 3.831915855407715
Batch 510: training loss 3.8686561584472656
Batch 540: training loss 3.6979455947875977
Batch 570: training loss 4.384698390960693
Batch 600: training loss 3.801480293273926
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 157: Average Training Loss: 3.8700269779205323, Average Validation Loss: 4.449312230373951
Batch 30: training loss 4.012541770935059
Batch 60: training loss 3.6199514865875244
Batch 90: training loss 3.8778200149536133
Batch 120: training loss 3.629776954650879
Batch 150: training loss 3.7719831466674805
Batch 180: training loss 3.8083181381225586
Batch 210: training loss 3.9891059398651123
Batch 240: training loss 3.780935764312744
Batch 270: training loss 4.096725940704346
Batch 300: training loss 3.9114017486572266
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9951171875
Batch 360: training loss 3.8288607597351074
Batch 390: training loss 3.8800482749938965
Batch 420: training loss 3.6417107582092285
Batch 450: training loss 3.6898434162139893
Batch 480: training loss 4.028451919555664
Batch 510: training loss 3.9814963340759277
Batch 540: training loss 3.5843613147735596
Batch 570: training loss 3.762911319732666
Batch 600: training loss 4.067011833190918
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 158: Average Training Loss: 3.866581551742554, Average Validation Loss: 4.444407818165232
Batch 30: training loss 3.8106894493103027
Batch 60: training loss 3.8573250770568848
Batch 90: training loss 3.8678159713745117
Batch 120: training loss 3.531705141067505
Batch 150: training loss 4.111005783081055
Batch 180: training loss 3.9593749046325684
Batch 210: training loss 3.7565317153930664
Batch 240: training loss 4.027041912078857
Batch 270: training loss 3.734984874725342
Batch 300: training loss 3.9104397296905518
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7198104858398438
Batch 360: training loss 3.7548294067382812
Batch 390: training loss 3.9153542518615723
Batch 420: training loss 3.933774471282959
Batch 450: training loss 3.9857985973358154
Batch 480: training loss 3.9274368286132812
Batch 510: training loss 3.95656156539917
Batch 540: training loss 3.825622797012329
Batch 570: training loss 3.8173022270202637
Batch 600: training loss 3.6652450561523438
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 159: Average Training Loss: 3.8626198516845704, Average Validation Loss: 4.449043081161824
Batch 30: training loss 3.8987669944763184
Batch 60: training loss 3.661975383758545
Batch 90: training loss 3.5837230682373047
Batch 120: training loss 3.76524019241333
Batch 150: training loss 3.7586240768432617
Batch 180: training loss 3.754883289337158
Batch 210: training loss 3.632108688354492
Batch 240: training loss 3.7188923358917236
Batch 270: training loss 3.8541438579559326
Batch 300: training loss 3.851243257522583
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8202545642852783
Batch 360: training loss 3.6775827407836914
Batch 390: training loss 3.7981667518615723
Batch 420: training loss 4.131833076477051
Batch 450: training loss 3.909787654876709
Batch 480: training loss 3.8482818603515625
Batch 510: training loss 3.927886486053467
Batch 540: training loss 3.861884355545044
Batch 570: training loss 3.7162744998931885
Batch 600: training loss 4.010293006896973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 160: Average Training Loss: 3.861348235702515, Average Validation Loss: 4.437033764859463
Batch 30: training loss 3.7482194900512695
Batch 60: training loss 3.8047404289245605
Batch 90: training loss 4.0488691329956055
Batch 120: training loss 3.848445415496826
Batch 150: training loss 4.047760009765625
Batch 180: training loss 3.7992448806762695
Batch 210: training loss 3.8049120903015137
Batch 240: training loss 3.7962253093719482
Batch 270: training loss 3.9076356887817383
Batch 300: training loss 3.9038960933685303
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8907651901245117
Batch 360: training loss 3.913039445877075
Batch 390: training loss 3.6196448802948
Batch 420: training loss 3.857247829437256
Batch 450: training loss 3.6762702465057373
Batch 480: training loss 3.6326212882995605
Batch 510: training loss 3.8516604900360107
Batch 540: training loss 3.8204712867736816
Batch 570: training loss 3.871110439300537
Batch 600: training loss 3.9244489669799805
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 161: Average Training Loss: 3.857971216964722, Average Validation Loss: 4.441290003188113
Batch 30: training loss 3.6900634765625
Batch 60: training loss 4.098825454711914
Batch 90: training loss 4.159493446350098
Batch 120: training loss 3.661529541015625
Batch 150: training loss 3.675086498260498
Batch 180: training loss 3.9142608642578125
Batch 210: training loss 4.0872697830200195
Batch 240: training loss 3.948232650756836
Batch 270: training loss 3.8962180614471436
Batch 300: training loss 3.6892151832580566
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9874653816223145
Batch 360: training loss 3.9436328411102295
Batch 390: training loss 3.8410744667053223
Batch 420: training loss 3.8662047386169434
Batch 450: training loss 3.7113094329833984
Batch 480: training loss 3.8296804428100586
Batch 510: training loss 3.7239489555358887
Batch 540: training loss 3.8832483291625977
Batch 570: training loss 3.8600423336029053
Batch 600: training loss 3.8676114082336426
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 162: Average Training Loss: 3.8532020160675047, Average Validation Loss: 4.439238964243138
Batch 30: training loss 3.8387179374694824
Batch 60: training loss 3.786358594894409
Batch 90: training loss 3.733992338180542
Batch 120: training loss 3.820065498352051
Batch 150: training loss 3.9047598838806152
Batch 180: training loss 3.910991668701172
Batch 210: training loss 3.7467422485351562
Batch 240: training loss 3.768049716949463
Batch 270: training loss 3.991121292114258
Batch 300: training loss 3.88840651512146
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9395203590393066
Batch 360: training loss 3.7562241554260254
Batch 390: training loss 3.7399673461914062
Batch 420: training loss 3.8962206840515137
Batch 450: training loss 3.833829402923584
Batch 480: training loss 3.8346447944641113
Batch 510: training loss 4.033055305480957
Batch 540: training loss 3.9380064010620117
Batch 570: training loss 3.76572847366333
Batch 600: training loss 3.7119107246398926
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 163: Average Training Loss: 3.84982201499939, Average Validation Loss: 4.433362626014872
Batch 30: training loss 3.8456737995147705
Batch 60: training loss 3.875793933868408
Batch 90: training loss 3.685189723968506
Batch 120: training loss 3.8136038780212402
Batch 150: training loss 3.9564242362976074
Batch 180: training loss 3.801581382751465
Batch 210: training loss 3.801877021789551
Batch 240: training loss 4.080057144165039
Batch 270: training loss 3.6162664890289307
Batch 300: training loss 3.895005226135254
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.724189519882202
Batch 360: training loss 3.9056448936462402
Batch 390: training loss 3.9869837760925293
Batch 420: training loss 3.757023572921753
Batch 450: training loss 3.8996236324310303
Batch 480: training loss 3.788428783416748
Batch 510: training loss 3.9309768676757812
Batch 540: training loss 3.9025301933288574
Batch 570: training loss 3.7382357120513916
Batch 600: training loss 3.6404361724853516
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 164: Average Training Loss: 3.8468289485931395, Average Validation Loss: 4.423231064005101
Batch 30: training loss 3.773810625076294
Batch 60: training loss 3.704458236694336
Batch 90: training loss 3.7070279121398926
Batch 120: training loss 3.886695384979248
Batch 150: training loss 3.89736270904541
Batch 180: training loss 3.9460556507110596
Batch 210: training loss 3.8568949699401855
Batch 240: training loss 3.8050286769866943
Batch 270: training loss 3.9682817459106445
Batch 300: training loss 4.095870018005371
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.568544864654541
Batch 360: training loss 3.8764305114746094
Batch 390: training loss 3.763864517211914
Batch 420: training loss 3.906266212463379
Batch 450: training loss 3.782465934753418
Batch 480: training loss 3.6744425296783447
Batch 510: training loss 4.023125648498535
Batch 540: training loss 4.047037601470947
Batch 570: training loss 3.719480037689209
Batch 600: training loss 3.725536346435547
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 165: Average Training Loss: 3.843618928527832, Average Validation Loss: 4.420914812290922
Batch 30: training loss 3.5464699268341064
Batch 60: training loss 3.9100914001464844
Batch 90: training loss 3.776761770248413
Batch 120: training loss 3.8393659591674805
Batch 150: training loss 3.7359683513641357
Batch 180: training loss 3.898287773132324
Batch 210: training loss 3.991332530975342
Batch 240: training loss 3.835336208343506
Batch 270: training loss 3.7648425102233887
Batch 300: training loss 3.8679251670837402
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.715836524963379
Batch 360: training loss 3.5760128498077393
Batch 390: training loss 3.9587035179138184
Batch 420: training loss 3.734649658203125
Batch 450: training loss 4.054691314697266
Batch 480: training loss 3.691763401031494
Batch 510: training loss 3.852445363998413
Batch 540: training loss 3.7808423042297363
Batch 570: training loss 3.633391857147217
Batch 600: training loss 3.6404500007629395
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 166: Average Training Loss: 3.8394303485870362, Average Validation Loss: 4.427920128436798
Batch 30: training loss 3.7611193656921387
Batch 60: training loss 3.9066505432128906
Batch 90: training loss 3.8101885318756104
Batch 120: training loss 3.6956300735473633
Batch 150: training loss 3.6924567222595215
Batch 180: training loss 3.6743950843811035
Batch 210: training loss 3.7971177101135254
Batch 240: training loss 4.034019947052002
Batch 270: training loss 3.8388006687164307
Batch 300: training loss 3.800137996673584
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.862989664077759
Batch 360: training loss 4.033307075500488
Batch 390: training loss 3.7519664764404297
Batch 420: training loss 3.755040168762207
Batch 450: training loss 3.891099452972412
Batch 480: training loss 3.7964534759521484
Batch 510: training loss 3.9680418968200684
Batch 540: training loss 3.7354955673217773
Batch 570: training loss 3.9399056434631348
Batch 600: training loss 3.952777862548828
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 167: Average Training Loss: 3.838991153717041, Average Validation Loss: 4.423561907829122
Batch 30: training loss 4.035806655883789
Batch 60: training loss 4.0202789306640625
Batch 90: training loss 3.8258185386657715
Batch 120: training loss 3.8024609088897705
Batch 150: training loss 3.8549861907958984
Batch 180: training loss 3.914212465286255
Batch 210: training loss 3.947380304336548
Batch 240: training loss 3.8034613132476807
Batch 270: training loss 3.7719974517822266
Batch 300: training loss 3.9542949199676514
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8964600563049316
Batch 360: training loss 3.864250898361206
Batch 390: training loss 3.99411678314209
Batch 420: training loss 3.894117593765259
Batch 450: training loss 3.7004029750823975
Batch 480: training loss 3.7269439697265625
Batch 510: training loss 4.057486534118652
Batch 540: training loss 3.9486095905303955
Batch 570: training loss 3.8802781105041504
Batch 600: training loss 3.916764736175537
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 168: Average Training Loss: 3.834593710708618, Average Validation Loss: 4.4237367447386395
Batch 30: training loss 3.7103095054626465
Batch 60: training loss 3.798046588897705
Batch 90: training loss 3.8623647689819336
Batch 120: training loss 3.955718517303467
Batch 150: training loss 3.8378348350524902
Batch 180: training loss 3.765702724456787
Batch 210: training loss 3.958846092224121
Batch 240: training loss 3.8536248207092285
Batch 270: training loss 3.721712589263916
Batch 300: training loss 3.7801918983459473
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.922006607055664
Batch 360: training loss 3.666527271270752
Batch 390: training loss 3.875269889831543
Batch 420: training loss 3.950562000274658
Batch 450: training loss 3.695462703704834
Batch 480: training loss 3.7964820861816406
Batch 510: training loss 3.886178970336914
Batch 540: training loss 3.702857732772827
Batch 570: training loss 3.9777708053588867
Batch 600: training loss 3.804990768432617
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 169: Average Training Loss: 3.8309842544555663, Average Validation Loss: 4.413352225689178
Batch 30: training loss 3.940479278564453
Batch 60: training loss 3.876340866088867
Batch 90: training loss 3.6681671142578125
Batch 120: training loss 3.928201198577881
Batch 150: training loss 3.8634021282196045
Batch 180: training loss 3.728914260864258
Batch 210: training loss 3.6651558876037598
Batch 240: training loss 3.949767589569092
Batch 270: training loss 3.818955898284912
Batch 300: training loss 3.8364930152893066
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.6581435203552246
Batch 360: training loss 3.7697954177856445
Batch 390: training loss 3.6429243087768555
Batch 420: training loss 3.7527670860290527
Batch 450: training loss 3.951859951019287
Batch 480: training loss 3.9209694862365723
Batch 510: training loss 3.955911874771118
Batch 540: training loss 3.810476779937744
Batch 570: training loss 3.955601692199707
Batch 600: training loss 3.9850406646728516
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 170: Average Training Loss: 3.8284864017486573, Average Validation Loss: 4.410975679438165
Batch 30: training loss 3.8235135078430176
Batch 60: training loss 4.1613030433654785
Batch 90: training loss 3.7458696365356445
Batch 120: training loss 3.8868207931518555
Batch 150: training loss 3.7020976543426514
Batch 180: training loss 3.7319047451019287
Batch 210: training loss 3.7847676277160645
Batch 240: training loss 3.823637008666992
Batch 270: training loss 3.8700156211853027
Batch 300: training loss 4.039259910583496
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.863896369934082
Batch 360: training loss 3.769317626953125
Batch 390: training loss 3.962326765060425
Batch 420: training loss 3.748410940170288
Batch 450: training loss 3.91323184967041
Batch 480: training loss 3.9059348106384277
Batch 510: training loss 3.844886302947998
Batch 540: training loss 3.8957290649414062
Batch 570: training loss 3.664034843444824
Batch 600: training loss 3.8319783210754395
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 171: Average Training Loss: 3.8236628311157226, Average Validation Loss: 4.414087194077512
Batch 30: training loss 3.8639941215515137
Batch 60: training loss 3.838711738586426
Batch 90: training loss 3.7012481689453125
Batch 120: training loss 3.8377702236175537
Batch 150: training loss 3.950563907623291
Batch 180: training loss 3.8387084007263184
Batch 210: training loss 3.6416735649108887
Batch 240: training loss 4.0273919105529785
Batch 270: training loss 3.91817569732666
Batch 300: training loss 3.9620537757873535
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.705432891845703
Batch 360: training loss 3.980555534362793
Batch 390: training loss 3.701716899871826
Batch 420: training loss 3.751894950866699
Batch 450: training loss 3.7690348625183105
Batch 480: training loss 3.5491178035736084
Batch 510: training loss 3.605954647064209
Batch 540: training loss 3.9925379753112793
Batch 570: training loss 4.114897727966309
Batch 600: training loss 3.615175724029541
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 172: Average Training Loss: 3.821247688293457, Average Validation Loss: 4.408972689445982
Batch 30: training loss 3.7352051734924316
Batch 60: training loss 3.7313575744628906
Batch 90: training loss 3.957158088684082
Batch 120: training loss 3.656838893890381
Batch 150: training loss 3.8660435676574707
Batch 180: training loss 3.755074977874756
Batch 210: training loss 3.7429113388061523
Batch 240: training loss 3.665900707244873
Batch 270: training loss 3.8592123985290527
Batch 300: training loss 3.611079454421997
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.901946544647217
Batch 360: training loss 3.8451051712036133
Batch 390: training loss 3.8462331295013428
Batch 420: training loss 3.9658684730529785
Batch 450: training loss 3.722060441970825
Batch 480: training loss 3.856520652770996
Batch 510: training loss 3.8058018684387207
Batch 540: training loss 3.858297824859619
Batch 570: training loss 3.9599533081054688
Batch 600: training loss 3.7844293117523193
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 173: Average Training Loss: 3.819891356277466, Average Validation Loss: 4.409211321079985
Batch 30: training loss 3.848034381866455
Batch 60: training loss 3.4776041507720947
Batch 90: training loss 3.667759895324707
Batch 120: training loss 3.818538188934326
Batch 150: training loss 3.578179359436035
Batch 180: training loss 3.9485597610473633
Batch 210: training loss 3.779536724090576
Batch 240: training loss 3.751343250274658
Batch 270: training loss 3.8254952430725098
Batch 300: training loss 3.859292984008789
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.005177974700928
Batch 360: training loss 3.973987102508545
Batch 390: training loss 3.943697929382324
Batch 420: training loss 3.753060817718506
Batch 450: training loss 3.8029894828796387
Batch 480: training loss 3.698577880859375
Batch 510: training loss 3.8188891410827637
Batch 540: training loss 3.8804728984832764
Batch 570: training loss 3.796560287475586
Batch 600: training loss 3.8329367637634277
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 174: Average Training Loss: 3.81552113571167, Average Validation Loss: 4.400839217165683
Batch 30: training loss 3.682446002960205
Batch 60: training loss 3.6984219551086426
Batch 90: training loss 3.8659520149230957
Batch 120: training loss 3.763192653656006
Batch 150: training loss 3.8110151290893555
Batch 180: training loss 3.7339794635772705
Batch 210: training loss 3.729773998260498
Batch 240: training loss 3.7988264560699463
Batch 270: training loss 3.9127516746520996
Batch 300: training loss 3.8082799911499023
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9687392711639404
Batch 360: training loss 3.684577226638794
Batch 390: training loss 3.892873764038086
Batch 420: training loss 3.9962358474731445
Batch 450: training loss 3.8407273292541504
Batch 480: training loss 3.8259291648864746
Batch 510: training loss 3.847865104675293
Batch 540: training loss 3.788663625717163
Batch 570: training loss 3.694725751876831
Batch 600: training loss 3.823694944381714
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 175: Average Training Loss: 3.8128743171691895, Average Validation Loss: 4.392187666385732
Batch 30: training loss 3.856300115585327
Batch 60: training loss 3.831167221069336
Batch 90: training loss 3.819885492324829
Batch 120: training loss 3.882206678390503
Batch 150: training loss 3.9281373023986816
Batch 180: training loss 4.044271469116211
Batch 210: training loss 3.7967371940612793
Batch 240: training loss 3.7809696197509766
Batch 270: training loss 3.622742176055908
Batch 300: training loss 4.049137592315674
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.008184909820557
Batch 360: training loss 3.628988265991211
Batch 390: training loss 3.7278223037719727
Batch 420: training loss 3.7625179290771484
Batch 450: training loss 3.619530200958252
Batch 480: training loss 3.9615936279296875
Batch 510: training loss 3.9293160438537598
Batch 540: training loss 3.705476760864258
Batch 570: training loss 3.754633903503418
Batch 600: training loss 3.996884822845459
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 176: Average Training Loss: 3.8093628414154055, Average Validation Loss: 4.3999713735377535
Batch 30: training loss 4.015182018280029
Batch 60: training loss 3.632638454437256
Batch 90: training loss 4.1111626625061035
Batch 120: training loss 3.8861162662506104
Batch 150: training loss 3.835533618927002
Batch 180: training loss 3.848911762237549
Batch 210: training loss 3.83796763420105
Batch 240: training loss 3.8737010955810547
Batch 270: training loss 3.818138837814331
Batch 300: training loss 3.707632541656494
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.043615818023682
Batch 360: training loss 3.810168743133545
Batch 390: training loss 3.8774943351745605
Batch 420: training loss 3.7762718200683594
Batch 450: training loss 3.740433692932129
Batch 480: training loss 3.737506866455078
Batch 510: training loss 3.933753490447998
Batch 540: training loss 3.9210128784179688
Batch 570: training loss 4.110060214996338
Batch 600: training loss 3.740941047668457
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 177: Average Training Loss: 3.805568599319458, Average Validation Loss: 4.393025834509667
Batch 30: training loss 3.6442041397094727
Batch 60: training loss 3.7406270503997803
Batch 90: training loss 3.824916362762451
Batch 120: training loss 3.7634267807006836
Batch 150: training loss 3.959603786468506
Batch 180: training loss 3.6607248783111572
Batch 210: training loss 3.6247098445892334
Batch 240: training loss 3.744042158126831
Batch 270: training loss 3.827548027038574
Batch 300: training loss 3.766930103302002
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9061930179595947
Batch 360: training loss 3.7166967391967773
Batch 390: training loss 3.7472169399261475
Batch 420: training loss 3.8883955478668213
Batch 450: training loss 3.753110885620117
Batch 480: training loss 3.7078139781951904
Batch 510: training loss 3.8001160621643066
Batch 540: training loss 3.74522066116333
Batch 570: training loss 3.937687397003174
Batch 600: training loss 3.895108461380005
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 178: Average Training Loss: 3.8040891109466552, Average Validation Loss: 4.396548149433542
Batch 30: training loss 3.672652006149292
Batch 60: training loss 3.665555238723755
Batch 90: training loss 3.7644243240356445
Batch 120: training loss 3.7697596549987793
Batch 150: training loss 3.7608096599578857
Batch 180: training loss 3.9359259605407715
Batch 210: training loss 3.8476200103759766
Batch 240: training loss 3.626382827758789
Batch 270: training loss 3.7797563076019287
Batch 300: training loss 3.8109092712402344
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.922015905380249
Batch 360: training loss 3.6361382007598877
Batch 390: training loss 3.71748423576355
Batch 420: training loss 3.879728317260742
Batch 450: training loss 3.754638195037842
Batch 480: training loss 3.783076763153076
Batch 510: training loss 3.7460343837738037
Batch 540: training loss 3.6300108432769775
Batch 570: training loss 3.605404853820801
Batch 600: training loss 3.9961423873901367
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 179: Average Training Loss: 3.8007306526184084, Average Validation Loss: 4.393016003547831
Batch 30: training loss 3.5835375785827637
Batch 60: training loss 3.7720131874084473
Batch 90: training loss 3.742335796356201
Batch 120: training loss 3.5752739906311035
Batch 150: training loss 3.927560567855835
Batch 180: training loss 4.029669284820557
Batch 210: training loss 3.805882453918457
Batch 240: training loss 3.926356315612793
Batch 270: training loss 3.7567906379699707
Batch 300: training loss 3.8179922103881836
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7980940341949463
Batch 360: training loss 3.842071771621704
Batch 390: training loss 3.9689886569976807
Batch 420: training loss 3.7358620166778564
Batch 450: training loss 3.6896166801452637
Batch 480: training loss 3.755786895751953
Batch 510: training loss 3.6780405044555664
Batch 540: training loss 3.854916572570801
Batch 570: training loss 3.8563642501831055
Batch 600: training loss 3.624133586883545
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 180: Average Training Loss: 3.797068994140625, Average Validation Loss: 4.3946663065159575
Batch 30: training loss 3.6929423809051514
Batch 60: training loss 3.9383745193481445
Batch 90: training loss 3.716364860534668
Batch 120: training loss 3.6503148078918457
Batch 150: training loss 3.829007387161255
Batch 180: training loss 3.677621364593506
Batch 210: training loss 3.799954414367676
Batch 240: training loss 4.054126739501953
Batch 270: training loss 3.908092975616455
Batch 300: training loss 3.7899956703186035
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8836708068847656
Batch 360: training loss 3.927694320678711
Batch 390: training loss 3.8202171325683594
Batch 420: training loss 3.845036506652832
Batch 450: training loss 3.903108596801758
Batch 480: training loss 3.877272129058838
Batch 510: training loss 4.015974998474121
Batch 540: training loss 3.8120620250701904
Batch 570: training loss 3.779289722442627
Batch 600: training loss 3.7099995613098145
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 181: Average Training Loss: 3.7943081035614012, Average Validation Loss: 4.379090806271168
Batch 30: training loss 3.7822165489196777
Batch 60: training loss 3.7277350425720215
Batch 90: training loss 3.6299962997436523
Batch 120: training loss 3.9489328861236572
Batch 150: training loss 3.720801830291748
Batch 180: training loss 3.8877594470977783
Batch 210: training loss 3.782109498977661
Batch 240: training loss 3.715909957885742
Batch 270: training loss 3.8130862712860107
Batch 300: training loss 3.834489345550537
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.537200450897217
Batch 360: training loss 3.9294071197509766
Batch 390: training loss 3.7703700065612793
Batch 420: training loss 3.5904970169067383
Batch 450: training loss 3.957141876220703
Batch 480: training loss 3.618560791015625
Batch 510: training loss 3.6934423446655273
Batch 540: training loss 3.799807548522949
Batch 570: training loss 3.648012161254883
Batch 600: training loss 3.922947406768799
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 182: Average Training Loss: 3.7915108612060546, Average Validation Loss: 4.3778493150751645
Batch 30: training loss 3.6941633224487305
Batch 60: training loss 3.8306360244750977
Batch 90: training loss 3.6426663398742676
Batch 120: training loss 3.8835105895996094
Batch 150: training loss 3.902533531188965
Batch 180: training loss 3.753323793411255
Batch 210: training loss 3.573047637939453
Batch 240: training loss 3.610391139984131
Batch 270: training loss 3.5172271728515625
Batch 300: training loss 3.874662399291992
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7242023944854736
Batch 360: training loss 3.6718482971191406
Batch 390: training loss 3.5457210540771484
Batch 420: training loss 3.87601375579834
Batch 450: training loss 3.946160316467285
Batch 480: training loss 3.991560935974121
Batch 510: training loss 3.682051181793213
Batch 540: training loss 3.580967426300049
Batch 570: training loss 3.6932406425476074
Batch 600: training loss 3.6144120693206787
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 183: Average Training Loss: 3.7884706104278565, Average Validation Loss: 4.382972220157055
Batch 30: training loss 3.8632311820983887
Batch 60: training loss 3.8935227394104004
Batch 90: training loss 3.534721851348877
Batch 120: training loss 3.879741668701172
Batch 150: training loss 3.5810904502868652
Batch 180: training loss 3.938446521759033
Batch 210: training loss 3.736114501953125
Batch 240: training loss 3.772627353668213
Batch 270: training loss 3.6667985916137695
Batch 300: training loss 3.7396607398986816
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8261160850524902
Batch 360: training loss 3.9077577590942383
Batch 390: training loss 3.6717395782470703
Batch 420: training loss 4.017325401306152
Batch 450: training loss 3.7847185134887695
Batch 480: training loss 3.5722105503082275
Batch 510: training loss 3.6695704460144043
Batch 540: training loss 3.894172191619873
Batch 570: training loss 3.8238630294799805
Batch 600: training loss 3.5928454399108887
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 184: Average Training Loss: 3.7870654296875, Average Validation Loss: 4.378488733413372
Batch 30: training loss 3.6096935272216797
Batch 60: training loss 3.6636366844177246
Batch 90: training loss 3.7718777656555176
Batch 120: training loss 3.6897292137145996
Batch 150: training loss 3.675431251525879
Batch 180: training loss 3.7911720275878906
Batch 210: training loss 3.8711905479431152
Batch 240: training loss 3.7015814781188965
Batch 270: training loss 3.770611524581909
Batch 300: training loss 3.8622732162475586
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.965508460998535
Batch 360: training loss 3.5872514247894287
Batch 390: training loss 3.885551691055298
Batch 420: training loss 3.685192584991455
Batch 450: training loss 3.7585372924804688
Batch 480: training loss 3.799539804458618
Batch 510: training loss 3.9614620208740234
Batch 540: training loss 3.913680076599121
Batch 570: training loss 4.004783630371094
Batch 600: training loss 3.592153787612915
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 185: Average Training Loss: 3.7837378383636473, Average Validation Loss: 4.377496313541494
Batch 30: training loss 3.903796911239624
Batch 60: training loss 3.962472915649414
Batch 90: training loss 3.6104071140289307
Batch 120: training loss 3.797232151031494
Batch 150: training loss 3.7506115436553955
Batch 180: training loss 3.720473527908325
Batch 210: training loss 3.9763503074645996
Batch 240: training loss 3.744961738586426
Batch 270: training loss 3.7714500427246094
Batch 300: training loss 3.6924381256103516
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7755212783813477
Batch 360: training loss 3.7802090644836426
Batch 390: training loss 3.891900062561035
Batch 420: training loss 3.93890380859375
Batch 450: training loss 3.8323075771331787
Batch 480: training loss 3.7422287464141846
Batch 510: training loss 3.7690138816833496
Batch 540: training loss 3.9589099884033203
Batch 570: training loss 3.8029909133911133
Batch 600: training loss 3.861640453338623
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 186: Average Training Loss: 3.781845166397095, Average Validation Loss: 4.373145448400619
Batch 30: training loss 3.701742649078369
Batch 60: training loss 3.724247455596924
Batch 90: training loss 3.8444623947143555
Batch 120: training loss 3.8780040740966797
Batch 150: training loss 3.885582447052002
Batch 180: training loss 3.693248748779297
Batch 210: training loss 3.6764378547668457
Batch 240: training loss 3.8703720569610596
Batch 270: training loss 3.7351272106170654
Batch 300: training loss 3.9151110649108887
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.693514347076416
Batch 360: training loss 3.646346092224121
Batch 390: training loss 3.568509101867676
Batch 420: training loss 3.7812180519104004
Batch 450: training loss 4.085373878479004
Batch 480: training loss 3.7560837268829346
Batch 510: training loss 3.863553047180176
Batch 540: training loss 3.819225549697876
Batch 570: training loss 3.948253870010376
Batch 600: training loss 3.7314109802246094
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 187: Average Training Loss: 3.77870778427124, Average Validation Loss: 4.370446428339532
Batch 30: training loss 3.7414329051971436
Batch 60: training loss 3.615889549255371
Batch 90: training loss 3.4777541160583496
Batch 120: training loss 3.6773972511291504
Batch 150: training loss 3.760143756866455
Batch 180: training loss 4.012208461761475
Batch 210: training loss 3.820343017578125
Batch 240: training loss 3.520846366882324
Batch 270: training loss 3.6588168144226074
Batch 300: training loss 3.713752269744873
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7350213527679443
Batch 360: training loss 3.810792922973633
Batch 390: training loss 3.722717523574829
Batch 420: training loss 3.6729395389556885
Batch 450: training loss 3.700953483581543
Batch 480: training loss 3.7211437225341797
Batch 510: training loss 3.8996355533599854
Batch 540: training loss 3.7728161811828613
Batch 570: training loss 3.702753782272339
Batch 600: training loss 3.904942750930786
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 188: Average Training Loss: 3.7743271884918212, Average Validation Loss: 4.377619256364539
Batch 30: training loss 3.863490343093872
Batch 60: training loss 3.856865406036377
Batch 90: training loss 3.7122135162353516
Batch 120: training loss 3.7582225799560547
Batch 150: training loss 3.955695629119873
Batch 180: training loss 3.6813888549804688
Batch 210: training loss 3.8596625328063965
Batch 240: training loss 3.5237302780151367
Batch 270: training loss 3.8068490028381348
Batch 300: training loss 3.9088687896728516
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.546624183654785
Batch 360: training loss 3.9516663551330566
Batch 390: training loss 3.693265438079834
Batch 420: training loss 3.6850032806396484
Batch 450: training loss 3.707166910171509
Batch 480: training loss 3.6458418369293213
Batch 510: training loss 3.7488627433776855
Batch 540: training loss 3.7560853958129883
Batch 570: training loss 3.94834041595459
Batch 600: training loss 3.7677061557769775
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 189: Average Training Loss: 3.772542986679077, Average Validation Loss: 4.3744236357668616
Batch 30: training loss 3.799224615097046
Batch 60: training loss 3.7972798347473145
Batch 90: training loss 3.853055000305176
Batch 120: training loss 3.901228189468384
Batch 150: training loss 3.9700112342834473
Batch 180: training loss 3.765841484069824
Batch 210: training loss 3.888889789581299
Batch 240: training loss 3.747687339782715
Batch 270: training loss 3.899773597717285
Batch 300: training loss 3.596444606781006
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.944094657897949
Batch 360: training loss 3.7104172706604004
Batch 390: training loss 3.975327968597412
Batch 420: training loss 3.8365907669067383
Batch 450: training loss 3.6890993118286133
Batch 480: training loss 3.7371466159820557
Batch 510: training loss 3.80100679397583
Batch 540: training loss 3.7966110706329346
Batch 570: training loss 3.6642024517059326
Batch 600: training loss 3.882308006286621
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 190: Average Training Loss: 3.7693132984161375, Average Validation Loss: 4.364690537148333
Batch 30: training loss 3.678330421447754
Batch 60: training loss 3.8613102436065674
Batch 90: training loss 3.7624971866607666
Batch 120: training loss 3.6273727416992188
Batch 150: training loss 3.73099422454834
Batch 180: training loss 3.579155921936035
Batch 210: training loss 3.8982961177825928
Batch 240: training loss 3.91202449798584
Batch 270: training loss 3.710310459136963
Batch 300: training loss 3.7381386756896973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7714483737945557
Batch 360: training loss 3.847982168197632
Batch 390: training loss 3.832940101623535
Batch 420: training loss 3.7843780517578125
Batch 450: training loss 3.855849027633667
Batch 480: training loss 3.6685543060302734
Batch 510: training loss 3.7753868103027344
Batch 540: training loss 3.6716177463531494
Batch 570: training loss 3.860161304473877
Batch 600: training loss 3.7910728454589844
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 191: Average Training Loss: 3.768367104721069, Average Validation Loss: 4.367294626033052
Batch 30: training loss 3.6999199390411377
Batch 60: training loss 3.6381335258483887
Batch 90: training loss 3.611697196960449
Batch 120: training loss 3.6930813789367676
Batch 150: training loss 3.797888994216919
Batch 180: training loss 3.5688867568969727
Batch 210: training loss 3.5957560539245605
Batch 240: training loss 3.8279237747192383
Batch 270: training loss 3.9554381370544434
Batch 300: training loss 3.7527294158935547
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7692952156066895
Batch 360: training loss 3.5940723419189453
Batch 390: training loss 3.780289649963379
Batch 420: training loss 3.623445510864258
Batch 450: training loss 3.790123462677002
Batch 480: training loss 3.578768014907837
Batch 510: training loss 3.4138691425323486
Batch 540: training loss 3.817478656768799
Batch 570: training loss 3.7386398315429688
Batch 600: training loss 3.9761109352111816
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 192: Average Training Loss: 3.7635501403808593, Average Validation Loss: 4.3626199072979865
Batch 30: training loss 3.9290006160736084
Batch 60: training loss 3.63778018951416
Batch 90: training loss 3.505136013031006
Batch 120: training loss 3.6941781044006348
Batch 150: training loss 3.784083843231201
Batch 180: training loss 3.7739782333374023
Batch 210: training loss 3.7159628868103027
Batch 240: training loss 3.6860837936401367
Batch 270: training loss 3.598173141479492
Batch 300: training loss 3.961880922317505
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8177809715270996
Batch 360: training loss 3.793773889541626
Batch 390: training loss 3.8826661109924316
Batch 420: training loss 3.9657037258148193
Batch 450: training loss 3.717503070831299
Batch 480: training loss 3.5705549716949463
Batch 510: training loss 3.673654079437256
Batch 540: training loss 3.8856348991394043
Batch 570: training loss 3.739299774169922
Batch 600: training loss 3.564335346221924
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 193: Average Training Loss: 3.760191542816162, Average Validation Loss: 4.358835666737658
Batch 30: training loss 3.7490222454071045
Batch 60: training loss 3.7099103927612305
Batch 90: training loss 3.78085994720459
Batch 120: training loss 3.641965389251709
Batch 150: training loss 3.5959184169769287
Batch 180: training loss 3.8377318382263184
Batch 210: training loss 3.6782069206237793
Batch 240: training loss 3.7524073123931885
Batch 270: training loss 3.6916866302490234
Batch 300: training loss 3.650951623916626
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7695517539978027
Batch 360: training loss 3.571521282196045
Batch 390: training loss 3.739975929260254
Batch 420: training loss 3.7716305255889893
Batch 450: training loss 3.672799587249756
Batch 480: training loss 3.7884581089019775
Batch 510: training loss 3.891674757003784
Batch 540: training loss 3.732130765914917
Batch 570: training loss 3.48852801322937
Batch 600: training loss 3.872246742248535
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 194: Average Training Loss: 3.7581533515930174, Average Validation Loss: 4.359898932436679
Batch 30: training loss 3.7631664276123047
Batch 60: training loss 3.6190261840820312
Batch 90: training loss 3.827768325805664
Batch 120: training loss 3.9322242736816406
Batch 150: training loss 3.8009986877441406
Batch 180: training loss 3.8334977626800537
Batch 210: training loss 3.9427499771118164
Batch 240: training loss 3.8712379932403564
Batch 270: training loss 3.8649744987487793
Batch 300: training loss 3.9192590713500977
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.728142023086548
Batch 360: training loss 3.7823588848114014
Batch 390: training loss 3.646045207977295
Batch 420: training loss 3.7089033126831055
Batch 450: training loss 3.726956367492676
Batch 480: training loss 3.6986989974975586
Batch 510: training loss 3.6221413612365723
Batch 540: training loss 3.9457521438598633
Batch 570: training loss 3.5381031036376953
Batch 600: training loss 3.877751350402832
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 195: Average Training Loss: 3.754773614501953, Average Validation Loss: 4.351371308590504
Batch 30: training loss 3.7782859802246094
Batch 60: training loss 3.8404459953308105
Batch 90: training loss 3.8709940910339355
Batch 120: training loss 3.6928203105926514
Batch 150: training loss 3.565504550933838
Batch 180: training loss 3.603593349456787
Batch 210: training loss 3.9188926219940186
Batch 240: training loss 3.8215975761413574
Batch 270: training loss 3.830231189727783
Batch 300: training loss 3.6491947174072266
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.589449405670166
Batch 360: training loss 3.7676658630371094
Batch 390: training loss 3.682076930999756
Batch 420: training loss 3.6890950202941895
Batch 450: training loss 3.87916898727417
Batch 480: training loss 3.6425209045410156
Batch 510: training loss 3.7227277755737305
Batch 540: training loss 3.881838798522949
Batch 570: training loss 3.7744312286376953
Batch 600: training loss 3.859039783477783
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 196: Average Training Loss: 3.7545444301605224, Average Validation Loss: 4.355415090601495
Batch 30: training loss 3.892435073852539
Batch 60: training loss 3.8229382038116455
Batch 90: training loss 3.676746129989624
Batch 120: training loss 3.6202969551086426
Batch 150: training loss 3.7894511222839355
Batch 180: training loss 3.834557294845581
Batch 210: training loss 3.7051539421081543
Batch 240: training loss 3.7992610931396484
Batch 270: training loss 3.8776636123657227
Batch 300: training loss 3.6694867610931396
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7465507984161377
Batch 360: training loss 3.6923439502716064
Batch 390: training loss 3.921471118927002
Batch 420: training loss 3.8713178634643555
Batch 450: training loss 3.588006019592285
Batch 480: training loss 3.621338367462158
Batch 510: training loss 3.7211523056030273
Batch 540: training loss 3.747114658355713
Batch 570: training loss 3.6227290630340576
Batch 600: training loss 3.848834991455078
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 197: Average Training Loss: 3.7512307598114014, Average Validation Loss: 4.351189166941541
Batch 30: training loss 3.8451666831970215
Batch 60: training loss 3.7971549034118652
Batch 90: training loss 3.5552797317504883
Batch 120: training loss 3.875288248062134
Batch 150: training loss 3.7220189571380615
Batch 180: training loss 3.745999813079834
Batch 210: training loss 3.506826877593994
Batch 240: training loss 3.496410608291626
Batch 270: training loss 3.768240451812744
Batch 300: training loss 4.04814338684082
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8071746826171875
Batch 360: training loss 3.6155648231506348
Batch 390: training loss 3.6328659057617188
Batch 420: training loss 3.842884063720703
Batch 450: training loss 3.726036548614502
Batch 480: training loss 3.601930856704712
Batch 510: training loss 3.5847034454345703
Batch 540: training loss 3.6277713775634766
Batch 570: training loss 4.121448993682861
Batch 600: training loss 3.7871415615081787
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 198: Average Training Loss: 3.7483674793243407, Average Validation Loss: 4.344450970913502
Batch 30: training loss 3.679689884185791
Batch 60: training loss 3.6229825019836426
Batch 90: training loss 3.743473529815674
Batch 120: training loss 3.8565926551818848
Batch 150: training loss 3.813103437423706
Batch 180: training loss 3.9210171699523926
Batch 210: training loss 3.5275964736938477
Batch 240: training loss 3.4910454750061035
Batch 270: training loss 3.6823978424072266
Batch 300: training loss 3.566619634628296
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.572655439376831
Batch 360: training loss 3.648249626159668
Batch 390: training loss 3.7848265171051025
Batch 420: training loss 3.4824631214141846
Batch 450: training loss 3.9297902584075928
Batch 480: training loss 3.655062675476074
Batch 510: training loss 3.7979612350463867
Batch 540: training loss 3.959050178527832
Batch 570: training loss 3.8321337699890137
Batch 600: training loss 3.687258720397949
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 199: Average Training Loss: 3.745971628189087, Average Validation Loss: 4.349222436864325
Batch 30: training loss 3.844804525375366
Batch 60: training loss 3.541574239730835
Batch 90: training loss 3.633176326751709
Batch 120: training loss 3.7577457427978516
Batch 150: training loss 3.866020441055298
Batch 180: training loss 3.7928333282470703
Batch 210: training loss 3.9316654205322266
Batch 240: training loss 3.685220718383789
Batch 270: training loss 3.502060890197754
Batch 300: training loss 3.9144058227539062
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.8770503997802734
Batch 360: training loss 3.7202253341674805
Batch 390: training loss 3.8397727012634277
Batch 420: training loss 3.9788436889648438
Batch 450: training loss 3.8197953701019287
Batch 480: training loss 3.6626434326171875
Batch 510: training loss 3.584806203842163
Batch 540: training loss 3.6327295303344727
Batch 570: training loss 3.8696351051330566
Batch 600: training loss 3.621253490447998
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 200: Average Training Loss: 3.7434974712371827, Average Validation Loss: 4.346446919948496
Batch 30: training loss 3.68341326713562
Batch 60: training loss 3.6726455688476562
Batch 90: training loss 3.671420097351074
Batch 120: training loss 3.957432746887207
Batch 150: training loss 3.73716139793396
Batch 180: training loss 3.5755646228790283
Batch 210: training loss 3.6315114498138428
Batch 240: training loss 3.5639028549194336
Batch 270: training loss 3.5581166744232178
Batch 300: training loss 3.811070203781128
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.9117493629455566
Batch 360: training loss 3.6760833263397217
Batch 390: training loss 3.745215892791748
Batch 420: training loss 3.7643401622772217
Batch 450: training loss 3.7828335762023926
Batch 480: training loss 3.568943977355957
Batch 510: training loss 3.7494959831237793
Batch 540: training loss 3.6981282234191895
Batch 570: training loss 3.7413384914398193
Batch 600: training loss 3.763705253601074
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 201: Average Training Loss: 3.7397883346557617, Average Validation Loss: 4.348318931904245
Batch 30: training loss 3.718698024749756
Batch 60: training loss 3.8939614295959473
Batch 90: training loss 3.8271381855010986
Batch 120: training loss 3.5596773624420166
Batch 150: training loss 3.6728808879852295
Batch 180: training loss 3.768000364303589
Batch 210: training loss 3.749101161956787
Batch 240: training loss 3.725677967071533
Batch 270: training loss 3.767542600631714
Batch 300: training loss 3.6303110122680664
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 4.079680442810059
Batch 360: training loss 3.7103750705718994
Batch 390: training loss 3.8412609100341797
Batch 420: training loss 3.7448339462280273
Batch 450: training loss 3.8206253051757812
Batch 480: training loss 3.4289190769195557
Batch 510: training loss 3.6425065994262695
Batch 540: training loss 3.7335758209228516
Batch 570: training loss 3.733426570892334
Batch 600: training loss 3.721161365509033
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 202: Average Training Loss: 3.7381842506408693, Average Validation Loss: 4.338476830340446
Batch 30: training loss 3.6371660232543945
Batch 60: training loss 3.801980972290039
Batch 90: training loss 4.027889251708984
Batch 120: training loss 3.9198431968688965
Batch 150: training loss 3.6679232120513916
Batch 180: training loss 3.8302831649780273
Batch 210: training loss 3.64713716506958
Batch 240: training loss 3.7217354774475098
Batch 270: training loss 3.6100449562072754
Batch 300: training loss 3.852142572402954
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.766561985015869
Batch 360: training loss 3.8471713066101074
Batch 390: training loss 3.6956143379211426
Batch 420: training loss 3.6723759174346924
Batch 450: training loss 3.768385887145996
Batch 480: training loss 3.8734545707702637
Batch 510: training loss 3.678771495819092
Batch 540: training loss 3.9540297985076904
Batch 570: training loss 3.5272433757781982
Batch 600: training loss 3.8853583335876465
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 203: Average Training Loss: 3.736165633392334, Average Validation Loss: 4.334492784865359
Batch 30: training loss 3.940424919128418
Batch 60: training loss 3.6256959438323975
Batch 90: training loss 3.7576041221618652
Batch 120: training loss 3.6161274909973145
Batch 150: training loss 3.7748889923095703
Batch 180: training loss 3.665625810623169
Batch 210: training loss 3.7288360595703125
Batch 240: training loss 3.7177186012268066
Batch 270: training loss 3.597461462020874
Batch 300: training loss 3.8527371883392334
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.640430450439453
Batch 360: training loss 3.6438093185424805
Batch 390: training loss 3.755173921585083
Batch 420: training loss 3.8938448429107666
Batch 450: training loss 4.089801788330078
Batch 480: training loss 3.8099372386932373
Batch 510: training loss 3.6157655715942383
Batch 540: training loss 3.8014626502990723
Batch 570: training loss 3.725705146789551
Batch 600: training loss 3.887603282928467
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 204: Average Training Loss: 3.733277753829956, Average Validation Loss: 4.336002826690674
Batch 30: training loss 3.791121482849121
Batch 60: training loss 3.4642977714538574
Batch 90: training loss 3.7828028202056885
Batch 120: training loss 3.5853047370910645
Batch 150: training loss 3.9065773487091064
Batch 180: training loss 3.745126247406006
Batch 210: training loss 3.7622509002685547
Batch 240: training loss 3.5970828533172607
Batch 270: training loss 3.856484889984131
Batch 300: training loss 3.7879390716552734
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.728545665740967
Batch 360: training loss 3.91259765625
Batch 390: training loss 3.836090087890625
Batch 420: training loss 3.7039690017700195
Batch 450: training loss 3.6295323371887207
Batch 480: training loss 3.556523323059082
Batch 510: training loss 3.841073751449585
Batch 540: training loss 3.64168643951416
Batch 570: training loss 3.731132984161377
Batch 600: training loss 3.887686252593994
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 205: Average Training Loss: 3.73042357673645, Average Validation Loss: 4.337511326404328
Batch 30: training loss 3.65065598487854
Batch 60: training loss 3.69746994972229
Batch 90: training loss 3.662482261657715
Batch 120: training loss 4.008072853088379
Batch 150: training loss 3.7223241329193115
Batch 180: training loss 3.554751396179199
Batch 210: training loss 3.780766248703003
Batch 240: training loss 3.647326946258545
Batch 270: training loss 3.6785941123962402
Batch 300: training loss 3.7770614624023438
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.6344566345214844
Batch 360: training loss 3.700117588043213
Batch 390: training loss 3.726353168487549
Batch 420: training loss 4.093282222747803
Batch 450: training loss 3.8949100971221924
Batch 480: training loss 3.7862229347229004
Batch 510: training loss 3.564701795578003
Batch 540: training loss 3.7562289237976074
Batch 570: training loss 3.8412764072418213
Batch 600: training loss 3.9723687171936035
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 206: Average Training Loss: 3.7285509029388426, Average Validation Loss: 4.335559134787702
Batch 30: training loss 3.747011184692383
Batch 60: training loss 3.6457037925720215
Batch 90: training loss 3.7393722534179688
Batch 120: training loss 3.739086627960205
Batch 150: training loss 4.034278869628906
Batch 180: training loss 3.532447338104248
Batch 210: training loss 3.916780471801758
Batch 240: training loss 3.8715333938598633
Batch 270: training loss 3.69588041305542
Batch 300: training loss 3.7175281047821045
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.611260414123535
Batch 360: training loss 3.77547550201416
Batch 390: training loss 3.8708648681640625
Batch 420: training loss 3.663114070892334
Batch 450: training loss 3.8628592491149902
Batch 480: training loss 3.7690978050231934
Batch 510: training loss 3.6138508319854736
Batch 540: training loss 3.882082939147949
Batch 570: training loss 3.6571121215820312
Batch 600: training loss 3.787649631500244
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 207: Average Training Loss: 3.7258680686950685, Average Validation Loss: 4.3311465243075755
Batch 30: training loss 3.667720079421997
Batch 60: training loss 3.5600595474243164
Batch 90: training loss 3.5666773319244385
Batch 120: training loss 3.5926012992858887
Batch 150: training loss 3.7006492614746094
Batch 180: training loss 3.571868658065796
Batch 210: training loss 3.693474769592285
Batch 240: training loss 3.7013885974884033
Batch 270: training loss 3.7329630851745605
Batch 300: training loss 3.5491068363189697
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.6417746543884277
Batch 360: training loss 3.5618042945861816
Batch 390: training loss 4.149560928344727
Batch 420: training loss 3.640469551086426
Batch 450: training loss 3.8102498054504395
Batch 480: training loss 3.6769332885742188
Batch 510: training loss 3.6970934867858887
Batch 540: training loss 3.715813159942627
Batch 570: training loss 3.8065552711486816
Batch 600: training loss 3.836345672607422
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 208: Average Training Loss: 3.7224786483764647, Average Validation Loss: 4.326786751442767
Batch 30: training loss 3.7692770957946777
Batch 60: training loss 3.5635366439819336
Batch 90: training loss 3.6697511672973633
Batch 120: training loss 3.895411491394043
Batch 150: training loss 3.959564208984375
Batch 180: training loss 3.8631865978240967
Batch 210: training loss 3.532893180847168
Batch 240: training loss 3.944721221923828
Batch 270: training loss 3.8114800453186035
Batch 300: training loss 3.7544236183166504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.7159876823425293
Batch 360: training loss 3.966317653656006
Batch 390: training loss 3.6939945220947266
Batch 420: training loss 3.792731761932373
Batch 450: training loss 3.9612841606140137
Batch 480: training loss 3.4665632247924805
Batch 510: training loss 3.6503677368164062
Batch 540: training loss 3.579749584197998
Batch 570: training loss 3.934351682662964
Batch 600: training loss 3.892624855041504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 209: Average Training Loss: 3.7201903881072997, Average Validation Loss: 4.327230250581782
Batch 30: training loss 3.7021703720092773
Batch 60: training loss 3.7038583755493164
Batch 90: training loss 3.5109610557556152
Batch 120: training loss 3.7224280834198
Batch 150: training loss 3.95513916015625
Batch 180: training loss 3.523935317993164
Batch 210: training loss 3.5959668159484863
Batch 240: training loss 3.906264305114746
Batch 270: training loss 3.619041681289673
Batch 300: training loss 3.7606797218322754
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.5959181785583496
Batch 360: training loss 3.845001697540283
Batch 390: training loss 3.752253770828247
Batch 420: training loss 3.6418638229370117
Batch 450: training loss 3.7347795963287354
Batch 480: training loss 3.742219924926758
Batch 510: training loss 3.7837650775909424
Batch 540: training loss 3.644052505493164
Batch 570: training loss 3.742318630218506
Batch 600: training loss 3.5958471298217773
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 210: Average Training Loss: 3.718196537017822, Average Validation Loss: 4.323437721171278
Batch 30: training loss 3.818894147872925
Batch 60: training loss 3.368654251098633
Batch 90: training loss 3.4491662979125977
Batch 120: training loss 3.6297411918640137
Batch 150: training loss 3.7884721755981445
Batch 180: training loss 3.607309341430664
Batch 210: training loss 3.6748132705688477
Batch 240: training loss 3.8951635360717773
Batch 270: training loss 3.5494112968444824
Batch 300: training loss 4.186617851257324
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.520575523376465
Batch 360: training loss 3.9665677547454834
Batch 390: training loss 4.033701419830322
Batch 420: training loss 3.8540327548980713
Batch 450: training loss 3.592508316040039
Batch 480: training loss 3.7961080074310303
Batch 510: training loss 3.9896349906921387
Batch 540: training loss 3.5668797492980957
Batch 570: training loss 3.777947425842285
Batch 600: training loss 3.793429374694824
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 211: Average Training Loss: 3.716170119857788, Average Validation Loss: 4.324539367188799
Batch 30: training loss 3.7010440826416016
Batch 60: training loss 3.671964645385742
Batch 90: training loss 3.532036781311035
Batch 120: training loss 3.7412569522857666
Batch 150: training loss 3.7275311946868896
Batch 180: training loss 3.624227523803711
Batch 210: training loss 3.754993438720703
Batch 240: training loss 3.7504732608795166
Batch 270: training loss 3.753512382507324
Batch 300: training loss 3.6710848808288574
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 330: training loss 3.6072335243225098
Batch 360: training loss 3.628387928009033
Batch 390: training loss 3.578972339630127
Batch 420: training loss 3.5819664001464844
Batch 450: training loss 3.5989720821380615
Batch 480: training loss 3.7818121910095215
Batch 510: training loss 3.6717915534973145
Batch 540: training loss 3.8036460876464844
Batch 570: training loss 3.7353692054748535
Batch 600: training loss 4.043490409851074
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Epoch 212: Average Training Loss: 3.71201770362854, Average Validation Loss: 4.3190391012962825
Batch 30: training loss 3.608768939971924
Batch 60: training loss 3.6241397857666016
Batch 90: training loss 3.8217835426330566
Batch 120: training loss 3.8360466957092285
Batch 150: training loss 3.887639045715332
^CSaving model...
Traceback (most recent call last):
  File "/home/e11824039/groups/192.039-2024W/capybaras/training_en_fr/train.py", line 174, in <module>
    train_loss, val_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, loss_function=loss_function,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e11824039/groups/192.039-2024W/capybaras/training_en_fr/train.py", line 124, in train_epoch
    predictions = model(sources, targets[:, :-1])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e11824039/groups/192.039-2024W/capybaras/training_en_fr/models/transformer.py", line 196, in forward
    dec_out = decoder_layer(dec_out, enc_out)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e11824039/groups/192.039-2024W/capybaras/training_en_fr/models/transformer.py", line 126, in forward
    att_out = self.attention(x, x, x)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e11824039/groups/192.039-2024W/capybaras/training_en_fr/models/transformer.py", line 58, in forward
    mask = torch.tril(torch.ones((sec_len, sec_len), requires_grad=False)).to(self.device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

Script done on 2025-01-26 20:40:13+00:00 [COMMAND_EXIT_CODE="130"]
