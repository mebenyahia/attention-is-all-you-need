Epoch 63: Average Training Loss: 2.785050943748129, Average Validation Loss: 3.222403128941854
Batch 100: tr loss 2.9071240425109863; avg tr loss 2.7539293932914735
Batch 200: tr loss 2.8445425033569336; avg tr loss 2.754512622356415
Batch 300: tr loss 2.735363006591797; avg tr loss 2.7563389865557353
Batch 400: tr loss 2.6631195545196533; avg tr loss 2.761102203130722
Batch 500: tr loss 2.85056734085083; avg tr loss 2.7645878138542175
Batch 600: tr loss 2.8703885078430176; avg tr loss 2.767042115132014
Batch 700: tr loss 2.840604782104492; avg tr loss 2.7662163921764917
Batch 800: tr loss 2.7005605697631836; avg tr loss 2.7665768256783485
Batch 900: tr loss 2.793145179748535; avg tr loss 2.767529537147946
Batch 1000: tr loss 2.888152599334717; avg tr loss 2.769515835523605
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.722165107727051; avg tr loss 2.7710284673083914
Batch 1200: tr loss 2.5288844108581543; avg tr loss 2.771697988907496
Batch 1300: tr loss 2.867615222930908; avg tr loss 2.7727383536558885
Batch 1400: tr loss 2.777179718017578; avg tr loss 2.772888033900942
Batch 1500: tr loss 2.8683950901031494; avg tr loss 2.774121851762136
Batch 1600: tr loss 2.8651976585388184; avg tr loss 2.7744207946956156
Batch 1700: tr loss 2.7567973136901855; avg tr loss 2.7752187595647926
Batch 1800: tr loss 2.8102660179138184; avg tr loss 2.776141952673594
Batch 1900: tr loss 2.9197022914886475; avg tr loss 2.7767841694229527
Batch 2000: tr loss 2.7113256454467773; avg tr loss 2.7782464650869367
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.696591377258301; avg tr loss 2.77920039608365
Batch 2200: tr loss 2.7537841796875; avg tr loss 2.779745310978456
Batch 2300: tr loss 2.7191290855407715; avg tr loss 2.7810872575511105
Epoch 64: Average Training Loss: 2.7811825803318935, Average Validation Loss: 3.2249264816443124
Batch 100: tr loss 2.797675848007202; avg tr loss 2.740725033283234
Batch 200: tr loss 2.600322723388672; avg tr loss 2.743523517847061
Batch 300: tr loss 2.5753140449523926; avg tr loss 2.748465972741445
Batch 400: tr loss 2.816422462463379; avg tr loss 2.7517310762405396
Batch 500: tr loss 2.5561156272888184; avg tr loss 2.7534629549980165
Batch 600: tr loss 2.665302276611328; avg tr loss 2.753673717578252
Batch 700: tr loss 2.704045295715332; avg tr loss 2.756576807158334
Batch 800: tr loss 2.984952926635742; avg tr loss 2.7597963228821754
Batch 900: tr loss 2.842090606689453; avg tr loss 2.7627725081973606
Batch 1000: tr loss 2.7892444133758545; avg tr loss 2.7640473022460936
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6851515769958496; avg tr loss 2.766293282292106
Batch 1200: tr loss 2.76659893989563; avg tr loss 2.768677592873573
Batch 1300: tr loss 2.8751959800720215; avg tr loss 2.769207472251012
Batch 1400: tr loss 2.7214553356170654; avg tr loss 2.7712783246380943
Batch 1500: tr loss 2.800258159637451; avg tr loss 2.772974481900533
Batch 1600: tr loss 2.76611590385437; avg tr loss 2.7729032173752786
Batch 1700: tr loss 2.719538688659668; avg tr loss 2.7734814872461206
Batch 1800: tr loss 2.7218403816223145; avg tr loss 2.773837095366584
Batch 1900: tr loss 2.831449508666992; avg tr loss 2.774573457617509
Batch 2000: tr loss 2.6522607803344727; avg tr loss 2.775707389354706
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.891249179840088; avg tr loss 2.7764186327798024
Batch 2200: tr loss 2.975860834121704; avg tr loss 2.7770512016253037
Batch 2300: tr loss 2.9079794883728027; avg tr loss 2.7775453716775647
Epoch 65: Average Training Loss: 2.7780641111904445, Average Validation Loss: 3.219694415728251
Batch 100: tr loss 2.927631139755249; avg tr loss 2.74360267162323
Batch 200: tr loss 2.840939998626709; avg tr loss 2.74311137676239
Batch 300: tr loss 2.8176028728485107; avg tr loss 2.7471023456255597
Batch 400: tr loss 2.730175495147705; avg tr loss 2.746766195297241
Batch 500: tr loss 2.6829094886779785; avg tr loss 2.749791721343994
Batch 600: tr loss 2.7810730934143066; avg tr loss 2.7535006511211395
Batch 700: tr loss 2.7292003631591797; avg tr loss 2.7548231785637993
Batch 800: tr loss 2.873188018798828; avg tr loss 2.7562260577082633
Batch 900: tr loss 2.7272210121154785; avg tr loss 2.7585481813218857
Batch 1000: tr loss 2.681396245956421; avg tr loss 2.7600706279277802
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7573485374450684; avg tr loss 2.761507351398468
Batch 1200: tr loss 2.743349552154541; avg tr loss 2.761946173508962
Batch 1300: tr loss 2.921769142150879; avg tr loss 2.762965435614953
Batch 1400: tr loss 2.788247585296631; avg tr loss 2.764743763889585
Batch 1500: tr loss 2.674834728240967; avg tr loss 2.766163050174713
Batch 1600: tr loss 2.791606903076172; avg tr loss 2.767533378303051
Batch 1700: tr loss 2.9483656883239746; avg tr loss 2.7671989224938787
Batch 1800: tr loss 2.7686920166015625; avg tr loss 2.768611727290683
Batch 1900: tr loss 2.613612651824951; avg tr loss 2.7698583427228427
Batch 2000: tr loss 2.7291340827941895; avg tr loss 2.7705585700273514
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.8945999145507812; avg tr loss 2.771413644041334
Batch 2200: tr loss 3.061988353729248; avg tr loss 2.772642933563753
Batch 2300: tr loss 2.704483985900879; avg tr loss 2.773373610040416
Epoch 66: Average Training Loss: 2.77391256285202, Average Validation Loss: 3.217286696036657
Batch 100: tr loss 2.7900543212890625; avg tr loss 2.7430905270576478
Batch 200: tr loss 2.648541212081909; avg tr loss 2.7477389895915985
Batch 300: tr loss 2.888583183288574; avg tr loss 2.749997661113739
Batch 400: tr loss 2.9120583534240723; avg tr loss 2.748560920357704
Batch 500: tr loss 2.903714179992676; avg tr loss 2.7515489768981936
Batch 600: tr loss 2.788848400115967; avg tr loss 2.752823620637258
Batch 700: tr loss 2.8537182807922363; avg tr loss 2.7547403870310103
Batch 800: tr loss 2.659867763519287; avg tr loss 2.7559923738241197
Batch 900: tr loss 2.7399468421936035; avg tr loss 2.756983162297143
Batch 1000: tr loss 2.842834949493408; avg tr loss 2.756973995447159
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6738882064819336; avg tr loss 2.757887563055212
Batch 1200: tr loss 2.7298083305358887; avg tr loss 2.7588128741582234
Batch 1300: tr loss 2.7714085578918457; avg tr loss 2.760435795417199
Batch 1400: tr loss 2.8582518100738525; avg tr loss 2.762204853466579
Batch 1500: tr loss 2.8267743587493896; avg tr loss 2.7643182713190715
Batch 1600: tr loss 2.7411415576934814; avg tr loss 2.7647477117180825
Batch 1700: tr loss 2.7519757747650146; avg tr loss 2.765584930532119
Batch 1800: tr loss 2.756518840789795; avg tr loss 2.7662316807111105
Batch 1900: tr loss 2.724630355834961; avg tr loss 2.766773137418847
Batch 2000: tr loss 2.78602933883667; avg tr loss 2.7675378782749176
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.937335968017578; avg tr loss 2.7687306692486717
Batch 2200: tr loss 2.7672698497772217; avg tr loss 2.769532114484093
Batch 2300: tr loss 2.831913471221924; avg tr loss 2.7704845514504806
Epoch 67: Average Training Loss: 2.7708371957007527, Average Validation Loss: 3.2177011966705322
Batch 100: tr loss 2.92716646194458; avg tr loss 2.7382580852508545
Batch 200: tr loss 2.7188849449157715; avg tr loss 2.735960637331009
Batch 300: tr loss 2.6967453956604004; avg tr loss 2.735757928689321
Batch 400: tr loss 2.8237454891204834; avg tr loss 2.7362982094287873
Batch 500: tr loss 2.7257423400878906; avg tr loss 2.7428406829833984
Batch 600: tr loss 2.870312452316284; avg tr loss 2.745262493689855
Batch 700: tr loss 2.7603859901428223; avg tr loss 2.7471688301222663
Batch 800: tr loss 2.8351197242736816; avg tr loss 2.7488514456152915
Batch 900: tr loss 2.716388702392578; avg tr loss 2.749243565665351
Batch 1000: tr loss 2.7206358909606934; avg tr loss 2.7515121829509734
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.877006769180298; avg tr loss 2.7545576537739147
Batch 1200: tr loss 2.758739471435547; avg tr loss 2.755378427505493
Batch 1300: tr loss 2.8080735206604004; avg tr loss 2.7578714838394753
Batch 1400: tr loss 2.6416611671447754; avg tr loss 2.7594941980498175
Batch 1500: tr loss 2.651665687561035; avg tr loss 2.7606749634742735
Batch 1600: tr loss 2.7898526191711426; avg tr loss 2.7615864688158034
Batch 1700: tr loss 2.957872152328491; avg tr loss 2.763613305652843
Batch 1800: tr loss 2.7465639114379883; avg tr loss 2.763653366698159
Batch 1900: tr loss 2.8014659881591797; avg tr loss 2.7646328573477894
Batch 2000: tr loss 2.693830728530884; avg tr loss 2.7654287190437317
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.9550230503082275; avg tr loss 2.765947478271666
Batch 2200: tr loss 2.8463501930236816; avg tr loss 2.766736675825986
Batch 2300: tr loss 2.668670177459717; avg tr loss 2.7668510348900504
Epoch 68: Average Training Loss: 2.7671627555894363, Average Validation Loss: 3.2175752023855844
Batch 100: tr loss 2.85601806640625; avg tr loss 2.7391318583488466
Batch 200: tr loss 2.6143574714660645; avg tr loss 2.7438618075847625
Batch 300: tr loss 2.65761137008667; avg tr loss 2.7423689277966816
Batch 400: tr loss 2.6986937522888184; avg tr loss 2.741374107003212
Batch 500: tr loss 2.726745128631592; avg tr loss 2.7419573183059693
Batch 600: tr loss 2.635180950164795; avg tr loss 2.7446430206298826
Batch 700: tr loss 2.789580821990967; avg tr loss 2.745954058510917
Batch 800: tr loss 2.912726640701294; avg tr loss 2.7480430495738983
Batch 900: tr loss 2.8798649311065674; avg tr loss 2.7505136121643914
Batch 1000: tr loss 2.6339497566223145; avg tr loss 2.7524410004615785
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.8074135780334473; avg tr loss 2.7515585008534518
Batch 1200: tr loss 2.7711448669433594; avg tr loss 2.753350882530212
Batch 1300: tr loss 2.8720474243164062; avg tr loss 2.754284756733821
Batch 1400: tr loss 2.585866928100586; avg tr loss 2.7552465769222803
Batch 1500: tr loss 2.6513831615448; avg tr loss 2.7564450240135194
Batch 1600: tr loss 2.7122409343719482; avg tr loss 2.7580039997398855
Batch 1700: tr loss 2.764000415802002; avg tr loss 2.7589303945092594
Batch 1800: tr loss 2.593130588531494; avg tr loss 2.7593574339813656
Batch 1900: tr loss 2.8058347702026367; avg tr loss 2.7600099300083363
Batch 2000: tr loss 2.8128390312194824; avg tr loss 2.760846412420273
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.8796944618225098; avg tr loss 2.7614716974894207
Batch 2200: tr loss 2.7786865234375; avg tr loss 2.7631192537871274
Batch 2300: tr loss 2.803370714187622; avg tr loss 2.7639119843814686
Epoch 69: Average Training Loss: 2.764167286531917, Average Validation Loss: 3.2143624424934387
Batch 100: tr loss 2.7814464569091797; avg tr loss 2.7388307762146
Batch 200: tr loss 2.8007798194885254; avg tr loss 2.7390931212902068
Batch 300: tr loss 2.7131354808807373; avg tr loss 2.737755363782247
Batch 400: tr loss 2.759359359741211; avg tr loss 2.741827208995819
Batch 500: tr loss 2.6459288597106934; avg tr loss 2.7454406452178954
Batch 600: tr loss 2.783273220062256; avg tr loss 2.747747803926468
Batch 700: tr loss 2.8582677841186523; avg tr loss 2.7498991669927326
Batch 800: tr loss 2.818981647491455; avg tr loss 2.7497113820910455
Batch 900: tr loss 2.809795618057251; avg tr loss 2.749731805589464
Batch 1000: tr loss 2.661098003387451; avg tr loss 2.7507022984027865
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.8233232498168945; avg tr loss 2.7518674312938343
Batch 1200: tr loss 2.7908177375793457; avg tr loss 2.7533568243185678
Batch 1300: tr loss 2.688842296600342; avg tr loss 2.753607073196998
Batch 1400: tr loss 2.79398250579834; avg tr loss 2.7539772306169783
Batch 1500: tr loss 2.774284601211548; avg tr loss 2.755482269446055
Batch 1600: tr loss 2.8395981788635254; avg tr loss 2.755291799902916
Batch 1700: tr loss 2.8249917030334473; avg tr loss 2.7559361251662757
Batch 1800: tr loss 2.642364501953125; avg tr loss 2.7550205408202277
Batch 1900: tr loss 2.7825019359588623; avg tr loss 2.7557526036312705
Batch 2000: tr loss 2.7816076278686523; avg tr loss 2.7572440346479414
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.8146867752075195; avg tr loss 2.7582535877681913
Batch 2200: tr loss 2.77667498588562; avg tr loss 2.7593699352307755
Batch 2300: tr loss 2.630774736404419; avg tr loss 2.7607930623966714
Epoch 70: Average Training Loss: 2.760932937417014, Average Validation Loss: 3.2187833885351815
Batch 100: tr loss 2.799727439880371; avg tr loss 2.7328553915023805
Batch 200: tr loss 2.830899715423584; avg tr loss 2.72731161236763
Batch 300: tr loss 2.7650928497314453; avg tr loss 2.730132536093394
Batch 400: tr loss 2.6970577239990234; avg tr loss 2.734341611862183
Batch 500: tr loss 2.753018379211426; avg tr loss 2.7357542366981504
Batch 600: tr loss 2.681713342666626; avg tr loss 2.7377902460098267
Batch 700: tr loss 2.734954833984375; avg tr loss 2.7373592233657837
Batch 800: tr loss 2.7128548622131348; avg tr loss 2.7398739627003668
Batch 900: tr loss 2.7586750984191895; avg tr loss 2.7414599129888746
Batch 1000: tr loss 2.910026788711548; avg tr loss 2.7417243728637697
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.716917037963867; avg tr loss 2.741480304327878
Batch 1200: tr loss 2.8102216720581055; avg tr loss 2.7433509838581087
Batch 1300: tr loss 2.737797260284424; avg tr loss 2.7445135619090153
Batch 1400: tr loss 2.6139814853668213; avg tr loss 2.7458217229161943
Batch 1500: tr loss 2.801388740539551; avg tr loss 2.747473596096039
Batch 1600: tr loss 2.6738338470458984; avg tr loss 2.7490225404500963
Batch 1700: tr loss 2.7921061515808105; avg tr loss 2.7496523809432984
Batch 1800: tr loss 2.85616397857666; avg tr loss 2.7514937954478795
Batch 1900: tr loss 2.785320997238159; avg tr loss 2.752867195606232
Batch 2000: tr loss 2.8207762241363525; avg tr loss 2.753537409901619
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.6995670795440674; avg tr loss 2.7548861047199793
Batch 2200: tr loss 2.753995895385742; avg tr loss 2.7563749687238173
Batch 2300: tr loss 2.5335774421691895; avg tr loss 2.757262957614401
Epoch 71: Average Training Loss: 2.7574601719607266, Average Validation Loss: 3.2119407951831818
Batch 100: tr loss 2.703793525695801; avg tr loss 2.7204130196571352
Batch 200: tr loss 2.6857364177703857; avg tr loss 2.725860983133316
Batch 300: tr loss 2.7628164291381836; avg tr loss 2.7271714981396995
Batch 400: tr loss 2.6990227699279785; avg tr loss 2.730291426181793
Batch 500: tr loss 2.7807445526123047; avg tr loss 2.7319783568382263
Batch 600: tr loss 2.643909454345703; avg tr loss 2.733329694668452
Batch 700: tr loss 2.7159619331359863; avg tr loss 2.733369722025735
Batch 800: tr loss 2.870182991027832; avg tr loss 2.7363190957903862
Batch 900: tr loss 2.7568225860595703; avg tr loss 2.7378956966929966
Batch 1000: tr loss 2.6387996673583984; avg tr loss 2.738522112607956
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.780609130859375; avg tr loss 2.7408284081112253
Batch 1200: tr loss 2.7673230171203613; avg tr loss 2.7427971373001734
Batch 1300: tr loss 2.7525546550750732; avg tr loss 2.743442106613746
Batch 1400: tr loss 2.6777148246765137; avg tr loss 2.7447829825537546
Batch 1500: tr loss 2.7702176570892334; avg tr loss 2.746254340648651
Batch 1600: tr loss 2.969809055328369; avg tr loss 2.747154269069433
Batch 1700: tr loss 2.8189024925231934; avg tr loss 2.747810798112084
Batch 1800: tr loss 2.9010276794433594; avg tr loss 2.7495206989182366
Batch 1900: tr loss 2.8294105529785156; avg tr loss 2.751120221740321
Batch 2000: tr loss 2.7212095260620117; avg tr loss 2.7519507296085357
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.777440071105957; avg tr loss 2.7529438677288236
Batch 2200: tr loss 2.7167320251464844; avg tr loss 2.7532607540217313
Batch 2300: tr loss 2.673340082168579; avg tr loss 2.7539905072295148
Epoch 72: Average Training Loss: 2.754354357109135, Average Validation Loss: 3.205307970444361
Batch 100: tr loss 2.896606683731079; avg tr loss 2.716364555358887
Batch 200: tr loss 2.8140530586242676; avg tr loss 2.71464834690094
Batch 300: tr loss 2.691868305206299; avg tr loss 2.719789554278056
Batch 400: tr loss 2.746462345123291; avg tr loss 2.722138409614563
Batch 500: tr loss 2.780247211456299; avg tr loss 2.723944634437561
Batch 600: tr loss 2.7011003494262695; avg tr loss 2.7273011394341786
Batch 700: tr loss 2.863602638244629; avg tr loss 2.732446582998548
Batch 800: tr loss 2.7052290439605713; avg tr loss 2.7336013063788416
Batch 900: tr loss 2.622389554977417; avg tr loss 2.734492797321743
Batch 1000: tr loss 2.8192739486694336; avg tr loss 2.7363983404636385
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.898247003555298; avg tr loss 2.7372610798749055
Batch 1200: tr loss 2.727602481842041; avg tr loss 2.739365235765775
Batch 1300: tr loss 2.634613513946533; avg tr loss 2.740464894954975
Batch 1400: tr loss 2.867457628250122; avg tr loss 2.743085073743548
Batch 1500: tr loss 2.7079291343688965; avg tr loss 2.744017317612966
Batch 1600: tr loss 2.8035497665405273; avg tr loss 2.7447050939500333
Batch 1700: tr loss 2.890298843383789; avg tr loss 2.7476749814257904
Batch 1800: tr loss 2.676527500152588; avg tr loss 2.7483097072442373
Batch 1900: tr loss 2.836397647857666; avg tr loss 2.7488821083620976
Batch 2000: tr loss 2.7231431007385254; avg tr loss 2.7499322283267973
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.749959945678711; avg tr loss 2.7505833829016915
Batch 2200: tr loss 2.8643338680267334; avg tr loss 2.7512586071274496
Batch 2300: tr loss 2.830056667327881; avg tr loss 2.7514563054623813
Epoch 73: Average Training Loss: 2.751681607229311, Average Validation Loss: 3.205737163623174
Batch 100: tr loss 2.6900124549865723; avg tr loss 2.724511594772339
Batch 200: tr loss 2.6435303688049316; avg tr loss 2.7210692274570465
Batch 300: tr loss 2.689034938812256; avg tr loss 2.723575279712677
Batch 400: tr loss 2.735867738723755; avg tr loss 2.7232069963216783
Batch 500: tr loss 2.8969411849975586; avg tr loss 2.7284591488838195
Batch 600: tr loss 2.600172996520996; avg tr loss 2.7296810976664223
Batch 700: tr loss 2.651310443878174; avg tr loss 2.732349171638489
Batch 800: tr loss 2.92818546295166; avg tr loss 2.733402553498745
Batch 900: tr loss 2.704089641571045; avg tr loss 2.7340652831395467
Batch 1000: tr loss 2.775207996368408; avg tr loss 2.7355372326374052
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7341275215148926; avg tr loss 2.736087586662986
Batch 1200: tr loss 2.7532639503479004; avg tr loss 2.737449913024902
Batch 1300: tr loss 2.579880714416504; avg tr loss 2.739147041577559
Batch 1400: tr loss 2.8615615367889404; avg tr loss 2.74060790845326
Batch 1500: tr loss 2.833340644836426; avg tr loss 2.7418532737096153
Batch 1600: tr loss 2.9327821731567383; avg tr loss 2.742480624765158
Batch 1700: tr loss 2.81925630569458; avg tr loss 2.7431083812433132
Batch 1800: tr loss 2.87086820602417; avg tr loss 2.7437382090091704
Batch 1900: tr loss 2.8905832767486572; avg tr loss 2.7450899654940555
Batch 2000: tr loss 2.756275177001953; avg tr loss 2.745551973938942
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.6310768127441406; avg tr loss 2.746450450306847
Batch 2200: tr loss 2.6263017654418945; avg tr loss 2.747648541493849
Batch 2300: tr loss 2.8430209159851074; avg tr loss 2.7476373081621914
Epoch 74: Average Training Loss: 2.747853116574141, Average Validation Loss: 3.2150601943333945
Batch 100: tr loss 2.790438652038574; avg tr loss 2.709908287525177
Batch 200: tr loss 2.5954208374023438; avg tr loss 2.7166704297065736
Batch 300: tr loss 2.7556939125061035; avg tr loss 2.7155514852205913
Batch 400: tr loss 2.7320570945739746; avg tr loss 2.724206439256668
Batch 500: tr loss 2.7384514808654785; avg tr loss 2.723807985305786
Batch 600: tr loss 2.7316720485687256; avg tr loss 2.7260906227429706
Batch 700: tr loss 2.714609146118164; avg tr loss 2.7290215563774107
Batch 800: tr loss 2.7210400104522705; avg tr loss 2.7300054943561554
Batch 900: tr loss 2.6431546211242676; avg tr loss 2.7307503864500258
Batch 1000: tr loss 2.735523223876953; avg tr loss 2.7312380340099334
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7370777130126953; avg tr loss 2.7320436510172756
Batch 1200: tr loss 2.6776249408721924; avg tr loss 2.7330190082391104
Batch 1300: tr loss 2.716324806213379; avg tr loss 2.734577818467067
Batch 1400: tr loss 2.6382932662963867; avg tr loss 2.736387919017247
Batch 1500: tr loss 2.7282087802886963; avg tr loss 2.737068760236104
Batch 1600: tr loss 2.7558727264404297; avg tr loss 2.738787381649017
Batch 1700: tr loss 2.5884652137756348; avg tr loss 2.739533658167895
Batch 1800: tr loss 2.7401771545410156; avg tr loss 2.739723858833313
Batch 1900: tr loss 2.7826156616210938; avg tr loss 2.7403873118601347
Batch 2000: tr loss 2.5635061264038086; avg tr loss 2.740853054881096
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7964954376220703; avg tr loss 2.7422162272816615
Batch 2200: tr loss 2.8058629035949707; avg tr loss 2.7436321491544895
Batch 2300: tr loss 2.6707658767700195; avg tr loss 2.744992623121842
Epoch 75: Average Training Loss: 2.7453190956913165, Average Validation Loss: 3.207565099000931
Batch 100: tr loss 2.687596082687378; avg tr loss 2.7170190715789797
Batch 200: tr loss 2.8820700645446777; avg tr loss 2.711801117658615
Batch 300: tr loss 2.641806125640869; avg tr loss 2.7133639001846315
Batch 400: tr loss 2.765291690826416; avg tr loss 2.7180180811882018
Batch 500: tr loss 2.743631601333618; avg tr loss 2.717241201877594
Batch 600: tr loss 2.5006117820739746; avg tr loss 2.7206015904744465
Batch 700: tr loss 2.7714452743530273; avg tr loss 2.7231478810310366
Batch 800: tr loss 2.7451272010803223; avg tr loss 2.7259679186344146
Batch 900: tr loss 2.733044385910034; avg tr loss 2.7265340277883743
Batch 1000: tr loss 2.770089626312256; avg tr loss 2.7282062368392945
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7182729244232178; avg tr loss 2.7294887334650215
Batch 1200: tr loss 2.6241602897644043; avg tr loss 2.7294919115304945
Batch 1300: tr loss 2.847984790802002; avg tr loss 2.731430900280292
Batch 1400: tr loss 2.7636661529541016; avg tr loss 2.732379536117826
Batch 1500: tr loss 2.7751641273498535; avg tr loss 2.734438414414724
Batch 1600: tr loss 2.7287850379943848; avg tr loss 2.7360185603797436
Batch 1700: tr loss 2.820186138153076; avg tr loss 2.7367447126612943
Batch 1800: tr loss 2.85333251953125; avg tr loss 2.738106280565262
Batch 1900: tr loss 2.692866325378418; avg tr loss 2.7388792295204967
Batch 2000: tr loss 2.8442673683166504; avg tr loss 2.7394528794288635
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7373857498168945; avg tr loss 2.7404503377278644
Batch 2200: tr loss 2.8529317378997803; avg tr loss 2.7419210748238996
Batch 2300: tr loss 2.6795973777770996; avg tr loss 2.74228363710901
Epoch 76: Average Training Loss: 2.742245274904238, Average Validation Loss: 3.2088738481203714
Batch 100: tr loss 2.7724080085754395; avg tr loss 2.704162106513977
Batch 200: tr loss 2.5693705081939697; avg tr loss 2.7097266352176668
Batch 300: tr loss 2.7655506134033203; avg tr loss 2.7078553811709085
Batch 400: tr loss 2.878662109375; avg tr loss 2.7151152217388153
Batch 500: tr loss 2.8954739570617676; avg tr loss 2.7154020147323608
Batch 600: tr loss 2.7303974628448486; avg tr loss 2.7180355751514433
Batch 700: tr loss 2.6983327865600586; avg tr loss 2.7204626968928745
Batch 800: tr loss 2.858367443084717; avg tr loss 2.7220323660969736
Batch 900: tr loss 2.765781879425049; avg tr loss 2.724309372107188
Batch 1000: tr loss 2.695268392562866; avg tr loss 2.72614803814888
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.

Batch 1100: tr loss 2.8262758255004883; avg tr loss 2.7301467392661354
Batch 1200: tr loss 2.6341934204101562; avg tr loss 2.7315465142329534
Batch 1300: tr loss 2.796811580657959; avg tr loss 2.7321637359032263
Batch 1400: tr loss 2.7787764072418213; avg tr loss 2.7323382471288955
Batch 1500: tr loss 2.783203125; avg tr loss 2.733319576581319
Batch 1600: tr loss 2.7712490558624268; avg tr loss 2.7332866847515107
Batch 1700: tr loss 2.8425745964050293; avg tr loss 2.7344953677233526
Batch 1800: tr loss 2.80249285697937; avg tr loss 2.7360203611850737
Batch 1900: tr loss 2.7973077297210693; avg tr loss 2.736799768397683
Batch 2000: tr loss 2.683988571166992; avg tr loss 2.737328655362129
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7930140495300293; avg tr loss 2.7373144159998213
Batch 2200: tr loss 2.682676315307617; avg tr loss 2.739316212805835
Batch 2300: tr loss 2.703774929046631; avg tr loss 2.739497633809629
Epoch 77: Average Training Loss: 2.7397979433829467, Average Validation Loss: 3.202822426954905
Batch 100: tr loss 2.9226737022399902; avg tr loss 2.6941776490211486
Batch 200: tr loss 2.781201124191284; avg tr loss 2.703214775323868
Batch 300: tr loss 2.712082862854004; avg tr loss 2.7079271117846173
Batch 400: tr loss 2.530460834503174; avg tr loss 2.710672971010208
Batch 500: tr loss 2.5490641593933105; avg tr loss 2.7107024655342102
Batch 600: tr loss 2.924257516860962; avg tr loss 2.713449405034383
Batch 700: tr loss 2.8681561946868896; avg tr loss 2.715429380621229
Batch 800: tr loss 2.730813503265381; avg tr loss 2.7170490074157714
Batch 900: tr loss 2.9762372970581055; avg tr loss 2.719590009583367
Batch 1000: tr loss 2.8926281929016113; avg tr loss 2.721553376674652
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.9967048168182373; avg tr loss 2.7243359899520874
Batch 1200: tr loss 2.5874149799346924; avg tr loss 2.724720744689306
Batch 1300: tr loss 2.6964914798736572; avg tr loss 2.7254620214609
Batch 1400: tr loss 2.7770419120788574; avg tr loss 2.7262160626479557
Batch 1500: tr loss 2.6720502376556396; avg tr loss 2.7278506269454956
Batch 1600: tr loss 2.877626419067383; avg tr loss 2.7294696961343288
Batch 1700: tr loss 2.6484832763671875; avg tr loss 2.7309119554126964
Batch 1800: tr loss 2.8202898502349854; avg tr loss 2.731292928987079
Batch 1900: tr loss 2.7184929847717285; avg tr loss 2.732560773648714
Batch 2000: tr loss 2.823457717895508; avg tr loss 2.7340485492944717
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.708897113800049; avg tr loss 2.734861838704064
Batch 2200: tr loss 2.7789688110351562; avg tr loss 2.7354283986308356
Batch 2300: tr loss 2.8132917881011963; avg tr loss 2.736953605879908
Epoch 78: Average Training Loss: 2.7371098876202877, Average Validation Loss: 3.2036908765633902
Batch 100: tr loss 2.7175052165985107; avg tr loss 2.7249293541908264
Batch 200: tr loss 2.6847667694091797; avg tr loss 2.7187548911571504
Batch 300: tr loss 2.6333205699920654; avg tr loss 2.715404415925344
Batch 400: tr loss 2.581130027770996; avg tr loss 2.7150268387794494
Batch 500: tr loss 2.7162747383117676; avg tr loss 2.7108706855773925
Batch 600: tr loss 2.635836601257324; avg tr loss 2.712638847430547
Batch 700: tr loss 2.692681312561035; avg tr loss 2.716035113675254
Batch 800: tr loss 2.740720272064209; avg tr loss 2.7176146060228348
Batch 900: tr loss 2.6520354747772217; avg tr loss 2.718311932881673
Batch 1000: tr loss 2.7073826789855957; avg tr loss 2.7197994997501374
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7629494667053223; avg tr loss 2.72140394189141
Batch 1200: tr loss 2.789752960205078; avg tr loss 2.722109943826993
Batch 1300: tr loss 2.673095941543579; avg tr loss 2.7238306250939
Batch 1400: tr loss 2.7909045219421387; avg tr loss 2.7245953035354615
Batch 1500: tr loss 2.653132438659668; avg tr loss 2.726155644575755
Batch 1600: tr loss 2.7539374828338623; avg tr loss 2.7276173801720143
Batch 1700: tr loss 2.6692466735839844; avg tr loss 2.7278231507189132
Batch 1800: tr loss 2.7474069595336914; avg tr loss 2.7293813653786976
Batch 1900: tr loss 2.849538803100586; avg tr loss 2.73091330591001
Batch 2000: tr loss 2.7145228385925293; avg tr loss 2.7315370116233826
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.671772003173828; avg tr loss 2.7327819391659327
Batch 2200: tr loss 2.7208452224731445; avg tr loss 2.733378271731463
Batch 2300: tr loss 2.8459925651550293; avg tr loss 2.7343407444331955
Epoch 79: Average Training Loss: 2.734380092637124, Average Validation Loss: 3.2080538868904114
Batch 100: tr loss 2.7111527919769287; avg tr loss 2.7004625535011293
Batch 200: tr loss 2.530160665512085; avg tr loss 2.7034853434562685
Batch 300: tr loss 2.890157699584961; avg tr loss 2.70664910475413
Batch 400: tr loss 2.7252163887023926; avg tr loss 2.711759964823723
Batch 500: tr loss 2.6267611980438232; avg tr loss 2.7116528005599974
Batch 600: tr loss 2.7566027641296387; avg tr loss 2.711663870016734
Batch 700: tr loss 2.829935073852539; avg tr loss 2.712962120260511
Batch 800: tr loss 2.7469468116760254; avg tr loss 2.712857042849064
Batch 900: tr loss 2.7021079063415527; avg tr loss 2.714274332523346
Batch 1000: tr loss 2.66440486907959; avg tr loss 2.716766084432602
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7785532474517822; avg tr loss 2.7185681091655383
Batch 1200: tr loss 2.815706968307495; avg tr loss 2.720474674701691
Batch 1300: tr loss 2.807790756225586; avg tr loss 2.721820433139801
Batch 1400: tr loss 2.7904934883117676; avg tr loss 2.7228814807959965
Batch 1500: tr loss 2.9156970977783203; avg tr loss 2.7236511945724486
Batch 1600: tr loss 2.788404941558838; avg tr loss 2.7241336192190646
Batch 1700: tr loss 2.75211763381958; avg tr loss 2.7254046522869784
Batch 1800: tr loss 2.567680835723877; avg tr loss 2.7262171053886415
Batch 1900: tr loss 2.722735643386841; avg tr loss 2.727209827899933
Batch 2000: tr loss 2.717729091644287; avg tr loss 2.728106889486313
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.870408058166504; avg tr loss 2.7284754153660367
Batch 2200: tr loss 2.668257713317871; avg tr loss 2.729575536251068
Batch 2300: tr loss 2.7821264266967773; avg tr loss 2.7306827536873195
Epoch 80: Average Training Loss: 2.7311659234579104, Average Validation Loss: 3.203938643137614
Batch 100: tr loss 2.6414709091186523; avg tr loss 2.6888056945800782
Batch 200: tr loss 2.7473556995391846; avg tr loss 2.6982021033763885
Batch 300: tr loss 2.7376351356506348; avg tr loss 2.7073506410916646
Batch 400: tr loss 2.7493984699249268; avg tr loss 2.7098653918504714
Batch 500: tr loss 2.6534242630004883; avg tr loss 2.710628347873688
Batch 600: tr loss 2.810363292694092; avg tr loss 2.7107801914215086
Batch 700: tr loss 2.5966849327087402; avg tr loss 2.7116914228030615
Batch 800: tr loss 2.6780333518981934; avg tr loss 2.711899893581867
Batch 900: tr loss 2.8036980628967285; avg tr loss 2.7133193193541634
Batch 1000: tr loss 2.71226167678833; avg tr loss 2.7146131269931795
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6866297721862793; avg tr loss 2.7156298687241294
Batch 1200: tr loss 2.977414131164551; avg tr loss 2.717725401918093
Batch 1300: tr loss 2.708341598510742; avg tr loss 2.718912057693188
Batch 1400: tr loss 2.628718376159668; avg tr loss 2.719812578473772
Batch 1500: tr loss 2.6702980995178223; avg tr loss 2.720897482395172
Batch 1600: tr loss 2.8384788036346436; avg tr loss 2.7219575043022632
Batch 1700: tr loss 2.807279109954834; avg tr loss 2.722133113075705
Batch 1800: tr loss 2.7279059886932373; avg tr loss 2.722804515891605
Batch 1900: tr loss 2.7463884353637695; avg tr loss 2.72471564267811
Batch 2000: tr loss 2.765718936920166; avg tr loss 2.7257634546756746
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.835569381713867; avg tr loss 2.726860316140311
Batch 2200: tr loss 2.587916374206543; avg tr loss 2.727612912979993
Batch 2300: tr loss 2.7601261138916016; avg tr loss 2.7285004655174587
Epoch 81: Average Training Loss: 2.728972953435911, Average Validation Loss: 3.200122594833374
Batch 100: tr loss 2.807926654815674; avg tr loss 2.7033229780197146
Batch 200: tr loss 2.7945961952209473; avg tr loss 2.6991104817390443
Batch 300: tr loss 2.8209638595581055; avg tr loss 2.697862394650777
Batch 400: tr loss 2.6467013359069824; avg tr loss 2.70221364736557
Batch 500: tr loss 2.5220208168029785; avg tr loss 2.704402237892151
Batch 600: tr loss 2.941431999206543; avg tr loss 2.7078750574588777
Batch 700: tr loss 2.783540725708008; avg tr loss 2.710315298352923
Batch 800: tr loss 2.674769878387451; avg tr loss 2.711594068109989
Batch 900: tr loss 2.831583261489868; avg tr loss 2.7134796436627706
Batch 1000: tr loss 2.8007893562316895; avg tr loss 2.71301859664917
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6712119579315186; avg tr loss 2.7152543982592494
Batch 1200: tr loss 2.8110151290893555; avg tr loss 2.717226615548134
Batch 1300: tr loss 2.7262725830078125; avg tr loss 2.7175800739801845
Batch 1400: tr loss 2.8143768310546875; avg tr loss 2.7178527573176794
Batch 1500: tr loss 2.7188339233398438; avg tr loss 2.718662288824717
Batch 1600: tr loss 2.7495508193969727; avg tr loss 2.7195540027320386
Batch 1700: tr loss 2.7391135692596436; avg tr loss 2.720338927297031
Batch 1800: tr loss 2.7370173931121826; avg tr loss 2.720877128707038
Batch 1900: tr loss 2.6186864376068115; avg tr loss 2.7214086422167325
Batch 2000: tr loss 2.8969743251800537; avg tr loss 2.722695441484451
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.6261730194091797; avg tr loss 2.723728175844465
Batch 2200: tr loss 2.675830364227295; avg tr loss 2.7241833411563525
Batch 2300: tr loss 2.5939693450927734; avg tr loss 2.7257689212716145
Epoch 82: Average Training Loss: 2.726289505844637, Average Validation Loss: 3.1921065151691437
Batch 100: tr loss 2.5968596935272217; avg tr loss 2.6933854007720948
Batch 200: tr loss 2.680300235748291; avg tr loss 2.696022026538849
Batch 300: tr loss 2.7426986694335938; avg tr loss 2.6930561288197836
Batch 400: tr loss 2.704435348510742; avg tr loss 2.700457352399826
Batch 500: tr loss 2.7152507305145264; avg tr loss 2.7027765278816225
Batch 600: tr loss 2.6938207149505615; avg tr loss 2.7033048470815024
Batch 700: tr loss 2.7370104789733887; avg tr loss 2.7047256258555823
Batch 800: tr loss 2.6864991188049316; avg tr loss 2.705136431455612
Batch 900: tr loss 2.78352952003479; avg tr loss 2.7076661298010083
Batch 1000: tr loss 2.6922707557678223; avg tr loss 2.7086421349048613
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7030370235443115; avg tr loss 2.712528151165355
Batch 1200: tr loss 2.746851921081543; avg tr loss 2.713402977983157
Batch 1300: tr loss 2.747790813446045; avg tr loss 2.7143026361098657
Batch 1400: tr loss 2.649200916290283; avg tr loss 2.7153530645370485
Batch 1500: tr loss 2.8139379024505615; avg tr loss 2.716210973262787
Batch 1600: tr loss 2.839057683944702; avg tr loss 2.7170766688883305
Batch 1700: tr loss 2.8597302436828613; avg tr loss 2.7180076698695914
Batch 1800: tr loss 2.719665765762329; avg tr loss 2.7195616428057354
Batch 1900: tr loss 2.746868133544922; avg tr loss 2.7210725911040057
Batch 2000: tr loss 2.7888588905334473; avg tr loss 2.7216746084690095
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7321486473083496; avg tr loss 2.721881057307834
Batch 2200: tr loss 2.8017306327819824; avg tr loss 2.722392574440349
Batch 2300: tr loss 2.93349027633667; avg tr loss 2.723389020691747
Epoch 83: Average Training Loss: 2.723740625300098, Average Validation Loss: 3.1983087261517844
Batch 100: tr loss 2.755384922027588; avg tr loss 2.67860004901886
Batch 200: tr loss 2.7385737895965576; avg tr loss 2.691404639482498
Batch 300: tr loss 2.6808347702026367; avg tr loss 2.6961510864893596
Batch 400: tr loss 2.64945650100708; avg tr loss 2.699841810464859
Batch 500: tr loss 2.8333628177642822; avg tr loss 2.699136585712433
Batch 600: tr loss 2.8226704597473145; avg tr loss 2.698728422721227
Batch 700: tr loss 2.723965883255005; avg tr loss 2.7008320569992064
Batch 800: tr loss 2.7377779483795166; avg tr loss 2.70259670317173
Batch 900: tr loss 2.6824522018432617; avg tr loss 2.7053765829404197
Batch 1000: tr loss 2.7245607376098633; avg tr loss 2.7076029863357545
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.5885796546936035; avg tr loss 2.707668985887007
Batch 1200: tr loss 2.701648235321045; avg tr loss 2.7093937196334203
Batch 1300: tr loss 2.585178852081299; avg tr loss 2.71028404877736
Batch 1400: tr loss 2.851339817047119; avg tr loss 2.7117512842587064
Batch 1500: tr loss 2.5186142921447754; avg tr loss 2.7131337990760804
Batch 1600: tr loss 2.656611442565918; avg tr loss 2.713978625535965
Batch 1700: tr loss 2.676374912261963; avg tr loss 2.7147173895555383
Batch 1800: tr loss 2.6912035942077637; avg tr loss 2.7161122494273715
Batch 1900: tr loss 2.691525936126709; avg tr loss 2.7177355249304522
Batch 2000: tr loss 2.6951799392700195; avg tr loss 2.718804678082466
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.9310710430145264; avg tr loss 2.7190740123249237
Batch 2200: tr loss 2.7094788551330566; avg tr loss 2.7192744196544996
Batch 2300: tr loss 2.8175466060638428; avg tr loss 2.7203227158214736
Epoch 84: Average Training Loss: 2.7211654203540228, Average Validation Loss: 3.197163869937261
Batch 100: tr loss 2.6831138134002686; avg tr loss 2.6905175828933716
Batch 200: tr loss 2.522991418838501; avg tr loss 2.694645960330963
Batch 300: tr loss 2.7185707092285156; avg tr loss 2.6973196228345233
Batch 400: tr loss 2.718052864074707; avg tr loss 2.6967883479595183
Batch 500: tr loss 2.614198684692383; avg tr loss 2.699668362617493
Batch 600: tr loss 2.5666632652282715; avg tr loss 2.7010329258441925
Batch 700: tr loss 2.581958532333374; avg tr loss 2.7027100947925025
Batch 800: tr loss 2.8343582153320312; avg tr loss 2.703280468583107
Batch 900: tr loss 2.7363784313201904; avg tr loss 2.704649245209164
Batch 1000: tr loss 2.588698148727417; avg tr loss 2.7051470363140107
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7267515659332275; avg tr loss 2.706257292140614
Batch 1200: tr loss 2.6560282707214355; avg tr loss 2.708220439950625
Batch 1300: tr loss 2.687915325164795; avg tr loss 2.709656615257263
Batch 1400: tr loss 2.5276031494140625; avg tr loss 2.710732572249004
Batch 1500: tr loss 2.7551538944244385; avg tr loss 2.7128000508944194
Batch 1600: tr loss 2.6621227264404297; avg tr loss 2.7132031567394734
Batch 1700: tr loss 2.7822680473327637; avg tr loss 2.7144042287153356
Batch 1800: tr loss 2.747354507446289; avg tr loss 2.7156849421395197
Batch 1900: tr loss 2.7338216304779053; avg tr loss 2.7167828372905127
Batch 2000: tr loss 2.6828222274780273; avg tr loss 2.7172969436645507
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7051804065704346; avg tr loss 2.7180651019868396
Batch 2200: tr loss 2.7427172660827637; avg tr loss 2.7183594424074347
Batch 2300: tr loss 2.812068462371826; avg tr loss 2.71883903928425
Epoch 85: Average Training Loss: 2.719102209338556, Average Validation Loss: 3.1919133762518563
Batch 100: tr loss 2.726095676422119; avg tr loss 2.6759930181503297
Batch 200: tr loss 2.744345188140869; avg tr loss 2.68535169005394
Batch 300: tr loss 2.6827232837677; avg tr loss 2.6906016945838926
Batch 400: tr loss 2.7040066719055176; avg tr loss 2.6926048254966735
Batch 500: tr loss 2.735644817352295; avg tr loss 2.6974132838249205
Batch 600: tr loss 2.627559185028076; avg tr loss 2.6973158597946165
Batch 700: tr loss 2.633732795715332; avg tr loss 2.698154217175075
Batch 800: tr loss 2.7270560264587402; avg tr loss 2.6964387610554694
Batch 900: tr loss 2.587040424346924; avg tr loss 2.6985084727075366
Batch 1000: tr loss 2.787156581878662; avg tr loss 2.699648142337799
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.774627685546875; avg tr loss 2.7007241533019326
Batch 1200: tr loss 2.765305280685425; avg tr loss 2.700985601147016
Batch 1300: tr loss 2.592115879058838; avg tr loss 2.701697428043072
Batch 1400: tr loss 2.6869890689849854; avg tr loss 2.7029859719957625
Batch 1500: tr loss 2.722710132598877; avg tr loss 2.704094641208649
Batch 1600: tr loss 2.7163069248199463; avg tr loss 2.7055518192052843
Batch 1700: tr loss 2.805272340774536; avg tr loss 2.706971344947815
Batch 1800: tr loss 2.7312674522399902; avg tr loss 2.708565960460239
Batch 1900: tr loss 2.791609764099121; avg tr loss 2.710098625233299
Batch 2000: tr loss 2.785717487335205; avg tr loss 2.711641565680504
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7060139179229736; avg tr loss 2.7128159517333623
Batch 2200: tr loss 2.880934238433838; avg tr loss 2.7144255516745828
Batch 2300: tr loss 2.7001378536224365; avg tr loss 2.7160640864786894
Epoch 86: Average Training Loss: 2.7163990796019193, Average Validation Loss: 3.1907022098700204
Batch 100: tr loss 2.8014936447143555; avg tr loss 2.700379881858826
Batch 200: tr loss 2.7038278579711914; avg tr loss 2.6970873296260836
Batch 300: tr loss 2.619163751602173; avg tr loss 2.6927972745895388
Batch 400: tr loss 2.682567834854126; avg tr loss 2.6950733077526094
Batch 500: tr loss 2.597160577774048; avg tr loss 2.694952866077423
Batch 600: tr loss 2.8388333320617676; avg tr loss 2.6981204096476237
Batch 700: tr loss 2.709890365600586; avg tr loss 2.698303882394518
Batch 800: tr loss 2.7216522693634033; avg tr loss 2.701286730468273
Batch 900: tr loss 2.769115686416626; avg tr loss 2.701039141284095
Batch 1000: tr loss 2.7362887859344482; avg tr loss 2.7016380898952486
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6548361778259277; avg tr loss 2.7026621146635574
Batch 1200: tr loss 2.7131152153015137; avg tr loss 2.703530859351158
Batch 1300: tr loss 2.723210334777832; avg tr loss 2.704934285420638
Batch 1400: tr loss 2.6095757484436035; avg tr loss 2.7052993370805467
Batch 1500: tr loss 2.580538749694824; avg tr loss 2.7065395833651227
Batch 1600: tr loss 2.7756457328796387; avg tr loss 2.707749283760786
Batch 1700: tr loss 2.726658344268799; avg tr loss 2.708374402242548
Batch 1800: tr loss 2.691983222961426; avg tr loss 2.7088801810476517
Batch 1900: tr loss 2.7877001762390137; avg tr loss 2.7108128382030285
Batch 2000: tr loss 2.8152403831481934; avg tr loss 2.7122284339666365
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.696070671081543; avg tr loss 2.712424362386976
Batch 2200: tr loss 2.551344871520996; avg tr loss 2.7132800130410626
Batch 2300: tr loss 2.662334442138672; avg tr loss 2.7138671479017837
Epoch 87: Average Training Loss: 2.71426269279812, Average Validation Loss: 3.1841621696949005
Batch 100: tr loss 2.8135900497436523; avg tr loss 2.694083218574524
Batch 200: tr loss 2.6756415367126465; avg tr loss 2.6890852105617524
Batch 300: tr loss 2.722325086593628; avg tr loss 2.6942826962471007
Batch 400: tr loss 2.7470080852508545; avg tr loss 2.6951678156852723
Batch 500: tr loss 2.7090940475463867; avg tr loss 2.693282639503479
Batch 600: tr loss 2.593949317932129; avg tr loss 2.6919509450594585
Batch 700: tr loss 2.886096477508545; avg tr loss 2.6932104601178852
Batch 800: tr loss 2.803887128829956; avg tr loss 2.6974093562364576
Batch 900: tr loss 2.652377128601074; avg tr loss 2.698746503723992
Batch 1000: tr loss 2.6515984535217285; avg tr loss 2.7007425656318667
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.584681272506714; avg tr loss 2.7029515663060275
Batch 1200: tr loss 2.7541117668151855; avg tr loss 2.704652122060458
Batch 1300: tr loss 2.832714080810547; avg tr loss 2.70420672416687
Batch 1400: tr loss 2.813265323638916; avg tr loss 2.705366039105824
Batch 1500: tr loss 2.8030192852020264; avg tr loss 2.7068403520584106
Batch 1600: tr loss 2.6055641174316406; avg tr loss 2.7079603385925295
Batch 1700: tr loss 2.753971576690674; avg tr loss 2.7084185577841366
Batch 1800: tr loss 2.730802536010742; avg tr loss 2.7089278433057995
Batch 1900: tr loss 2.766648292541504; avg tr loss 2.709484954382244
Batch 2000: tr loss 2.6777548789978027; avg tr loss 2.709700127005577
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.8365044593811035; avg tr loss 2.7104167378516424
Batch 2200: tr loss 2.6420435905456543; avg tr loss 2.711748092066158
Batch 2300: tr loss 2.650660991668701; avg tr loss 2.7115356632937555
Epoch 88: Average Training Loss: 2.711715669660438, Average Validation Loss: 3.197901060183843
Batch 100: tr loss 2.713352680206299; avg tr loss 2.667743456363678
Batch 200: tr loss 2.6416873931884766; avg tr loss 2.673352862596512
Batch 300: tr loss 2.6082334518432617; avg tr loss 2.6791049989064533
Batch 400: tr loss 2.6869163513183594; avg tr loss 2.6832132691144945
Batch 500: tr loss 2.7110328674316406; avg tr loss 2.6842091484069823
Batch 600: tr loss 2.7223246097564697; avg tr loss 2.6866945028305054
Batch 700: tr loss 2.752180576324463; avg tr loss 2.690039108140128
Batch 800: tr loss 2.7447643280029297; avg tr loss 2.6905388215184214
Batch 900: tr loss 2.6464695930480957; avg tr loss 2.6925460584958394
Batch 1000: tr loss 2.7626936435699463; avg tr loss 2.693181846380234
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.634873867034912; avg tr loss 2.6948606618967923
Batch 1200: tr loss 2.7737207412719727; avg tr loss 2.696239833831787
Batch 1300: tr loss 2.773326873779297; avg tr loss 2.699039994569925
Batch 1400: tr loss 2.8216724395751953; avg tr loss 2.701367623465402
Batch 1500: tr loss 2.6752562522888184; avg tr loss 2.7020905645688376
Batch 1600: tr loss 2.6750028133392334; avg tr loss 2.702725829333067
Batch 1700: tr loss 2.8156776428222656; avg tr loss 2.7033290393212264
Batch 1800: tr loss 2.641507148742676; avg tr loss 2.7045387936962975
Batch 1900: tr loss 2.591975212097168; avg tr loss 2.7049953440616004
Batch 2000: tr loss 2.8025918006896973; avg tr loss 2.7061508190631867
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7016329765319824; avg tr loss 2.706764248098646
Batch 2200: tr loss 2.6642942428588867; avg tr loss 2.707817937894301
Batch 2300: tr loss 2.6779918670654297; avg tr loss 2.7090503393048824
Epoch 89: Average Training Loss: 2.709349048931037, Average Validation Loss: 3.1885832846164703
Batch 100: tr loss 2.6097211837768555; avg tr loss 2.6726178312301636
Batch 200: tr loss 2.8516955375671387; avg tr loss 2.683077598810196
Batch 300: tr loss 2.622140884399414; avg tr loss 2.683990048567454
Batch 400: tr loss 2.7031736373901367; avg tr loss 2.6836720472574234
Batch 500: tr loss 2.8487601280212402; avg tr loss 2.6872289547920225
Batch 600: tr loss 2.7063097953796387; avg tr loss 2.6901072319348653
Batch 700: tr loss 2.926239490509033; avg tr loss 2.693397479738508
Batch 800: tr loss 2.8733012676239014; avg tr loss 2.692744093239307
Batch 900: tr loss 2.6800663471221924; avg tr loss 2.695937263700697
Batch 1000: tr loss 2.609257221221924; avg tr loss 2.6969160220623016
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.670429229736328; avg tr loss 2.6971587540886617
Batch 1200: tr loss 2.713629961013794; avg tr loss 2.697178052266439
Batch 1300: tr loss 2.6867122650146484; avg tr loss 2.698855241445395
Batch 1400: tr loss 2.7443881034851074; avg tr loss 2.698800289971488
Batch 1500: tr loss 2.702256679534912; avg tr loss 2.7003166375160217
Batch 1600: tr loss 2.6317052841186523; avg tr loss 2.7017330931127073
Batch 1700: tr loss 2.6571197509765625; avg tr loss 2.701631317419164
Batch 1800: tr loss 2.6934702396392822; avg tr loss 2.7032313304477267
Batch 1900: tr loss 2.7991881370544434; avg tr loss 2.7046527642952767
Batch 2000: tr loss 2.653494358062744; avg tr loss 2.7055180530548095
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.8136348724365234; avg tr loss 2.7057648950531368
Batch 2200: tr loss 2.9745829105377197; avg tr loss 2.706272872794758
Batch 2300: tr loss 2.719116687774658; avg tr loss 2.706280957304913
Epoch 90: Average Training Loss: 2.7067343418712095, Average Validation Loss: 3.1837492982546487
Batch 100: tr loss 2.7703664302825928; avg tr loss 2.657587432861328
Batch 200: tr loss 2.4989047050476074; avg tr loss 2.6651935529708863
Batch 300: tr loss 2.6226859092712402; avg tr loss 2.672764953772227
Batch 400: tr loss 2.7322514057159424; avg tr loss 2.6780294007062913
Batch 500: tr loss 2.737004518508911; avg tr loss 2.6801625056266785
Batch 600: tr loss 2.72355318069458; avg tr loss 2.6839601322015127
Batch 700: tr loss 2.6501762866973877; avg tr loss 2.6884373457091195
Batch 800: tr loss 2.6227197647094727; avg tr loss 2.6894489386677742
Batch 900: tr loss 2.7937166690826416; avg tr loss 2.6903916891415913
Batch 1000: tr loss 2.6697821617126465; avg tr loss 2.690985833644867
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6970794200897217; avg tr loss 2.691075557795438
Batch 1200: tr loss 2.7174901962280273; avg tr loss 2.692730251351992
Batch 1300: tr loss 2.758180618286133; avg tr loss 2.693809790611267
Batch 1400: tr loss 2.670753002166748; avg tr loss 2.6956069231033326
Batch 1500: tr loss 2.733098030090332; avg tr loss 2.696140991528829
Batch 1600: tr loss 2.604383945465088; avg tr loss 2.696386803686619
Batch 1700: tr loss 2.7093520164489746; avg tr loss 2.6974180542721466
Batch 1800: tr loss 2.732123851776123; avg tr loss 2.699428129725986
Batch 1900: tr loss 2.864659547805786; avg tr loss 2.700673976195486
Batch 2000: tr loss 2.946063995361328; avg tr loss 2.702084136605263
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.617626905441284; avg tr loss 2.703065288066864
Batch 2200: tr loss 2.874256134033203; avg tr loss 2.7039553305235775
Batch 2300: tr loss 2.788384437561035; avg tr loss 2.7048995427463365
Epoch 91: Average Training Loss: 2.7048544595876245, Average Validation Loss: 3.1930520137151084
Batch 100: tr loss 2.604283094406128; avg tr loss 2.6654419684410096
Batch 200: tr loss 2.6634585857391357; avg tr loss 2.6668677258491518
Batch 300: tr loss 2.572939395904541; avg tr loss 2.674307057062785
Batch 400: tr loss 2.7983663082122803; avg tr loss 2.67690372467041
Batch 500: tr loss 2.7001116275787354; avg tr loss 2.679088212966919
Batch 600: tr loss 2.6475601196289062; avg tr loss 2.6804521691799166
Batch 700: tr loss 2.726829767227173; avg tr loss 2.681928176879883
Batch 800: tr loss 2.6310930252075195; avg tr loss 2.6835732170939446
Batch 900: tr loss 2.806140184402466; avg tr loss 2.685100662973192
Batch 1000: tr loss 2.6809840202331543; avg tr loss 2.687466866016388
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.698230743408203; avg tr loss 2.689105355306105
Batch 1200: tr loss 2.8270537853240967; avg tr loss 2.690858596563339
Batch 1300: tr loss 2.7346038818359375; avg tr loss 2.6920528432039115
Batch 1400: tr loss 2.5341954231262207; avg tr loss 2.693034816810063
Batch 1500: tr loss 2.720301628112793; avg tr loss 2.693485322793325
Batch 1600: tr loss 2.519808053970337; avg tr loss 2.6943366043269634
Batch 1700: tr loss 2.6511683464050293; avg tr loss 2.6955433250876033
Batch 1800: tr loss 2.783191204071045; avg tr loss 2.697570732037226
Batch 1900: tr loss 2.7196741104125977; avg tr loss 2.698838363572171
Batch 2000: tr loss 2.8205387592315674; avg tr loss 2.69971207678318
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7092599868774414; avg tr loss 2.7005059017453874
Batch 2200: tr loss 2.744014263153076; avg tr loss 2.7011165887659248
Batch 2300: tr loss 2.7644202709198; avg tr loss 2.70245492655298
Epoch 92: Average Training Loss: 2.7025623659225047, Average Validation Loss: 3.1898767749468484
Batch 100: tr loss 2.5307576656341553; avg tr loss 2.666301920413971
Batch 200: tr loss 2.6795148849487305; avg tr loss 2.666876577138901
Batch 300: tr loss 2.7828032970428467; avg tr loss 2.669283448855082
Batch 400: tr loss 2.7833690643310547; avg tr loss 2.6734407150745394
Batch 500: tr loss 2.670517683029175; avg tr loss 2.67706734085083
Batch 600: tr loss 2.7015082836151123; avg tr loss 2.6782513892650606
Batch 700: tr loss 2.6032862663269043; avg tr loss 2.6792041972705296
Batch 800: tr loss 2.5834693908691406; avg tr loss 2.6810771042108534
Batch 900: tr loss 2.639105796813965; avg tr loss 2.6826205102602643
Batch 1000: tr loss 2.7217698097229004; avg tr loss 2.6848026168346406
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7345707416534424; avg tr loss 2.6862461974404077
Batch 1200: tr loss 2.7829365730285645; avg tr loss 2.6880388216177624
Batch 1300: tr loss 2.6757876873016357; avg tr loss 2.6883249519421506
Batch 1400: tr loss 2.782787799835205; avg tr loss 2.689835276944297
Batch 1500: tr loss 2.6187596321105957; avg tr loss 2.6904940176010133
Batch 1600: tr loss 2.754391670227051; avg tr loss 2.6919238501787186
Batch 1700: tr loss 2.662003755569458; avg tr loss 2.693012745520648
Batch 1800: tr loss 2.8136820793151855; avg tr loss 2.6951029298040603
Batch 1900: tr loss 2.8274354934692383; avg tr loss 2.696713270011701
Batch 2000: tr loss 2.677866220474243; avg tr loss 2.696906241416931
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.6001744270324707; avg tr loss 2.6971256987253827
Batch 2200: tr loss 2.8159074783325195; avg tr loss 2.6984843094782396
Batch 2300: tr loss 2.7280941009521484; avg tr loss 2.6996622840217923
Epoch 93: Average Training Loss: 2.7000297890791716, Average Validation Loss: 3.185009171565374
Batch 100: tr loss 2.815809726715088; avg tr loss 2.67014497756958
Batch 200: tr loss 2.8275389671325684; avg tr loss 2.6662469244003297
Batch 300: tr loss 2.9593722820281982; avg tr loss 2.6696928254763286
Batch 400: tr loss 2.7836451530456543; avg tr loss 2.674615828990936
Batch 500: tr loss 2.607736587524414; avg tr loss 2.6775724840164186
Batch 600: tr loss 2.685600757598877; avg tr loss 2.679747460683187
Batch 700: tr loss 2.7080771923065186; avg tr loss 2.6834450163160053
Batch 800: tr loss 2.5707221031188965; avg tr loss 2.684172967672348
Batch 900: tr loss 2.5875165462493896; avg tr loss 2.685371337996589
Batch 1000: tr loss 2.795513153076172; avg tr loss 2.685807827949524
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6338086128234863; avg tr loss 2.6879412813620136
Batch 1200: tr loss 2.6579160690307617; avg tr loss 2.6878848669926327
Batch 1300: tr loss 2.7902302742004395; avg tr loss 2.688644755253425
Batch 1400: tr loss 2.797900915145874; avg tr loss 2.688239394085748
Batch 1500: tr loss 2.6440305709838867; avg tr loss 2.6888929988543193
Batch 1600: tr loss 2.779567241668701; avg tr loss 2.6899352419376372
Batch 1700: tr loss 2.775355339050293; avg tr loss 2.691568940387053
Batch 1800: tr loss 2.750363826751709; avg tr loss 2.6921497070789338
Batch 1900: tr loss 2.7864794731140137; avg tr loss 2.693272976498855
Batch 2000: tr loss 2.9144034385681152; avg tr loss 2.6943533728122713
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.625122308731079; avg tr loss 2.6955961376144773
Batch 2200: tr loss 2.7464041709899902; avg tr loss 2.6965931368957867
Batch 2300: tr loss 2.7402968406677246; avg tr loss 2.6976836392153865
Epoch 94: Average Training Loss: 2.6980822760700773, Average Validation Loss: 3.1881983280181885
Batch 100: tr loss 2.725283622741699; avg tr loss 2.67788512468338
Batch 200: tr loss 2.7231311798095703; avg tr loss 2.6752094173431398
Batch 300: tr loss 2.6320714950561523; avg tr loss 2.674883402188619
Batch 400: tr loss 2.7196385860443115; avg tr loss 2.6754751169681548
Batch 500: tr loss 2.659759044647217; avg tr loss 2.679050322055817
Batch 600: tr loss 2.699094295501709; avg tr loss 2.6778643945852916
Batch 700: tr loss 2.7010416984558105; avg tr loss 2.6792430312292916
Batch 800: tr loss 2.781304359436035; avg tr loss 2.6805530017614365
Batch 900: tr loss 2.5650343894958496; avg tr loss 2.6818255384763083
Batch 1000: tr loss 2.6561148166656494; avg tr loss 2.682847989797592
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.7118005752563477; avg tr loss 2.6848067795146595
Batch 1200: tr loss 2.6811516284942627; avg tr loss 2.6863161834081013
Batch 1300: tr loss 2.638676166534424; avg tr loss 2.6863383870858413
Batch 1400: tr loss 2.7279300689697266; avg tr loss 2.6892352647440774
Batch 1500: tr loss 2.8004415035247803; avg tr loss 2.6901938331921897
Batch 1600: tr loss 3.056626796722412; avg tr loss 2.6902648769319057
Batch 1700: tr loss 2.6393115520477295; avg tr loss 2.6910122097239775
Batch 1800: tr loss 2.8218798637390137; avg tr loss 2.6920362729496428
Batch 1900: tr loss 2.665318250656128; avg tr loss 2.692364777389326
Batch 2000: tr loss 2.62522554397583; avg tr loss 2.6933819575309754
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.79996919631958; avg tr loss 2.6945570599465145
Batch 2200: tr loss 2.676520824432373; avg tr loss 2.6950928662040017
Batch 2300: tr loss 2.743534564971924; avg tr loss 2.6955635758068253
Epoch 95: Average Training Loss: 2.696146685922512, Average Validation Loss: 3.185695618391037
Batch 100: tr loss 2.687027931213379; avg tr loss 2.6783609580993653
Batch 200: tr loss 2.645956039428711; avg tr loss 2.670222532749176
Batch 300: tr loss 2.7393813133239746; avg tr loss 2.6699289854367576
Batch 400: tr loss 2.7442708015441895; avg tr loss 2.670171759724617
Batch 500: tr loss 2.599936008453369; avg tr loss 2.6709533672332766
Batch 600: tr loss 2.713977098464966; avg tr loss 2.6752575000127155
Batch 700: tr loss 2.807936668395996; avg tr loss 2.6785699064391
Batch 800: tr loss 2.6576972007751465; avg tr loss 2.678932177722454
Batch 900: tr loss 2.7120258808135986; avg tr loss 2.6804984482129415
Batch 1000: tr loss 2.697751045227051; avg tr loss 2.680366812705994
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.6483705043792725; avg tr loss 2.680860993428664
Batch 1200: tr loss 2.768883228302002; avg tr loss 2.682194884618123
Batch 1300: tr loss 2.5668578147888184; avg tr loss 2.683104135439946
Batch 1400: tr loss 2.7107813358306885; avg tr loss 2.6832226027761186
Batch 1500: tr loss 2.5953381061553955; avg tr loss 2.6850563195546466
Batch 1600: tr loss 2.721108913421631; avg tr loss 2.686010338664055
Batch 1700: tr loss 2.7469940185546875; avg tr loss 2.687617948616252
Batch 1800: tr loss 2.6902036666870117; avg tr loss 2.6889453834957546
Batch 1900: tr loss 2.6146657466888428; avg tr loss 2.6897554743917365
Batch 2000: tr loss 2.808978319168091; avg tr loss 2.691394144177437
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.6751999855041504; avg tr loss 2.691982539608365
Batch 2200: tr loss 2.601161003112793; avg tr loss 2.6925591912052846
Batch 2300: tr loss 2.6230812072753906; avg tr loss 2.69346456952717
Epoch 96: Average Training Loss: 2.693824277090945, Average Validation Loss: 3.1880740423997245
Batch 100: tr loss 2.7184605598449707; avg tr loss 2.6607023334503173
Batch 200: tr loss 2.5630884170532227; avg tr loss 2.6673099851608275
Batch 300: tr loss 2.5299839973449707; avg tr loss 2.6664091126124063
Batch 400: tr loss 2.654151201248169; avg tr loss 2.668971107006073
Batch 500: tr loss 2.7216081619262695; avg tr loss 2.667242874622345
Batch 600: tr loss 2.722829580307007; avg tr loss 2.670249422391256
Batch 700: tr loss 2.7419683933258057; avg tr loss 2.67178493976593
Batch 800: tr loss 2.5957369804382324; avg tr loss 2.6725461316108703
Batch 900: tr loss 2.669553518295288; avg tr loss 2.674587912824419
Batch 1000: tr loss 2.743845224380493; avg tr loss 2.674720992565155
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.815906524658203; avg tr loss 2.6771108336882157
Batch 1200: tr loss 2.711820602416992; avg tr loss 2.681329671740532
Batch 1300: tr loss 2.6468987464904785; avg tr loss 2.6828190016746523
Batch 1400: tr loss 2.7249226570129395; avg tr loss 2.682909939970289
Batch 1500: tr loss 2.731654405593872; avg tr loss 2.6847271394729613
Batch 1600: tr loss 2.494507312774658; avg tr loss 2.6851352310180663
Batch 1700: tr loss 2.587913990020752; avg tr loss 2.685919554233551
Batch 1800: tr loss 2.6276907920837402; avg tr loss 2.6863261612256366
Batch 1900: tr loss 2.7180004119873047; avg tr loss 2.687288998428144
Batch 2000: tr loss 2.794532299041748; avg tr loss 2.688970237851143
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.739701986312866; avg tr loss 2.6897359122548785
Batch 2200: tr loss 2.7247867584228516; avg tr loss 2.690917039784518
Batch 2300: tr loss 2.789311408996582; avg tr loss 2.6916409961037013
Epoch 97: Average Training Loss: 2.692093783067762, Average Validation Loss: 3.1856866578261056
Batch 100: tr loss 2.758331298828125; avg tr loss 2.650076439380646
Batch 200: tr loss 2.5670864582061768; avg tr loss 2.655091905593872
Batch 300: tr loss 2.678597927093506; avg tr loss 2.6601512813568116
Batch 400: tr loss 2.7114086151123047; avg tr loss 2.661477754116058
Batch 500: tr loss 2.755507707595825; avg tr loss 2.666016414165497
Batch 600: tr loss 2.7389297485351562; avg tr loss 2.6664048667748768
Batch 700: tr loss 2.6584017276763916; avg tr loss 2.669583407810756
Batch 800: tr loss 2.673002243041992; avg tr loss 2.670761418938637
Batch 900: tr loss 2.7512640953063965; avg tr loss 2.672022757795122
Batch 1000: tr loss 2.707754135131836; avg tr loss 2.6732385210990905
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.8872501850128174; avg tr loss 2.674757398041812
Batch 1200: tr loss 2.9002556800842285; avg tr loss 2.6770057314634323
Batch 1300: tr loss 2.793241500854492; avg tr loss 2.6788262625840993
Batch 1400: tr loss 2.5722856521606445; avg tr loss 2.680526317187718
Batch 1500: tr loss 2.7602529525756836; avg tr loss 2.681755926132202
Batch 1600: tr loss 2.7404630184173584; avg tr loss 2.6817900496721268
Batch 1700: tr loss 2.7037830352783203; avg tr loss 2.683069842703202
Batch 1800: tr loss 2.5415406227111816; avg tr loss 2.6841846716403963
Batch 1900: tr loss 2.8748717308044434; avg tr loss 2.6858070693517986
Batch 2000: tr loss 2.7017416954040527; avg tr loss 2.687392257809639
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.665217161178589; avg tr loss 2.687534296058473
Batch 2200: tr loss 2.711325168609619; avg tr loss 2.6882150939377873
Batch 2300: tr loss 2.667750835418701; avg tr loss 2.6894574242052824
Epoch 98: Average Training Loss: 2.6900476984196557, Average Validation Loss: 3.1855275630950928
Batch 100: tr loss 2.5858349800109863; avg tr loss 2.6553235387802125
Batch 200: tr loss 2.608407974243164; avg tr loss 2.658379639387131
Batch 300: tr loss 2.5456748008728027; avg tr loss 2.659051253000895
Batch 400: tr loss 2.808929920196533; avg tr loss 2.6642992520332336
Batch 500: tr loss 2.659554958343506; avg tr loss 2.666484649181366
Batch 600: tr loss 2.719813585281372; avg tr loss 2.669667164882024
Batch 700: tr loss 2.789262056350708; avg tr loss 2.6711685494014197
Batch 800: tr loss 2.729010581970215; avg tr loss 2.670724081993103
Batch 900: tr loss 2.6718814373016357; avg tr loss 2.6726385916603936
Batch 1000: tr loss 2.571639060974121; avg tr loss 2.674258296728134
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.764693260192871; avg tr loss 2.675788712501526
Batch 1200: tr loss 2.663513422012329; avg tr loss 2.675791725118955
Batch 1300: tr loss 2.8953018188476562; avg tr loss 2.678468676530398
Batch 1400: tr loss 2.625570774078369; avg tr loss 2.679958322899682
Batch 1500: tr loss 2.752406120300293; avg tr loss 2.682124468167623
Batch 1600: tr loss 2.7203054428100586; avg tr loss 2.6817871002852915
Batch 1700: tr loss 2.8376917839050293; avg tr loss 2.6830718989933238
Batch 1800: tr loss 2.8020360469818115; avg tr loss 2.684272130727768
Batch 1900: tr loss 2.7293434143066406; avg tr loss 2.6852235284604524
Batch 2000: tr loss 2.8219871520996094; avg tr loss 2.685571069955826
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.7189438343048096; avg tr loss 2.68674342303049
Batch 2200: tr loss 2.570042133331299; avg tr loss 2.6868111259287053
Batch 2300: tr loss 2.756241798400879; avg tr loss 2.687429357611615
Epoch 99: Average Training Loss: 2.687760920044505, Average Validation Loss: 3.1794950465361276
Batch 100: tr loss 2.667445659637451; avg tr loss 2.651164629459381
Batch 200: tr loss 2.5781197547912598; avg tr loss 2.6549406170845034
Batch 300: tr loss 2.6728315353393555; avg tr loss 2.659278004169464
Batch 400: tr loss 2.715013027191162; avg tr loss 2.6625333696603777
Batch 500: tr loss 2.7625315189361572; avg tr loss 2.663521510601044
Batch 600: tr loss 2.508114814758301; avg tr loss 2.663721756140391
Batch 700: tr loss 2.626917839050293; avg tr loss 2.6637894722393582
Batch 800: tr loss 2.7549779415130615; avg tr loss 2.6652780318260194
Batch 900: tr loss 2.722028970718384; avg tr loss 2.666747188038296
Batch 1000: tr loss 2.6560046672821045; avg tr loss 2.668008929729462
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 1100: tr loss 2.633115291595459; avg tr loss 2.6697585036537865
Batch 1200: tr loss 2.6513688564300537; avg tr loss 2.6717189884185792
Batch 1300: tr loss 2.663907289505005; avg tr loss 2.673206335214468
Batch 1400: tr loss 2.7289187908172607; avg tr loss 2.6755906093120574
Batch 1500: tr loss 2.5735816955566406; avg tr loss 2.677141161282857
Batch 1600: tr loss 2.796664237976074; avg tr loss 2.6790857456624506
Batch 1700: tr loss 2.7525534629821777; avg tr loss 2.6798683883162107
Batch 1800: tr loss 2.7039647102355957; avg tr loss 2.681168757279714
Batch 1900: tr loss 2.6813557147979736; avg tr loss 2.681714698389957
Batch 2000: tr loss 2.7700798511505127; avg tr loss 2.6826756550073623
Saving checkpoint to accelerator_checkpoints_0...
Deleting 1 checkpoints to make room for new checkpoint.
Batch 2100: tr loss 2.5976696014404297; avg tr loss 2.683892763455709
Batch 2200: tr loss 2.7640151977539062; avg tr loss 2.6850114593722605
Batch 2300: tr loss 2.7386698722839355; avg tr loss 2.685708966566169
Epoch 100: Average Training Loss: 2.685966719030927, Average Validation Loss: 3.188553204139074
Saving model...
Script done.